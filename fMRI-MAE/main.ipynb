{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from models import get_vit\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "\n",
    "import lightning as pl\n",
    "from typing import List\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.strategies import DDPStrategy\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, RichModelSummary, RichProgressBar\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064ce98-5adc-4b87-8d39-e4f6e0be8456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08179db-9c6a-4bc6-a245-79fae6884ca2",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1c3fe-28ab-40b7-8906-c6a9c8070d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(config)\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "if use_contrastive_loss:\n",
    "    global_batch_size = global_batch_size // 2 # contrastive loss doubles the batch size with the same samples and different masks\n",
    "print(\"global_batch_size\", global_batch_size)\n",
    "\n",
    "use_cls_token = True if use_contrastive_loss else use_cls_token\n",
    "print(\"use_cls_token\", use_cls_token)\n",
    "\n",
    "num_patches = int(\n",
    "    (img_size[0] / patch_size)\n",
    "    * (img_size[1] / patch_size)\n",
    "    * (img_size[2] / patch_size)\n",
    "    * num_frames\n",
    ")\n",
    "num_patches_per_timepoint = num_patches // num_frames\n",
    "num_encoder_patches = int(num_patches_per_timepoint * (1 - tube_start_masking_ratio) * num_frames)\n",
    "num_decoder_patches = int(num_patches_per_timepoint * (1 - decoder_mask_ratio) * num_frames)\n",
    "print(\"num_patches\", num_patches)\n",
    "print(\"num_encoder_patches\", num_encoder_patches)\n",
    "print(\"num_decoder_patches\", num_decoder_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5313372-a58c-497d-a265-ef83480ebcaf",
   "metadata": {},
   "source": [
    "# DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b46c3-3cbe-45b7-89b3-21caa6ff9d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataloader import fMRIDataModule\n",
    "datamodule = fMRIDataModule(train_urls=train_urls, test_urls=test_urls, batch_size=global_batch_size,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d05a32-12eb-494a-82df-dce4c7e9924c",
   "metadata": {},
   "source": [
    "# Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4963686-7d07-448f-b519-d93a8c71848d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FMRITrainer(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder_model: str, \n",
    "        decoder_model: str,\n",
    "        img_size: List[int],\n",
    "        patch_size: int,\n",
    "        num_frames: int,\n",
    "        frame_patch_size: int,\n",
    "        use_rope_emb: bool,\n",
    "        use_cls_token: bool,\n",
    "        masking_strategy: str,\n",
    "        max_steps: int,\n",
    "        tube_start_masking_ratio: float,\n",
    "        tube_end_masking_ratio: float,\n",
    "        use_contrastive_loss: bool,\n",
    "        decoder_mask_ratio: float,\n",
    "        constrastive_loss_weight: float,\n",
    "        max_lr: float,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(img_size) == 3 # 3D volumes\n",
    "        self.max_steps = max_steps\n",
    "        self.tube_start_masking_ratio = tube_start_masking_ratio\n",
    "        self.tube_end_masking_ratio = tube_end_masking_ratio\n",
    "        self.masking_strategy = masking_strategy\n",
    "        self.frame_patch_size = frame_patch_size\n",
    "        self.num_frames = num_frames\n",
    "        self.decoder_mask_ratio = decoder_mask_ratio\n",
    "        self.constrastive_loss_weight = constrastive_loss_weight\n",
    "        self.max_lr = max_lr\n",
    "        self.batch_size = batch_size\n",
    "        self.use_contrastive_loss = use_contrastive_loss\n",
    "        self.use_cls_token = use_cls_token\n",
    "    \n",
    "        self.model = get_vit(\n",
    "            size={\"encoder\": encoder_model, \"decoder\": decoder_model},\n",
    "            image_size=img_size,  # depth, height, width\n",
    "            image_patch_size=(patch_size,patch_size,patch_size),  # depth, height, width patch size\n",
    "            frames=num_frames,\n",
    "            frame_patch_size=frame_patch_size,\n",
    "            channels=1,\n",
    "            use_rope_emb=use_rope_emb,\n",
    "            use_cls_token=use_cls_token,\n",
    "        )\n",
    "        self.aug_transform = utils.DataPrepper(\n",
    "            num_frames=num_frames,\n",
    "            masking_strategy=masking_strategy,\n",
    "            patch_depth=patch_size,\n",
    "            patch_height=patch_size,\n",
    "            patch_width=patch_size,\n",
    "            frame_patch_size=frame_patch_size,\n",
    "        )\n",
    "        self.num_patches = int(\n",
    "            (img_size[0] / patch_size)\n",
    "            * (img_size[1] / patch_size)\n",
    "            * (img_size[2] / patch_size)\n",
    "            * (num_frames/frame_patch_size)\n",
    "        )\n",
    "        patchify_brain = Rearrange(\n",
    "            \"b c (f pf) (d pd) (h ph) (w pw) -> b f d h w (pd ph pw pf c)\",\n",
    "            pd=patch_size,\n",
    "            ph=patch_size,\n",
    "            pw=patch_size,\n",
    "            pf=1,\n",
    "        )\n",
    "        if self.masking_strategy==\"MNI\":\n",
    "            MNI_brain = nib.load(\"/weka/proj-fmri/paulscotti/old_fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "            brain_pos_voxels = MNI_brain[6:94,8:112,10:82]\n",
    "            self.brain_pos_pats = patchify_brain(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "            self.brain_pos_pats_vit = rearrange(self.brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "            \n",
    "        ## LOSS\n",
    "        self.mse = nn.MSELoss()\n",
    "        if use_contrastive_loss:\n",
    "            self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))  # learned logit scale\n",
    "\n",
    "\n",
    "    def shared_step(self, batch, batch_idx, phase: str):\n",
    "        tube_mask_ratio = utils.get_masking_ratio(\n",
    "            current_epoch=self.global_step, \n",
    "            total_epochs=self.max_steps, \n",
    "            start_masking_ratio=self.tube_start_masking_ratio, \n",
    "            end_masking_ratio=self.tube_end_masking_ratio\n",
    "        )\n",
    "        \n",
    "        input_func = batch['func.npy'].to(self.device)\n",
    "        if self.masking_strategy==\"MNI\":\n",
    "            func, _ = self.aug_transform(input_func)\n",
    "            brain_pos_pats = self.brain_pos_pats\n",
    "            brain_pos_pats_vit = self.brain_pos_pats_vit\n",
    "        else:\n",
    "            func, brain_pos_voxels = self.aug_transform(input_func)\n",
    "            brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "            brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "        \n",
    "        if self.use_contrastive_loss and phase==\"train\":  # create positive pairs by duplicating the batch\n",
    "            func = torch.cat([func, func], dim=0)\n",
    "            brain_pos_pats = torch.cat([brain_pos_pats, brain_pos_pats], dim=0)\n",
    "            brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "        \n",
    "        func = func.unsqueeze(1)\n",
    "        batch_size=func.shape[0]\n",
    "        # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "        tube_mask = torch.zeros(self.num_patches // (self.num_frames//self.frame_patch_size)).to(torch.bool).to(self.device)\n",
    "        batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "        mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "        # check if there's not enough brain left for code to continue\n",
    "        if len(mask_idx_candidates) < int(self.num_patches/(self.num_frames//self.frame_patch_size)*(1-tube_mask_ratio)) + 50: # atleast 50 token for the decoder to reconstruct\n",
    "            print(\"Brain volume skipped due to not enough brain-positive patches remaining...\")\n",
    "            return\n",
    "        mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "        tube_idx = mask_idx_candidates[:int(self.num_patches / (self.num_frames//self.frame_patch_size) * (1 - tube_mask_ratio))]\n",
    "        tube_mask[tube_idx] = True\n",
    "        tube_mask = tube_mask.tile(self.num_frames//self.frame_patch_size)\n",
    "        \n",
    "        # create decoder mask\n",
    "        decoder_mask = torch.zeros(self.num_patches // (self.num_frames//self.frame_patch_size)).to(torch.bool).to(self.device)\n",
    "        remaining_mask_idx = mask_idx_candidates[int(self.num_patches / (self.num_frames//self.frame_patch_size) * (1 - tube_mask_ratio)):]\n",
    "        decoder_mask_idx = remaining_mask_idx[:int(self.num_patches / (self.num_frames//self.frame_patch_size) * (1 - self.decoder_mask_ratio))]\n",
    "        decoder_mask[decoder_mask_idx] = True\n",
    "        decoder_mask = decoder_mask.tile(self.num_frames//frame_patch_size)\n",
    "        \n",
    "        # encode the tube patches\n",
    "        encoder_out = self.model(func.to(self.device), encoder_mask=tube_mask.to(self.device))\n",
    "        if self.use_cls_token:\n",
    "            enc_cls_token = encoder_out[:,:1,:]\n",
    "\n",
    "        # decode both the encoder_out patches and masked decoder patches\n",
    "        decoder_out = self.model(encoder_out, encoder_mask=tube_mask, decoder_mask=decoder_mask)\n",
    "        # subset only the reconstructed decoder patches\n",
    "        output = decoder_out[:, -decoder_mask.sum():]\n",
    "        \n",
    "        # compare to ground truth and calculate loss\n",
    "        target_patches = self.model.patchify(func)\n",
    "        target_patches_vit = rearrange(target_patches, \"b ... d -> b (...) d\")\n",
    "        target = target_patches_vit[:, decoder_mask]\n",
    "        loss = self.mse(output, target)\n",
    "        self.log(f\"{phase}/recon_loss\", loss, on_step=True, on_epoch=False, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        # contrastive loss\n",
    "        if self.use_contrastive_loss and phase==\"train\":\n",
    "            n_b = len(func) // 2\n",
    "            cls_token1 = enc_cls_token[:n_b, 0, :]  # first half of batch, cls_token shape B, 1, d_model\n",
    "            cls_token2 = enc_cls_token[n_b:, 0, :]\n",
    "            contrastive_loss = utils.contrastive_loss(cls_token1, cls_token2, temperature=self.logit_scale)\n",
    "            cnt_loss = self.constrastive_loss_weight * contrastive_loss\n",
    "            self.log(f\"{phase}/contrastive_loss\", cnt_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "            loss += cnt_loss\n",
    "        self.log(f\"{phase}/loss\", loss, on_step=True, on_epoch=False, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        return loss\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, batch_idx, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, batch_idx, \"val\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, batch_idx, \"test\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.max_lr)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.max_lr,\n",
    "            total_steps=self.max_steps\n",
    "        )\n",
    "        return {\"optimizer\": self.optimizer, \"lr_scheduler\": {\n",
    "            \"scheduler\": self.lr_scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1\n",
    "        }}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c721e70f-a053-43eb-b34a-ce7c6920e3a0",
   "metadata": {},
   "source": [
    "# Callbacks and trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143cd8ae-4faa-43e7-9243-10e91a037071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.abspath(f'../ckpts/{model_name}'),\n",
    "    filename='{epoch}-{train/loss:.5f}',\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='train/loss',\n",
    "    mode='min',\n",
    ")\n",
    "callbacks = [\n",
    "    LearningRateMonitor(),\n",
    "    checkpoint_callback,\n",
    "    RichModelSummary(max_depth=2),\n",
    "    RichProgressBar()\n",
    "]\n",
    "trainer = Trainer(\n",
    "    devices=\"auto\",\n",
    "    num_nodes=int(os.getenv('NUM_NODES', 1)),\n",
    "    precision=\"16-mixed\",\n",
    "    logger=WandbLogger(project='found', id=model_name, name=model_name),\n",
    "    callbacks=callbacks,\n",
    "    max_steps=max_steps,\n",
    "    # strategy = DDPStrategy(process_group_backend=\"gloo\"), # COMMENT THIS IF USING NOTEBOOK\n",
    "    val_check_interval=eval_per_n_steps,\n",
    "    log_every_n_steps=10,\n",
    "    limit_val_batches=limit_val_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d211f-583d-4d0e-bbef-e217d5996d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "module = FMRITrainer(\n",
    "    encoder_model=encoder_model, \n",
    "    decoder_model=decoder_model, \n",
    "    img_size=img_size, \n",
    "    patch_size=patch_size, \n",
    "    num_frames=num_frames, \n",
    "    masking_strategy=masking_strategy,\n",
    "    tube_start_masking_ratio=tube_start_masking_ratio, \n",
    "    tube_end_masking_ratio=tube_end_masking_ratio,\n",
    "    decoder_mask_ratio=decoder_mask_ratio, \n",
    "    frame_patch_size=frame_patch_size, \n",
    "    use_rope_emb=use_rope_emb, \n",
    "    use_cls_token=use_cls_token,\n",
    "    max_lr=max_lr, \n",
    "    max_steps=max_steps,\n",
    "    batch_size=global_batch_size,\n",
    "    use_contrastive_loss=use_contrastive_loss,\n",
    "    constrastive_loss_weight=constrastive_loss_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e418f-2705-4e9a-8c70-c1e4ea1e01a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(module, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1a37f-ba4a-47c1-a0b4-ee00462df24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b7ae8-68cb-4a22-809d-7abff3f365df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
