{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "seed = 0\n",
    "import utils\n",
    "\n",
    "from IPython.display import clear_output # function to clear print outputs in cell\n",
    "%load_ext autoreload \n",
    "# this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2b2ad-e1ef-4262-8263-6ae9a0766caa",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127deb2-bf23-4a2e-8f86-fa8b22183546",
   "metadata": {},
   "source": [
    "### betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d286a8a-2377-4444-86d5-fa0d728d3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vox = np.load(\"glmsingle/TYPEB_FITHRF.npz\")['betasmd'][:,0,0].T\n",
    "# vox.shape # num_trials, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7063dcb1-5ae5-4a9d-a7b0-cb3323224840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>key</th>\n",
       "      <th>sub</th>\n",
       "      <th>ses</th>\n",
       "      <th>run</th>\n",
       "      <th>start</th>\n",
       "      <th>events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.2957616, 0.8341554, 0.977095, 0.7594984, 0....</td>\n",
       "      <td>sub-01_ses-25_run-09</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 45455}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.6828736, 0.76685095, 0.801588, 1.1775866, 0...</td>\n",
       "      <td>sub-01_ses-25_run-09</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 45455}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.3467954, 0.62295175, 0.53726166, 0.94804835...</td>\n",
       "      <td>sub-01_ses-25_run-09</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 45455}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.2597265, 0.56107175, 0.88799864, 1.0987167,...</td>\n",
       "      <td>sub-01_ses-25_run-09</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 45455}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.9291683, 0.7384197, 0.43327004, 0.96608216,...</td>\n",
       "      <td>sub-01_ses-25_run-09</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>64</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 45455}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146683</th>\n",
       "      <td>[0.39492762, 0.9567513, 1.9104024, 0.6862465, ...</td>\n",
       "      <td>sub-01_ses-09_run-07</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 46391}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146684</th>\n",
       "      <td>[0.6176336, 0.97453976, 1.6229396, 0.7125873, ...</td>\n",
       "      <td>sub-01_ses-09_run-07</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 46391}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146685</th>\n",
       "      <td>[0.36875397, 0.9014565, 2.026524, 0.7028155, 0...</td>\n",
       "      <td>sub-01_ses-09_run-07</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 46391}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146686</th>\n",
       "      <td>[0.11672564, 0.7731965, 2.1594627, 0.8890982, ...</td>\n",
       "      <td>sub-01_ses-09_run-07</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 46391}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146687</th>\n",
       "      <td>[-0.31813443, 0.6642333, 1.2111845, 0.8097815,...</td>\n",
       "      <td>sub-01_ses-09_run-07</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>79</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 46391}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146688 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  feature  \\\n",
       "0       [1.2957616, 0.8341554, 0.977095, 0.7594984, 0....   \n",
       "1       [1.6828736, 0.76685095, 0.801588, 1.1775866, 0...   \n",
       "2       [1.3467954, 0.62295175, 0.53726166, 0.94804835...   \n",
       "3       [1.2597265, 0.56107175, 0.88799864, 1.0987167,...   \n",
       "4       [0.9291683, 0.7384197, 0.43327004, 0.96608216,...   \n",
       "...                                                   ...   \n",
       "146683  [0.39492762, 0.9567513, 1.9104024, 0.6862465, ...   \n",
       "146684  [0.6176336, 0.97453976, 1.6229396, 0.7125873, ...   \n",
       "146685  [0.36875397, 0.9014565, 2.026524, 0.7028155, 0...   \n",
       "146686  [0.11672564, 0.7731965, 2.1594627, 0.8890982, ...   \n",
       "146687  [-0.31813443, 0.6642333, 1.2111845, 0.8097815,...   \n",
       "\n",
       "                         key  sub  ses  run  start  \\\n",
       "0       sub-01_ses-25_run-09    1   25    9      0   \n",
       "1       sub-01_ses-25_run-09    1   25    9     16   \n",
       "2       sub-01_ses-25_run-09    1   25    9     32   \n",
       "3       sub-01_ses-25_run-09    1   25    9     48   \n",
       "4       sub-01_ses-25_run-09    1   25    9     64   \n",
       "...                      ...  ...  ...  ...    ...   \n",
       "146683  sub-01_ses-09_run-07    1    9    7     15   \n",
       "146684  sub-01_ses-09_run-07    1    9    7     31   \n",
       "146685  sub-01_ses-09_run-07    1    9    7     47   \n",
       "146686  sub-01_ses-09_run-07    1    9    7     63   \n",
       "146687  sub-01_ses-09_run-07    1    9    7     79   \n",
       "\n",
       "                                                   events  \n",
       "0       [{'index': 12, 'nsd_id': 45455}, {'index': 16,...  \n",
       "1       [{'index': 12, 'nsd_id': 45455}, {'index': 16,...  \n",
       "2       [{'index': 12, 'nsd_id': 45455}, {'index': 16,...  \n",
       "3       [{'index': 12, 'nsd_id': 45455}, {'index': 16,...  \n",
       "4       [{'index': 12, 'nsd_id': 45455}, {'index': 16,...  \n",
       "...                                                   ...  \n",
       "146683  [{'index': 12, 'nsd_id': 46391}, {'index': 16,...  \n",
       "146684  [{'index': 12, 'nsd_id': 46391}, {'index': 16,...  \n",
       "146685  [{'index': 12, 'nsd_id': 46391}, {'index': 16,...  \n",
       "146686  [{'index': 12, 'nsd_id': 46391}, {'index': 16,...  \n",
       "146687  [{'index': 12, 'nsd_id': 46391}, {'index': 16,...  \n",
       "\n",
       "[146688 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.read_parquet('/weka/proj-fmri/paulscotti/fMRI-foundation-model/flat/checkpoints/nsdflat_large/epoch99/test.parquet')\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d05c11e-b4f9-4411-8ab1-d2d515696a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sessions 40  sess 1  n_runs 12  n_TRs 285  n_features 1024\n"
     ]
    }
   ],
   "source": [
    "n_sessions = 40 # ses ranges from 1 to 40\n",
    "sess = 1\n",
    "n_runs = features[(features['ses']==sess)]['run'].max()\n",
    "n_TRs = len(features[(features['ses']==sess)&(features['run']==1)])\n",
    "n_features = len(features[(features['ses']==sess)]['feature'].values[0])\n",
    "print(f\"n_sessions {n_sessions}  sess {sess}  n_runs {n_runs}  n_TRs {n_TRs}  n_features {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b8c2fbf5-e9ba-4718-a4e7-02319ddd8fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "583\n"
     ]
    }
   ],
   "source": [
    "# DESIGN MATRIX\n",
    "images = []\n",
    "indices = []\n",
    "run_ids = []\n",
    "for r in range(1,n_runs+1):\n",
    "    images = np.append(images, [even['nsd_id'] for even in features[(features['run']==r)&(features['ses']==sess)&(features['start']==0)]['events'].values[0]])\n",
    "    indices = np.append(indices, [even['index'] for even in features[(features['run']==r)&(features['ses']==sess)&(features['start']==0)]['events'].values[0]])\n",
    "    run_ids = np.append(run_ids, np.repeat(r, len([even['index'] for even in features[(features['run']==r)&(features['ses']==sess)&(features['start']==0)]['events'].values[0]])))\n",
    "\n",
    "len_images = len(images)\n",
    "print(len_images)\n",
    "unique_images = np.unique(images)\n",
    "len_unique_images = len(unique_images)\n",
    "print(len_unique_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d6f195fe-a0b3-4d1d-a5ba-0ff4d86937cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run = 1\n",
      "run = 2\n",
      "run = 3\n",
      "run = 4\n",
      "run = 5\n",
      "run = 6\n",
      "run = 7\n",
      "run = 8\n",
      "run = 9\n",
      "run = 10\n",
      "run = 11\n",
      "run = 12\n",
      "(1024, 285)\n"
     ]
    }
   ],
   "source": [
    "data = [np.zeros((n_features, n_TRs)) for _ in range(n_runs)]\n",
    "for cur_run in range(1,n_runs+1):\n",
    "    print(f\"run = {cur_run}\")\n",
    "    for timepoint in range(n_TRs):\n",
    "        feats = features[(features['run']==cur_run)&(features['ses']==sess)&(features['start']==0)]['feature'].values[0]\n",
    "        data[cur_run-1][:, timepoint] = feats\n",
    "    # data[cur_run-1] = utils.zscore(data[cur_run-1])\n",
    "print(data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9d18c25a-dfa3-4548-9fa3-94846f46cc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 1024)\n"
     ]
    }
   ],
   "source": [
    "vox = None\n",
    "delay = 4\n",
    "for ii,(im,idx,run) in enumerate(zip(images,indices,run_ids)):\n",
    "    run = int(run - 1)\n",
    "    idx = int(idx + delay)\n",
    "    cur_features = data[run][:,idx]\n",
    "    if vox is None:\n",
    "        vox = cur_features[None]\n",
    "    else:\n",
    "        vox = np.vstack((vox, cur_features[None]))\n",
    "        \n",
    "print(vox.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda83a7-803f-4d30-9301-0b0a533409fb",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7ddf09b0-18fa-4c02-b609-2f6083a159d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 73k NSD images\n",
    "data_path = \"/weka/proj-medarc/shared/mindeyev2_dataset\"\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:]\n",
    "images = torch.Tensor(images).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27ba8ffe-0c74-4571-85f3-65fbcd0a42cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([750, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "images_idx = np.load(\"sess1_images.npy\").astype(np.int64)\n",
    "images = images[images_idx]\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b80aeb2d-6d53-431c-90ed-658dca7ecebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 1024)\n",
      "torch.Size([750, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(vox.shape)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8f554db1-f7cd-40d2-ab62-5d1e282c2bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "562 188\n"
     ]
    }
   ],
   "source": [
    "utils.seed_everything(0)\n",
    "\n",
    "all_indices = np.random.permutation(np.arange(len(images)))\n",
    "print(len(all_indices))\n",
    "train_image_indices = all_indices[:int(len(images)*.75)]\n",
    "test_image_indices = all_indices[int(len(images)*.75):]\n",
    "print(len(train_image_indices), len(test_image_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "590f2b4b-db7c-42a1-bfd0-cc578e6af988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voxels have been zscored\n",
      "-0.0011424535909438304 1.0088983015449284\n",
      "vox (750, 1024)\n"
     ]
    }
   ],
   "source": [
    "train_mean = np.mean(vox[train_image_indices],axis=0)\n",
    "train_std = np.std(vox[train_image_indices],axis=0)\n",
    "\n",
    "vox = utils.zscore(vox,train_mean=train_mean,train_std=train_std)\n",
    "print(\"voxels have been zscored\")\n",
    "print(vox[:,0].mean(), vox[:,0].std())\n",
    "print(\"vox\", vox.shape)\n",
    "\n",
    "images = torch.Tensor(images)\n",
    "vox = torch.Tensor(vox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cc5d2e32-6027-4a19-bef4-5ca068db35bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float32 # change depending on your mixed_precision\n",
    "\n",
    "accelerator = Accelerator(split_batches=False)# mixed_precision=\"fp16\") # ['no', 'fp8', 'fp16', 'bf16']\n",
    "batch_size = 8 # 8 # 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 1280195\n",
      "device: cuda\n",
      "global_batch_size 8\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "global_batch_size = batch_size * num_devices\n",
    "print(\"global_batch_size\", global_batch_size)\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "# set data_type to match your mixed precision (automatically set based on deepspeed config)\n",
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    data_type = torch.bfloat16\n",
    "elif accelerator.mixed_precision == \"fp16\":\n",
    "    data_type = torch.float16\n",
    "else:\n",
    "    data_type = torch.float32\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: testing__\n",
      "--data_path=/weka/proj-medarc/shared/mindeyev2_dataset                     --model_name=testing__                     --no-multi_subject --subj=1 --batch_size=8                     --hidden_dim=1024 --clip_scale=1.                     --no-blurry_recon --blur_scale=.5                     --no-use_prior --prior_scale=30 --no-visualize_prior                     --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=20 --no-use_image_aug                     --ckpt_interval=999 --no-ckpt_saving --no-wandb_log --new_test\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"testing__\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/weka/proj-medarc/shared/mindeyev2_dataset \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --no-multi_subject --subj=1 --batch_size={batch_size} \\\n",
    "                    --hidden_dim=1024 --clip_scale=1. \\\n",
    "                    --no-blurry_recon --blur_scale=.5 \\\n",
    "                    --no-use_prior --prior_scale=30 --no-visualize_prior \\\n",
    "                    --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=20 --no-use_image_aug \\\n",
    "                    --ckpt_interval=999 --no-ckpt_saving --no-wandb_log --new_test\"# \\\n",
    "                    #--multisubject_ckpt=../../train_logs/multisubject_subj01_1024hid_nolow_300ep_seed0\"\n",
    "    # --multisubject_ckpt=../../train_logs/multisubject_subj01_1024hid_nolow_300ep_seed0\"\n",
    "    # /weka/proj-fmri/paulscotti/MindEye2_git/train_logs/final_subj01_pretrained_40sess_24bs\n",
    "    # subj01_pretrained_1sess_1024hid_nolow_seed7\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 0\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/weka/proj-fmri/shared/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=0,\n",
    "    help=\"Number of training sessions to include (if multi_subject, this variable doesnt matter)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"output visualizations from unCLIP every ckpt_interval (requires more memory!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=32,\n",
    "    help=\"Batch size can be increased by 10x if only training v2c and not diffusion diffuser\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--low_mem\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to preload images to cpu to speed things up but consume more memory\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=2,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    import kornia.augmentation as K\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    # Define the blurring augmentations\n",
    "    blur_augment = K.RandomGaussianBlur(kernel_size=(21, 21), sigma=(51.0, 51.0), p=1.)\n",
    "    \n",
    "if multi_subject:\n",
    "    subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 8 num_iterations_per_epoch = 70 num_samples_per_epoch = 562\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    # num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "    num_samples_per_epoch = len(train_image_indices)\n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e1942b0e-1223-40e6-b543-2f7ff2e8ebcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "\n",
    "train_data[f'subj0{subj}'] = torch.utils.data.TensorDataset(torch.tensor(train_image_indices))\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(torch.tensor(test_image_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "81084834-035f-4465-ad59-59e6b806a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 0 sessions\n",
      "num_voxels for subj01: 1024\n",
      "Loaded all subj train dls and vox!\n",
      "\n",
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for s in subj_list:\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    train_dl = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "\n",
    "    num_voxels_list.append(vox[0].shape[-1])\n",
    "    num_voxels[f'subj0{s}'] = vox[0].shape[-1]\n",
    "    voxels[f'subj0{s}'] = vox\n",
    "    print(f\"num_voxels for subj0{s}: {num_voxels[f'subj0{s}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and vox!\\n\")\n",
    "\n",
    "# Validate only on one subject\n",
    "if multi_subject: \n",
    "    subj = subj_list[0] # cant validate on the actual held out person so picking first in subj_list\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=24, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenOpenCLIPImageEmbedder(\n",
      "  (model): CLIP(\n",
      "    (visual): VisionTransformer(\n",
      "      (conv1): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (patch_dropout): Identity()\n",
      "      (ln_pre): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "      (transformer): Transformer(\n",
      "        (resblocks): ModuleList(\n",
      "          (0-47): 48 x ResidualAttentionBlock(\n",
      "            (ln_1): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1664, out_features=1664, bias=True)\n",
      "            )\n",
      "            (ls_1): Identity()\n",
      "            (ln_2): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1664, out_features=8192, bias=True)\n",
      "              (gelu): GELU(approximate='none')\n",
      "              (c_proj): Linear(in_features=8192, out_features=1664, bias=True)\n",
      "            )\n",
      "            (ls_2): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_post): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (token_embedding): Embedding(49408, 1280)\n",
      "    (ln_final): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## USING OpenCLIP ViT-bigG ###\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "# from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "# from omegaconf import OmegaConf\n",
    "\n",
    "try:\n",
    "    print(clip_img_embedder)\n",
    "except:\n",
    "    clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "        arch=\"ViT-bigG-14\",\n",
    "        version=\"laion2b_s39b_b160k\",\n",
    "        output_tokens=True,\n",
    "        only_tokens=True,\n",
    "    )\n",
    "    clip_img_embedder.to(device)\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664\n",
    "\n",
    "# ## USING OPEN AI CLIP ViT-L ###\n",
    "# import clip\n",
    "# try:\n",
    "#     print(clip_model)\n",
    "# except:\n",
    "#     clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "#     preprocess = transforms.Compose([\n",
    "#         transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "#         transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "#                              std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "#     ])\n",
    "# def clip_img_embedder(image):\n",
    "#     preproc_img = preprocess(image)\n",
    "#     return clip_model.encode_image(preproc_img)\n",
    "# clip_seq_dim = 1\n",
    "# clip_emb_dim = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "1,049,600 total\n",
      "1,049,600 trainable\n",
      "param counts:\n",
      "1,049,600 total\n",
      "1,049,600 trainable\n",
      "torch.Size([2, 1, 1024]) torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len=1): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx=0):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(self.seq_len)], dim=1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test on subject 1 with fake data\n",
    "b = torch.randn((2,1,num_voxels_list[0]))\n",
    "print(b.shape, model.ridge(b,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7b8de65a-6d3b-4248-bea9-9b6f4d562321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "453,360,280 total\n",
      "453,360,280 trainable\n",
      "param counts:\n",
      "454,409,880 total\n",
      "454,409,880 trainable\n",
      "b.shape torch.Size([2, 1, 1024])\n",
      "torch.Size([2, 256, 1664]) torch.Size([2, 256, 1664]) torch.Size([1]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "# from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, h=4096, in_dim=15724, out_dim=768, seq_len=1, n_blocks=n_blocks, drop=.15, \n",
    "                 clip_size=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "        \n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.backbone_linear = nn.Linear(h * seq_len, out_dim, bias=True) \n",
    "        if clip_scale>0:\n",
    "            self.clip_proj = self.projector(clip_size, clip_size, h=clip_size)\n",
    "            \n",
    "    def projector(self, in_dim, out_dim, h=2048):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, out_dim)\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "    \n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make empty tensors\n",
    "        c,b = torch.Tensor([0.]), torch.Tensor([[0.],[0.]])\n",
    "        \n",
    "        # Mixer blocks\n",
    "        residual1 = x\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.backbone_linear(x).reshape(len(x), -1, self.clip_size)\n",
    "        if clip_scale>0:\n",
    "            c = self.clip_proj(backbone)\n",
    "        \n",
    "        return backbone, c, b\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, \n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,1,hidden_dim))\n",
    "print(\"b.shape\",b.shape)\n",
    "\n",
    "backbone_, clip_, blur_ = model.backbone(b)\n",
    "print(backbone_.shape, clip_.shape, blur_[0].shape, blur_[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397c0d7-52a3-4153-823b-c27d2eb3eeba",
   "metadata": {},
   "source": [
    "### Adding diffusion prior + unCLIP if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 52\n",
    "    heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = VersatileDiffusionPriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)\n",
    "    \n",
    "    # prep unCLIP\n",
    "    if visualize_prior:\n",
    "        config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "        config = OmegaConf.to_container(config, resolve=True)\n",
    "        unclip_params = config[\"model\"][\"params\"]\n",
    "        network_config = unclip_params[\"network_config\"]\n",
    "        denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "        first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "        conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "        sampler_config = unclip_params[\"sampler_config\"]\n",
    "        scale_factor = unclip_params[\"scale_factor\"]\n",
    "        disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "        offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "        first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "        sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "        diffusion_engine = DiffusionEngine(network_config=network_config,\n",
    "                               denoiser_config=denoiser_config,\n",
    "                               first_stage_config=first_stage_config,\n",
    "                               conditioner_config=conditioner_config,\n",
    "                               sampler_config=sampler_config,\n",
    "                               scale_factor=scale_factor,\n",
    "                               disable_first_stage_autocast=disable_first_stage_autocast)\n",
    "        # set to inference\n",
    "        diffusion_engine.eval().requires_grad_(False)\n",
    "        diffusion_engine.to(device)\n",
    "\n",
    "        ckpt_path = '/weka/proj-fmri/shared/mindeyev2_dataset/unclip6_epoch0_step110000.ckpt'\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "        image = images[:1].to(device)\n",
    "        batch={\"jpg\": image,\n",
    "              \"original_size_as_tuple\": torch.ones(image.shape[0], 2).to(device) * 768,\n",
    "              \"crop_coords_top_left\": torch.zeros(image.shape[0], 2).to(device)}\n",
    "        out = diffusion_engine.conditioner(batch)\n",
    "        vector_suffix = out[\"vector\"].to(device)\n",
    "        print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 1400\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "454,409,880 total\n",
      "454,409,880 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "# model.backbone.requires_grad_(False)\n",
    "\n",
    "if use_prior:\n",
    "    opt_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ])\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    if num_iterations_per_epoch==0:\n",
    "        num_iterations_per_epoch=1\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ea5cd-f7ba-4f15-8056-3cd2535bca97",
   "metadata": {},
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cc7b19fa-0c75-4786-b24a-3b51e1737918",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'rtmindeye'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "607a7c7b-fe5e-41a4-80bf-d2814b3a57cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load multisubject stage1 ckpt if set\n",
    "if multisubject_ckpt is not None and not resume_from_ckpt:\n",
    "    load_ckpt(\"last\",outdir=multisubject_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,multisubj_loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "00ea5ae0-5c92-4276-af5b-25a17ba4dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(multisubject_ckpt+'/last.pth', map_location='cpu')\n",
    "# state_dict = checkpoint['model_state_dict']\n",
    "# model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "\n",
    "model, optimizer, train_dl, lr_scheduler = accelerator.prepare(model, optimizer, train_dl, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing__ starting with epoch 0 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█████████████████████████████████▍                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 1/20 [00:05<01:53,  5.99s/it, test/blurry_pixcorr=0, test/loss=4.43, test/loss_clip_total=4.43, test/loss_prior=0, test/num_steps=7, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0357, test/test_fwd_pct_correct=0.0298, train/blurry_pixcorr=0, train/bwd_pct_correct=0.129, train/fwd_pct_correct=0.125, train/loss=2.17, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=2.17, train/loss_prior=0, train/lr=0.000154, train/num_steps=70, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 2/20 [00:12<01:49,  6.07s/it, test/blurry_pixcorr=0, test/loss=3.87, test/loss_clip_total=3.87, test/loss_prior=0, test/num_steps=14, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0476, test/test_fwd_pct_correct=0.0417, train/blurry_pixcorr=0, train/bwd_pct_correct=0.166, train/fwd_pct_correct=0.412, train/loss=2.25, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=2.25, train/loss_prior=0, train/lr=0.0003, train/num_steps=140, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 3/20 [00:18<01:43,  6.09s/it, test/blurry_pixcorr=0, test/loss=3.6, test/loss_clip_total=3.6, test/loss_prior=0, test/num_steps=21, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0476, test/test_fwd_pct_correct=0.0357, train/blurry_pixcorr=0, train/bwd_pct_correct=0.207, train/fwd_pct_correct=0.452, train/loss=1.93, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=1.93, train/loss_prior=0, train/lr=0.000298, train/num_steps=210, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 4/20 [00:24<01:36,  6.03s/it, test/blurry_pixcorr=0, test/loss=4, test/loss_clip_total=4, test/loss_prior=0, test/num_steps=28, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0357, test/test_fwd_pct_correct=0.0536, train/blurry_pixcorr=0, train/bwd_pct_correct=0.302, train/fwd_pct_correct=0.448, train/loss=1.5, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=1.5, train/loss_prior=0, train/lr=0.000291, train/num_steps=280, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 5/20 [00:30<01:29,  5.98s/it, test/blurry_pixcorr=0, test/loss=3.88, test/loss_clip_total=3.88, test/loss_prior=0, test/num_steps=35, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0536, test/test_fwd_pct_correct=0.0298, train/blurry_pixcorr=0, train/bwd_pct_correct=0.421, train/fwd_pct_correct=0.502, train/loss=1.25, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=1.25, train/loss_prior=0, train/lr=0.00028, train/num_steps=350, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 6/20 [00:35<01:23,  5.95s/it, test/blurry_pixcorr=0, test/loss=4.02, test/loss_clip_total=4.02, test/loss_prior=0, test/num_steps=42, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0179, test/test_fwd_pct_correct=0.0595, train/blurry_pixcorr=0, train/bwd_pct_correct=0.457, train/fwd_pct_correct=0.5, train/loss=1.14, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=1.14, train/loss_prior=0, train/lr=0.000265, train/num_steps=420, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 7/20 [00:41<01:16,  5.90s/it, test/blurry_pixcorr=0, test/loss=3.76, test/loss_clip_total=3.76, test/loss_prior=0, test/num_steps=49, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0417, test/test_fwd_pct_correct=0.0536, train/blurry_pixcorr=0, train/bwd_pct_correct=0.607, train/fwd_pct_correct=0.65, train/loss=1.05, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=1.05, train/loss_prior=0, train/lr=0.000246, train/num_steps=490, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                                                                                                                                              | 8/20 [00:47<01:10,  5.86s/it, test/blurry_pixcorr=0, test/loss=3.76, test/loss_clip_total=3.76, test/loss_prior=0, test/num_steps=56, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0417, test/test_fwd_pct_correct=0.0417, train/blurry_pixcorr=0, train/bwd_pct_correct=0.645, train/fwd_pct_correct=0.693, train/loss=0.865, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=0.865, train/loss_prior=0, train/lr=0.000225, train/num_steps=560, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                                                                                                                             | 9/20 [00:53<01:04,  5.84s/it, test/blurry_pixcorr=0, test/loss=3.73, test/loss_clip_total=3.73, test/loss_prior=0, test/num_steps=63, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0476, test/test_fwd_pct_correct=0.0357, train/blurry_pixcorr=0, train/bwd_pct_correct=0.655, train/fwd_pct_correct=0.666, train/loss=0.832, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=0.832, train/loss_prior=0, train/lr=0.000201, train/num_steps=630, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                                                                           | 10/20 [00:59<00:58,  5.81s/it, test/blurry_pixcorr=0, test/loss=3.77, test/loss_clip_total=3.77, test/loss_prior=0, test/num_steps=70, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0595, test/test_fwd_pct_correct=0.0476, train/blurry_pixcorr=0, train/bwd_pct_correct=0.679, train/fwd_pct_correct=0.675, train/loss=0.752, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=0.752, train/loss_prior=0, train/lr=0.000176, train/num_steps=700, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                          | 11/20 [01:04<00:52,  5.80s/it, test/blurry_pixcorr=0, test/loss=3.82, test/loss_clip_total=3.82, test/loss_prior=0, test/num_steps=77, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0595, test/test_fwd_pct_correct=0.0536, train/blurry_pixcorr=0, train/bwd_pct_correct=0.673, train/fwd_pct_correct=0.691, train/loss=0.701, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=0.701, train/loss_prior=0, train/lr=0.00015, train/num_steps=770, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                         | 12/20 [01:10<00:46,  5.79s/it, test/blurry_pixcorr=0, test/loss=3.98, test/loss_clip_total=3.98, test/loss_prior=0, test/num_steps=84, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0417, test/test_fwd_pct_correct=0.0357, train/blurry_pixcorr=0, train/bwd_pct_correct=0.686, train/fwd_pct_correct=0.677, train/loss=0.631, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=0.631, train/loss_prior=0, train/lr=0.000124, train/num_steps=840, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                        | 13/20 [01:16<00:40,  5.79s/it, test/blurry_pixcorr=0, test/loss=4.28, test/loss_clip_total=4.28, test/loss_prior=0, test/num_steps=91, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0536, test/test_fwd_pct_correct=0.0417, train/blurry_pixcorr=0, train/bwd_pct_correct=0.723, train/fwd_pct_correct=0.702, train/loss=0.542, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=0.542, train/loss_prior=0, train/lr=9.87e-5, train/num_steps=910, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                       | 14/20 [01:22<00:35,  5.83s/it, test/blurry_pixcorr=0, test/loss=4.32, test/loss_clip_total=4.32, test/loss_prior=0, test/num_steps=98, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0655, test/test_fwd_pct_correct=0.0357, train/blurry_pixcorr=0, train/bwd_pct_correct=0.714, train/fwd_pct_correct=0.698, train/loss=0.563, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=0.563, train/loss_prior=0, train/lr=7.5e-5, train/num_steps=980, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                     | 15/20 [01:28<00:29,  5.87s/it, test/blurry_pixcorr=0, test/loss=4.48, test/loss_clip_total=4.48, test/loss_prior=0, test/num_steps=105, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0655, test/test_fwd_pct_correct=0.0417, train/blurry_pixcorr=0, train/bwd_pct_correct=0.691, train/fwd_pct_correct=0.702, train/loss=0.552, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=0.552, train/loss_prior=0, train/lr=5.36e-5, train/num_steps=1050, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                    | 16/20 [01:34<00:23,  5.89s/it, test/blurry_pixcorr=0, test/loss=4.64, test/loss_clip_total=4.64, test/loss_prior=0, test/num_steps=112, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0655, test/test_fwd_pct_correct=0.0417, train/blurry_pixcorr=0, train/bwd_pct_correct=0.736, train/fwd_pct_correct=0.709, train/loss=0.51, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=0.51, train/loss_prior=0, train/lr=3.51e-5, train/num_steps=1120, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                   | 17/20 [01:40<00:17,  5.91s/it, test/blurry_pixcorr=0, test/loss=4.73, test/loss_clip_total=4.73, test/loss_prior=0, test/num_steps=119, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0536, test/test_fwd_pct_correct=0.0476, train/blurry_pixcorr=0, train/bwd_pct_correct=0.713, train/fwd_pct_correct=0.675, train/loss=0.531, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=0.531, train/loss_prior=0, train/lr=2.01e-5, train/num_steps=1190, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                  | 18/20 [01:46<00:11,  5.90s/it, test/blurry_pixcorr=0, test/loss=4.81, test/loss_clip_total=4.81, test/loss_prior=0, test/num_steps=126, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0595, test/test_fwd_pct_correct=0.0476, train/blurry_pixcorr=0, train/bwd_pct_correct=0.73, train/fwd_pct_correct=0.705, train/loss=0.502, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=0.502, train/loss_prior=0, train/lr=9.06e-6, train/num_steps=1260, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                 | 19/20 [01:52<00:05,  5.93s/it, test/blurry_pixcorr=0, test/loss=4.86, test/loss_clip_total=4.86, test/loss_prior=0, test/num_steps=133, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0595, test/test_fwd_pct_correct=0.0476, train/blurry_pixcorr=0, train/bwd_pct_correct=0.73, train/fwd_pct_correct=0.718, train/loss=0.497, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=0.497, train/loss_prior=0, train/lr=2.29e-6, train/num_steps=1330, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [01:58<00:00,  5.90s/it, test/blurry_pixcorr=0, test/loss=4.86, test/loss_clip_total=4.86, test/loss_prior=0, test/num_steps=140, test/recon_cossim=0, test/recon_mse=0, test/test_bwd_pct_correct=0.0595, test/test_fwd_pct_correct=0.0476, train/blurry_pixcorr=0, train/bwd_pct_correct=0.73, train/fwd_pct_correct=0.711, train/loss=0.496, train/loss_blurry_cont_total=0, train/loss_blurry_total=0, train/loss_clip_total=0.496, train/loss_prior=0, train/lr=1.2e-8, train/num_steps=1400, train/recon_cossim=0, train/recon_mse=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "\n",
      "===Finished!===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "test_image, test_voxel = None, None\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "skip_train = True if epoch>=(num_epochs-1) else False # skip training if you are resuming from a fully trained model\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    loss_blurry_cont_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. \n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i, behav in enumerate(train_dl):  \n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0.\n",
    "            \n",
    "            behav = behav[0]\n",
    "\n",
    "            image = images[behav.long().cpu()].to(device)\n",
    "            voxel = vox[behav.long().cpu()]\n",
    "            # voxel = (voxel - train_mean) / train_std\n",
    "            voxel = torch.Tensor(voxel).unsqueeze(1).to(device)\n",
    "\n",
    "            if use_image_aug: \n",
    "                image = img_augment(image)\n",
    "\n",
    "            clip_target = clip_img_embedder(image)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                voxel, perm, betas, select = utils.mixco(voxel)\n",
    "\n",
    "            voxel_ridge = model.ridge(voxel,0) #[model.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "            # voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "            backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge)\n",
    "\n",
    "            if clip_scale>0:\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if use_prior:\n",
    "                loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                loss_prior_total += loss_prior.item()\n",
    "                loss_prior *= prior_scale\n",
    "                loss += loss_prior\n",
    "\n",
    "                recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                recon_mse += mse(prior_out, clip_target).item()\n",
    "\n",
    "            if clip_scale>0:\n",
    "                if epoch < int(mixup_pct * num_epochs):                \n",
    "                    loss_clip = utils.mixco_nce(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006,\n",
    "                        perm=perm, betas=betas, select=select)\n",
    "                else:\n",
    "                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=epoch_temp)\n",
    "\n",
    "                loss_clip_total += loss_clip.item()\n",
    "                loss_clip *= clip_scale\n",
    "                loss += loss_clip\n",
    "\n",
    "            if blurry_recon:     \n",
    "                image_enc_pred, transformer_feats = blurry_image_enc_\n",
    "\n",
    "                image_enc = autoenc.encode(2*image-1).latent_dist.mode() * 0.18215\n",
    "                loss_blurry = l1(image_enc_pred, image_enc)\n",
    "                loss_blurry_total += loss_blurry.item()\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    image_enc_shuf = image_enc[perm]\n",
    "                    betas_shape = [-1] + [1]*(len(image_enc.shape)-1)\n",
    "                    image_enc[select] = image_enc[select] * betas[select].reshape(*betas_shape) + \\\n",
    "                        image_enc_shuf[select] * (1 - betas[select]).reshape(*betas_shape)\n",
    "\n",
    "                image_norm = (image - mean)/std\n",
    "                image_aug = (blur_augs(image) - mean)/std\n",
    "                _, cnx_embeds = cnx(image_norm)\n",
    "                _, cnx_aug_embeds = cnx(image_aug)\n",
    "\n",
    "                cont_loss = utils.soft_cont_loss(\n",
    "                    nn.functional.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    temp=0.2)\n",
    "                loss_blurry_cont_total += cont_loss.item()\n",
    "\n",
    "                loss += (loss_blurry + 0.1*cont_loss) * blur_scale #/.18215\n",
    "\n",
    "            if clip_scale>0:\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            if blurry_recon:\n",
    "                with torch.no_grad():\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    blurry_pixcorr += pixcorr.item()\n",
    "            \n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "                \n",
    "            if train_i >= num_iterations_per_epoch-1:\n",
    "                break\n",
    "                \n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, behav in enumerate(test_dl):  \n",
    "                behav = behav[0]\n",
    "\n",
    "                loss=0.\n",
    "\n",
    "                if behav.ndim==2:\n",
    "                    image = images[behav[:,0].long().cpu()].to(device)\n",
    "                    voxel = vox[behav.long().cpu()].mean(1)\n",
    "                else:\n",
    "                    image = images[behav.long().cpu()].to(device)\n",
    "                    voxel = vox[behav.long().cpu()]\n",
    "                    \n",
    "                voxel = torch.Tensor(voxel).unsqueeze(1).to(device)\n",
    "\n",
    "                clip_target = clip_img_embedder(image.float())\n",
    "                \n",
    "                voxel_ridge = model.ridge(voxel,0)\n",
    "\n",
    "                backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge)\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                    clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                # for some evals, only doing a subset of the samples per batch because of computational cost\n",
    "                random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                \n",
    "                if use_prior:\n",
    "                    loss_prior, contaminated_prior_out = model.diffusion_prior(text_embed=backbone[random_samps], image_embed=clip_target[random_samps])\n",
    "                    test_loss_prior_total += loss_prior.item()\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                    \n",
    "                    if visualize_prior:\n",
    "                        # now get unCLIP prediction without feeding it the image embed to get uncontaminated reconstruction\n",
    "                        prior_out = model.diffusion_prior.p_sample_loop(backbone[random_samps].shape, \n",
    "                                        text_cond = dict(text_embed = backbone[random_samps]), \n",
    "                                        cond_scale = 1., timesteps = timesteps)\n",
    "\n",
    "                        test_recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target[random_samps]).mean().item()\n",
    "                        test_recon_mse += mse(prior_out, clip_target[random_samps]).item()\n",
    "                        \n",
    "                if clip_scale>0:\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006)\n",
    "\n",
    "                    test_loss_clip_total += loss_clip.item()\n",
    "                    loss_clip = loss_clip * clip_scale\n",
    "                    loss += loss_clip\n",
    "\n",
    "                if blurry_recon:\n",
    "                    image_enc_pred, _ = blurry_image_enc_\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    test_blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    # forward and backward top 1 accuracy        \n",
    "                    labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                    test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                    test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            # if utils.is_interactive(): clear_output(wait=True)\n",
    "            if skip_train: break\n",
    "            print(\"---\")\n",
    "\n",
    "            # assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"train/loss_blurry_cont_total\": loss_blurry_cont_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            # if finished training, save jpg recons if they exist\n",
    "            if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                if blurry_recon:    \n",
    "                    image_enc = autoenc.encode(2*image[:4]-1).latent_dist.mode() * 0.18215\n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc_pred[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "\n",
    "                    plt.show()\n",
    "                        \n",
    "                if use_prior and visualize_prior: # output recons every ckpt\n",
    "                    idx = np.random.randint(0, 3)\n",
    "                    print(f\"reconstructing... idx={idx}\")\n",
    "                    samples = utils.unclip_recon(prior_out[[idx]],\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix)\n",
    "                    if utils.is_interactive():\n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(image[idx]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(samples[0]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint and reconstruct\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt(f'last')\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5702acf6-45fe-44f5-8842-c0e2d4d8e8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa6fd4b4590>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGeCAYAAABGlgGHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAACUlklEQVR4nO2de5wcZZnvf9X3uV+TmUzuN5IACYQgEFDRAy4oq6C7eMyJhLOLelTcBd1lFfe4uroaXJblqLiIruiugCgriLpcRCRBJBDIBRIuIYQkk9tkksncL32prvNH1fvWW9XV3VXVb013zzzfzycfyEynpqanp+up3/N7fo+iaZoGgiAIgiCIMhEq9wkQBEEQBDG9oWKEIAiCIIiyQsUIQRAEQRBlhYoRgiAIgiDKChUjBEEQBEGUFSpGCIIgCIIoK1SMEARBEARRVqgYIQiCIAiirFAxQhAEQRBEWYmU+wTckM1mcfToUTQ0NEBRlHKfDkEQBEEQLtA0DcPDw+jq6kIoVED/0Dzw5S9/WQNg+bNs2bK8j//Rj36U8/h4PO7lS2qapmmHDh3KOQ79oT/0h/7QH/pDf6rjz6FDhwpe5z0rI2eccQZ+97vf8b9HIoUP0djYiD179vC/+1E2GhoaAACHDh1CY2Oj539PEARBEMTkMzQ0hLlz5/LreD48FyORSASdnZ2uH68oiqfH5zsGoBc2VIwQBEEQRHVRTIjwbGDdu3cvurq6sGjRIqxfvx7d3d0FHz8yMoL58+dj7ty5uPLKK/HKK68U/RrJZBJDQ0OWPwRBEARBTE08FSPnn38+fvzjH+Oxxx7DnXfeif379+Md73gHhoeHHR+/bNky3H333Xj44Ydxzz33IJvN4sILL8Thw4cLfp2NGzeiqamJ/5k7d66X0yQIgiAIoopQNE3T/P7jgYEBzJ8/H//6r/+K6667rujj0+k0VqxYgXXr1uFrX/ta3sclk0kkk0n+d9ZzGhwcpDYNQRAEQVQJQ0NDaGpqKnr9Lmm0t7m5GaeddhrefPNNV4+PRqNYvXp10cfH43HE4/FSTo0gCIIgiCqhpNCzkZER7Nu3D7NmzXL1eFVVsWvXLtePJwiCIAhi6uOpGPnbv/1bbN68GQcOHMCzzz6LD37wgwiHw1i3bh0AYMOGDbj55pv547/61a/it7/9Ld566y1s374dH/3oR3Hw4EF87GMfk/tdEARBEARRtXhq0xw+fBjr1q1DX18fZsyYgbe//e147rnnMGPGDABAd3e3JWGtv78fH//4x9HT04OWlhasWbMGzz77LE4//XS53wVBEARBEFVLSQbWycKtAYYgCIIgiMrB7fWbFuURBEEQBFFWqBghCIIgCKKsUDFCEARBEERZoWKEIAiCIIiyQsWIA6PJDO7avA8HTo6W+1QIgiAIYspDxYgDj+7uwcZHX8e3ntxb7lMhCIIgiCkPFSMODI2nAQD9Y6kynwlBEARBTH2oGHFAzerRKxNptcxnQhAEQRBTHypGHEhnswCAiXS2zGdCEARBEFMfKkYcUFVSRgiCIAhisqBixIG00aZJZkgZIQiCIIigoWLEgYzK2jSkjBAEQRBE0FAx4gAZWAmCIAhi8qBixIE094xQm4YgCIIggoaKEQdUNk2TUaFpWpnPhiAIgiCmNlSMOMAMrJoGpFRSRwiCIAgiSKgYcSAjFCDUqiEIgiCIYKFixIFM1mzNJMnEShAEQRCBQsWIAxnVLEZIGSEIgiCIYKFixIFMVmjTZEgZIQiCIIggoWLEAasyQsUIQRAEQQQJFSMOiJ4RatMQBEEQRLBQMeKAtRghZYQgCIIggoSKEQeso71UjBAEQRBEkFAx4oDFM0KbewmCIAgiUKgYccAyTUPKCEEQBEEEChUjDlDoGUEQBEFMHlSMOEChZwRBEAQxeVAx4gC1aQiCIAhi8oiU+wQqEauBlYoRgiAIYuqy8ZHXAAX4zLuXoCERLcs5kDLiAIWeEQRBEJXEb14+iiu/+0cc7h+Tfuzv/+Et3LX5rbJe76gYcYByRgiCIIhK4sHtR/DSoQE8trtH6nGzWQ2acf8dCSlSj+0FKkYcIGWEIAiCqCTSxk3ywT65ykha8EiGw1SMVBSWYoQ8IwRBEESZYV7Gg6fkFiOqcL2LhspXElAx4kBaaNNQzghBEARRbljRcLBvVOpxxZvvMLVpKguV2jQEQRBEBcEiJw73j1tumEs+rjA9Sp6RCsMaekbKCEEQBFFemIKhZjUc6R+XeFy9sFEUIETFSGVhCT0jzwhBEARRZsSbZJm+EdYJKKdfBKBiJIdsVoPQpaE2DUEQBOEaTdOKP8gHon1Apm+EFTnl9IsAVIzkII45AdSmIQiCINyx+8ggzv7qE/jhM/ulH1tU7A+clKeMsPZPOf0iABUjOYjVJ0DKCEEQBOGO7d39GBxP48Hth6UfW5x66T4lTxlRjSInUsaMEYCKkRzSqrUYodFegiAIwg2s5fF6zzDGU3KvHaJn5IDE4DNW5ITJM1JZZGwjU2RgJQiCINzAWilqVsMrRwelHltU7bv7xnJUfL+wIofaNBWG/QecVjVpP3SCIAhi6iK2UnYeGpB8bPNGOaVm0TM0Iem4ZGCtSNIOhQeZWAmCIIhiqGqQxYh+bFYzyJqoYZ6RKHlGKgv2YqqJhvnHqBghCIIgipEOUBlh16Z5rbUA5C3Mo9HeCiUtVImxiP70TGRoooYgCIIojCq0Ug73j+PkSFLasZkysnhGPQDggCRlxBztJQNrRcGqxGg4hAQrRkgZIQiCIIqQsbX5X5KojjDPyOKZejFyUFLWCC9GqE1TWbAfeDikIGG0aqgYIQiCIIqRUYMsRpgyUgdAXiQ8zxmhNk1lYVFGeDFCbRqCIAiiMGzysqU2CgDYIakYyWY1sJR51qY52DcqJXqePCMVijjmlIjqTw8FnxEEQRDFSBs5VWvmtwLQlREpBYPQ/lnQXgdFAcZSKk5I8KSQZ6RCYaFnkbDQpqHgM4IgCKIITBk5o6sR8UgIQxMZ7D9ZutFUzBipjYXR1VQDQA8/szM4lsap0ZSHY5NnpCLJCOuUExFq0xAEQRDuYNePRDSMM2c3AQBeOjyQ87hkRsXuI4OuVRNRGQmHFCxo18d77bHwaTWLD/7bH3HJbZswlsq4OrYq+CTLCRUjNsQ2TTxK0zQEQRCEO7iyHlJw9txmAMDO7oGcx3354Vfwp995BpveOOHquGKYWjQUwrxWw8RqG+995s2TeOvkKPrH0ugZdJfQmqY4+MqEvZiiYpuGlBGCIAiiCGLL4yxWjNhMrGOpDB7eeRQAsK93xNVxWf6VogChkMInarbs67M87uEdR/j/j7u8iVb5OZNnpKKwGlhptJcgCIJwB7+whxScO78FAPDykUEc7jfbKb97rZcXCm43+4rHBYD3n9WFSEjBiwf7sfuIvpBvNJnB468c5//G7bEzWVJGKhK+wVAMPSMDK0EQBFGENB+TDaGruQZvX9IOTQN+urWbP+bXLx3l/z/m8kbXPn7b0ZjA+1bOAgDc/cf9AIAnXj1uUUNcKyMqeUYqkkyW2jQEQRBTmVsffx3f27xPytitCA8QMyZT1p8/DwDwsxcOIZXJYnA8jc17TJ+IV2UkKozf/sVFCwAAv3npGHqHJ/DLnUcs/2asypSRSFm/egWSESpbyhkhCIKYWvQMTuC7T+0DAKQzWfzVJUulHdt+Yb/09A7MbIijdziJ377ag/GUipRq3ty6b6UY6oUwfrt6XgtWz2vGju4BfPvJvfjD3pMAgDktNTjcP+69TUOekcqCKyPkGSEIgphyiO/ntz3xBn6x7XDex7q9oDPs7ZRoOISPnKerI/c8dxC/fvkYAKC9Pq4f322bJo968RcXLTSO3Q01q+GsOU1Y3tno6dh2P0q5oGLEhpOB1e0PlSAIgqhs7MvsPv+Ll/HHN0/mPG7bwVM48yuP419/u8f1sXk7RVAZPvK2uQgpwHNvncIze/UWzYfOmQ3AQyuFj99aL9nvPbMTnY0J/verVs9GbSzs6dhp8oxUJuJumjjf2kueEYIgiKkAU79b62J4/1ldyGQ1fPIn29A7bM3l+MX2I1CzGrY75ITkI+0QINbVXINLVnQAALIacObsRqyY1QAAGE+7CyYTb5JFouEQrlk7n3/uT1d1ocajok/KSIWSdoqDJ2WEIAhiSmDecCr4l6tXYXlnA4aTGTxitFAAQNM0bHq9F4A3ZdxURqwXdmZkBYD3r+pCTVS3a7o3sFqNsSIfPX8+zl/Yiv/zzkWY0RBHDVdGvBU65BmpMFSnnJEMKSMEQRBTAXExXDwSxp+vmQMAeOyVHv6YPceHcdRIMHXb7gCsAxAi71w6A6fPakRDPIIPnN3luZVSaLNuU20UP/s/a/F3ly8HAKEYIWWkqrHspqE4eIIgiCmFuAwVAC47oxMAsHX/KfQZW3Cfel0cv3WnMABmC8h+YQ+FFPz8k2ux6aZ3YVZTDS8YvLZSoi4269Z6VPTJM1KhWNo0xqI8Gu0lCIKYGth3scxtrcWZsxuR1fTgMAB4ak8vf7wnZaSAylAfj6DNmKJhvg7XJtM8nhEnfCsj1KapLETJikLPCIIgphZOEy+XG+rIY6/0YHA8jW0H+/nnvIz3mhf2wkUDKxjcj9/m94zkPXaVhZ5RMWIjLcbBRykOniAIolLRNA23PKqnqbrFaeLl8jP1YuSPb57EI7uOQc1qaK6NAvBmYM3nGbFT67Vg8LBZt9ZjoZOpxjbNV77yFSiKYvmzfPnygv/mgQcewPLly5FIJLBy5Uo88sgjJZ1w0KhCz4+maQiCICqXQ6fG8b3N+3Dr43uQteWH5EPcP8ZYMrMBS2bWI61q+JfH9VyRy07XC5RMVkPK5RBDPs+InVpjmsbtsUXTbTG8toCqVhk544wzcOzYMf7nmWeeyfvYZ599FuvWrcN1112HHTt24KqrrsJVV12F3bt3l3TSQWIqI4pgYKU2DUEQRKWx7+QIAL094lbBVoWUbRHWqukbTQEA3rdqFv+c5+26RdopiZh56XWjYOTLGXGiJuZ1bLhKPSORSASdnZ38T3t7e97Hfutb38Lll1+Om266CStWrMDXvvY1nHPOObjjjjtKOukgMStbfewLIGWEIAiiEtl/YpT/v/vEUecLO2vVALrZdO2iNv6YMZfhZHZzbD5i4RA/tpuiwZNnxGNyeNUqI3v37kVXVxcWLVqE9evXo7u7O+9jt2zZgksvvdTyscsuuwxbtmzxfqaThJOBNZnJSt/uSBAEQZTG/pNmMeJ16VzUpgSc0dWIOS01AIB3LG1HLBLiY7KeVYYi7RRFUTwVDb48I679KFXoGTn//PPx4x//GI899hjuvPNO7N+/H+94xzswPDzs+Pienh50dHRYPtbR0YGenh7HxzOSySSGhoYsfyYLJwMroBckBEEQROVgKUZcGzadWymKomCDEa3+4bfNBeB9TNZLZoeXpFSzTePCM+IxgTVfauxkE/Hy4Pe+9738/1etWoXzzz8f8+fPx89//nNcd9110k5q48aN+Md//Edpx/OCkzIC6K0a8e8EQRBEeRGLERmGzY+/YxE+esF81Bq+C6+TKW49I5ZjuzhvL62UGo+RFF4KnSAp6as3NzfjtNNOw5tvvun4+c7OThw/ftzysePHj6Ozs9Px8Yybb74Zg4OD/M+hQ4dKOU1PiKFnUaGvRyZWgiCIymEireLIwDj/u9e2hFMrRVEUXogAphnUTaGjaZqvqRc3hY6quveMsCInpWb591rw2NXqGREZGRnBvn37MGvWLMfPr127Fk8++aTlY0888QTWrl1b8LjxeByNjY2WP5OFfewrEaFIeIIgiEpDVEUA7xtw3ZlB9fd/N5HwqjBa7ErB8NAC8qKMiAr+mIvrVlXGwf/t3/4tNm/ejAMHDuDZZ5/FBz/4QYTDYaxbtw4AsGHDBtx888388TfccAMee+wx3HbbbXj99dfxla98BS+++CI+85nPyP0uJGL/oZvL8qgYIQiCqBTsxYjXpXN2A6sTTCXxMn4LeJt6cXOj66WVEo+EwOqKCVeTOpXhGfFUjBw+fBjr1q3DsmXL8OEPfxhtbW147rnnMGPGDABAd3c3jh0z1zBfeOGFuO+++/D9738fZ511Fv7rv/4Lv/zlL3HmmWfK/S4kYg+toUh4giCI8nJ8aALPvnnSoj7kKCOu97z4MZm6v6gD7to0Xjb3emmliJM6XlSXcntGPBlY77///oKf37RpU87Hrr76alx99dWeTqqc2A1IcdrcSxAEMen0Dk/gZ1sP4XevHcdLhwcBAP/3ihX42DsWAQDeOmFv07g0marulQBPJlPVLEZkh5PlmwAqdOzRlCp9bDhIaDeNjbTN3JSg4DOCIIhJ54af7sRtT7zBCxEAeGSXqbzvN9JXvSgMgLkBV7Z6wVR1/dge/Ciu2jTuYuYZ3s7bW6ETFFSM2MjYqmaKhCcIgph8eocnAAB/9T+W4FefuQgAsPPQAPqNuHbWplkxSx9wCCLkK+Fl4sW4qIcUIOQqnIxN6sjNGQGESR0P6a5VZWCdDth/6GYKKykjBEEQkwV7L37XshlYNacZyzoakNWAp/eeQP9oCv1jaQDAilkNALxngchu03hRXACh0El5GL913abxkO7q8byDgooRGxnbDgDa3EsQBDH5mF4G/TL1rmX6oMTmPSfwlqGKzGpKoLUuDsB94mjaYWtvPryoF6pHX4cZqOZCGfHo6zANrO6PTcpIhWH/oVObhiAIYvJJ24K+LmbFyBsnsO+E7hdZ2F4nqBduE0fd+y+8TaV4a3d4S2D15xlxcxNdlaO90wG7ZEUGVoIgiNLQNM3zeyh7L44ZCsa581tRFwujbzSFX790FICtGPEaeubBwCo7sh0wVfcgxm8TPoy3pIxUGMzcFOWjvZQzQhAEUQpffGg3zvnaEzh0asz1v0lnmDKiX6ZikRAuWtIOAPjD3pMA9GLEy0UdEOLg3QSTefFeeGj/AN723nhuAXkpdFTyjFQkZgVqa9OQgZUgCMIXT7zag7GUileOut/AnnZoTbx7+UzLYxbPqPekXgDe/Bd+2jRefR3uzLHBtWlotLdCsUcFk4GVIAjCP30jSZwc0cdxR5PuWimAc2w7M7EyrG0aj1t7vcTBB5DX4Sfd1W0rxUubZkosypuK5EzTRKhNQxAE4Zc3jo/w/x91OfFi2YArXNxnNdVgWYc+yhsJKZjTUuO9TZO1tuILwQsGF34U1eOIrJ/dNK6VkaiXnTrkGalIchfl6U9RkpQRgiAIz7xxfJj///CEN5MpAERtF3emjsxrq0UkHPKkXgDmaK+bi6+nnBGP22/NsWEvnhGXhU6MbRsmz0jVYv/B0NZegiCI4iQzKr7661ex+Y0Tlo+LxYjbNo2458Xe9vjQOXNQGwvjsjM6AXgzggLCKKuLi6+3JFOPWSCegsk8+lFcFmj5FKhy4GlR3nTALllRzghBEERx/vjmSdz9x/148vXj2HzTu/nH/RQjaXHPi+0iuayzAbu/chmPXPcS8AXk5pcUgu94SavQNA2Kkv/f+PWMePGjuFZdouZ5F0IQoMgzUmmQgZUgCMI7Rwf0XTIH+8bQO6T/v6ZpFs/ISNLbxAvgrGCIu19qYqavLyteXYsc291mXf3YmgYkM4VvSM3juhztNa4tKTXLx43zYQaTuW3TsEKncIEmLvcjz0gF4SRZUTFCEARRHFaAAMDWA6f0jw0nMTie5h8fSaZz/p0T7OLsZukcUy8Ad+10Lxd25usAins72MK5qMc2DVC8VePVj+K2BWQp+lwWOkFBxYiAKlTVpoGVpmkIgiCKcXwoyf//hf16MSK2aABg1KUykvYwfssmHgF3ZlAvmR3hkIJYxDCDFruwe2ylxCMhsK5PsVaNZz+Kywkj0ShMykgFIf5g2C9BIkKhZwRBEMU4PiwqI/0AgD09ejHCvHcjrg2s7lWGUEjhx/c0PeLW2xF12fLweFxFUXirRnahw0PPXBY5ABAu4IeZDKgYEcgUUEaSpIwQBEHkRVRGXu8ZwuB4misjq2Y3A/BgYGXFSMRttLqXXA1vo6y1LgPEvB4XcB985tkz4tLA6qUdFjRUjAiIJqLcNg0pIwRBTG00rbgBNB/MMxILh6BpwPaD/dy8unp+MwAvxYi/ADF3u1jcT9MA7gsGflwPF3W3x/aSjeLluF7SaIOm/GdQQaTV3P5Z3KjMizmpCYIgqpnNb5zAqn/8LX7z8lHP/zaZUdE3qke+v/M0fZnd8/tPYa+hjJwzrwUAMOwxZ8TtWns/Y7JelZFix/aT1+E2hVX1vJtGV4pSmaylFZN73MqIggeoGLEg/mDYPHmcJbCSZ4QgiCnMU6/3Yngigz++edLzvz0xrLdoYuEQ3nN6BwDg1y8dxWhKRTSs4IyuRgC6MuJGfUlnvakXZvBZ8WLHvvKj6LFdRqt7jYMHzHAy2QoGK3KAwuft1YsSJFSMCDiF4TCndlrVClaYBEEQ1czRgXEA7mPVRZhfZGZjHOctbAMAHDGOt3hGPZprYwD0kC03k4lcGXF5Yfeyn8bzsV23UrzveHFrYPW8KC9qfm+Ffp7cKExtmsrCqbKNCz9UUkcIgpiqHBvUPR9uF86JML9IR2MCC9pq0V4f559b2tGA2miYj7G6majx6uvwtkPGX5ppsWka1U+bxm04meqtnaIoiqsoe1JGKhQn+S4mVIw0UUMQxFTl2KChjPgw6x/nxUgciqLgvIUt/HPLOuoRCimoM1oSbkysab++DhfnrnrY2ise2/00jXwDq5/NurUuNg6TZ6RCcXJwR8Ih/oMiEytBEFORZEbFyRHdgOqrTWN4RmY2JAAAb1vQyj+3tKMBAFAX1y+OXpQRtwWDnzaN+w24bqdpvMXBA0KGics2jZd2SsKFMuKntRQUVIwI5KsSedYItWkIgpiC9AyagWV+2jTHhTYNYC1GlvFiRFdG3BQjaY8Fg6c2jefJFG8TL24LKPHYxc7ba2vJ7bH9FDlBUf4zqCDybXNk470UCU8QxFSELbkD/LVpeg0Da0ej7hVZMasRFyxqxdpFbZjXWgsAaIi7b9NkfI6yumvT+Etgddum8VIwuB1J9tNOsbeuJtIq/vvlY5ZdQeQZqVAyeapEM2uElBGCIKYezC8C+J2msSoj4ZCC+z+xFj/9xAU82dOLMmLfnl4Ms01T+NiapnkPVPM6fuulGHFISn3+rT4c7Bu1HduboRfIbV3dv7Ub19+3Hf+26U3+GPKMVCj5VkvHeZuGlBGCIKYexyxtGnfBZCL2YsQJb20av9M0hd+jnZahuj52kQwTr14U8dhsh0x33xg+8oPn8H9+ss3yOD8ZJnZl5HVjTxDLhAHIM1Kx5JMGuTJCbRqCIKYgLGME8N6mGU+pGJrQL9SsTeNEvYc2TdqjMmIaQYsUDJZlqHLTXb22loDcFtDrPUPQNLO4A6xqTiktoO5TYwD0VFaGSnHwlUm+ON847achCGIKIyojaVXjd8xuYBfO2liYFxxOmNM0biLbvZlBve5i0Y9dCYvyjBZQOn/BIGZteit0rO2lQ/36sUWF309rKSioGBHI5Okl0n4agiCmMqIyAnhTR8QWjVJgDX19PArAmzIifX+MUGS5XjrndvxW9WaMFY/N2jSHWDGiigWDsMDVU6Ca/tyNp1Wk1Sw3KYuFTj5rQjmgYkQg32w7GVgJgpjKiMoI4M3EamaM5G/RAEC9oYy4mqbxulnXZcGQ8eEZcb1ZV0IwGVNG0qqGrHGuGVU8Zy+eEWPCKJXBsYEJ3pKxFCM+xpGDgooRgXxjTvEIGVgJgpiajKUyfNyTvfd5yRrpdWFeBUwDq5vNvXyy0fXEi7dgMnEZajHMi7r88dt8vg7AVEfEAsqTZ0Qo0JyOK56zl6C2oCj/GVQQZpVofVrY0qEkeUYIgphiMPm+Ph5Ba52+0M6TMiJEwReizpOB1es0jbuCwdcyO4eo+UOnxvDCgVOWx5Uy2jueUpHNajjUb7bL2M2vnwkgwFqgMb8IYFdGyDNSkeQd7TWUkQlSRgiCmGKwjJFZTQnXUykix3ngWWFlxMs0jdeckSBj1WscMkyu+48XcPX3tvDNxPo5G4WOj9HesbSK3uGkpVBg/89ukkMKeGaLl2OPp2zKCHlGKp98buh4lEZ7CYKYmhwzlJFZzTWuJ0dEmDIy02Ux4maaxm9ke7GMFD/hYTU8Dj6LbFbDRFrFG8dHAJgtKkAodDxc2MX9MWLBAAhtGo9m3pxjF2zTVI5nJP8c1jSEDKwEQUw3jhrKSFdTAsMTunfES5um1zCwdhQxsHpp0/hdZscKhnwKQlr13pZghQ6gX9jFySOnloefFlAyk8UBW+pqytam8apeiIVl/2iqyDmXX5co/xlUEGRgJQhiusGVkaYaR39EITRNc5W+CojKiPytvayVAgATBW4a/SSZJiLmscdSKg70mSpD0qHl4UV1YV4XANh7fNjyObNN4/24gDA2bFNGkk6hZ9SmqSzy9SmZgZVCzwiCmGowZWRWc8L1UjjGSDLDHzuzqIGVhZ65MLB6LBrEYqTQuXs1xgK6T0O8sIt7Y5zGZL0UOkx1B8y4dvux+Zizx4KBqUW9Q0n0j5nL8VJCseZHKQoKKkYESBkhCGK6wTJGuppqeCKo2zYNM682JCKWu3wnmDKSymSLJrymjffaaMTdRTIUUvhNY6Fz9zs9Ik6mHOxz9l9kfFzYxULnDbsyoqqWc/baSmE/j54ha4aMk2fEq+oSBFSMCBT3jFAxQhDE1EHTNBwbMJWRWpdTKQy3LRrA9IwAxX0jXnNGAHcTNX6W2YnHHktlLN4OxzFZr8c2Ch1W2DHso72eCyhBLQJ0TxBQus8lKKgYEcgnDcYpZ4QgiCnI0EQGo4aSoCsj7qZSGG4zRgC9/c1u7IYnCh/fTzvFTdaIn2V2+rHNqReLMiLBf2EvGmY311iO7dczIhpvAWDxzHoA+q6bTImTOkFQ/jOoINQ8cb4JatMQBDEFYRkjzbVR1MTCQiKou/c6njHSUFwZAYSskWIjuD4UDDcprH5MpoB5YR+aSOOwECCWFFoefgLVxGMDeruLeW9K9YwkbEXO4hn1/P/t6a7kGakwTAOrfWsvGVgJgph6iJM0AIQ2jTtlZGBMHxltMZJbi+F2vJenYfuKP89/bD+bdQGz0Nl7fMSyRddRGfE69SIUI/NaaxEzCjB7wVBKkQMAS2YKxQhvAbGgNipGKop8RiEysBIEMRURM0YA9zteGOyCKU6FFMJt8Fm6BGWkkKrjdWSYH9sodOwTL2L2lJ+xYfHYgFGMGM+lPWfES2oskKuMLGqvA1vHk9MCImWksiADK0EQ0wkzfdVfMeI1tt1tJLyfbbJuUljTvlUG/bxfOzZk+biojPjZ2gtYlZG5rbX8emMvGLweNxxSLEWieOxkhjwjFU0+AyurMCmBlSCIqQTPGGFtGp5k6u69jvkkYi6VEZ41UtTA6v0i6WaaRs2zDLXosY3nZX+elFQAUPO0+YtRaytGuDKiluYZEY8dCSmY1ZTI2wIiZaTCUPOYm3g1SbtpCIKYAmSzGu557iAe390DAJjTohcjXkPPUh4vlHUuU1gzPqZpaoSJl3z4DfliF3XN8Is0JMzMFIZfBaMmao48WzwjJSoj+rH1857dUoNIOISYYTmoRM8I7aYRyLeciY/2kjJCEESVc7BvFH/3Xy/j+f2nAABvW9CCy87oBAAeeua2GEkH1qbxrjK4WfJnbqn1p4wwTutowLaD/dbQM9/mWPPxomckJ2fEo5ojnvfclloAyG0BVVACKxUjAvnGybiBlZQRgiCqmGxWw4a7t+Jg3xhqomH83eXLsGHtAn7X7blNw5NSPRpYi4z2Bt+mKS0LZOnMer0Yseym8ZdmyvwoiqJnjNiLkXQJbRpejLTqxYj92H4LqCCgYkQgX2gNGVgJgpgKHB0cx8G+MUTDCh6/8Z2Y11Zr+byYNOoG7hlxeQHmbZoinhF/bZrc0DNN06Ao5jH8TOkAVl9HNKxgflsdANuiPJ/+C+ZJ7GrSC5FY2N5K8d+mqTVaQPNYMWJrAfkdRw6C8pdDFUS+1D/2YkmpWf7DIwiCqDb29o4AABa11+cUIoD/0V7ZbRqzyHF/ibK3aR7acRirv/YEtuzr44/xm8BaI+zdmdtai5qo9aKuH7u0Qmduq+7bsY/2ZnyO9gLAgnb9Z3zW3CbrsfneG38TQEFAxYhAMWUEsL74CIIgqok3j+vFyJKOesfP+52mcXuhrAsyZ8QWevbAi4cxMJbG8/vFYsSngVVo0yxoq0OcT1iWHge/vLMBAHDeglYAuQVDKcrIV688E4/d+A6sXdRmPbbNM+JlB1BQUJtGIO9uGqEYSWbUHDMTQRBENbC3Vw/tWjozTzFiyPppVUNazRYtMrwaWNlor9ucES8XdnGaJqNmsfPQAABbK0VCm8YpJVXTNN9Fw7uWzcQLf38p2uv1FNtck6l/z0giGsbyzkb+d3beds8IKSMVRr4+ZSQc4j8s8o0QBFEMNasVHDH1y9BEGv/jtk34yq9e8fXvWZtmSZ5iJCFMdrhp1Zg5I+4uZmwk1u1uGi+tCbFN83rPMD9/cfDA956XmKiMiCmprN1htu/9qAwzGuLc2yJztNdOvnRX8oxUGIWcxQnKGiEIwiXX/PB5XHjLkxieSEs97s7uAbx1YhSP7Drm+d9qmsbbNEtnNjg+JibceLlp1bCLmmtlJObOwOpna684TbPtYD//uBjJ4HsDrtCmmd9el/eiDpSe2WEPPfMbB+/m2H6X+wUBFSMChRzcrEc4QVkjBEEUIJXJ4rm3+tA/lrasm5fBkQE9MbVYaJgTvcNJDCczCIcUbmy0oygKv/B6UUa8e0Zc5ox4Ge0V2jQvWoqRXJOp1wt7rWBgXdBWl3f8Fig9s8Pepkmrk6CMVIBnpPxnUEEUMiBRCitBEG44MjDON7sOF1EAvHLUKEbGUqrnyb69hioyv62WZyc5kXCx44Xh9eLuOvQsTxp2IcT02O15ihG/SkCt4XUJGVkg8TwjskDpxUhu6Jl/z4gd+3lXUhw8GVgFCjm4zawRUkYIgsjPQWF/SbGLrleO9I/z/x9JZtBUE3X9b4uZVxleJmpY6JnbEVymjIymVGSzGkJ5LoI8DdtTAqt+7ONDExYPR1L4PnjLw+PFd0FbHf7H8plYPENXRVgqt33HC1C6gpFvtFeGryOfMkJx8BUG3xTpqIzkjnIRBEHYOXTKbM0UM2p6hbVpAD/FSGG/CMPLfpqURwWDGVgBYCytcqVERM1qfAeMlzYNK6IyNsXIqoz4m6YJhxTc/b/fxv9uDybLCK0UMWTND/ZJHZmtlHyekUoY7S3/GVQQhVzLCaMSdjt/TxDE9KRbKEb8eDsKwbbsAsVNoHZ4xkgRZcRL8JlXz0g8Yhpk852/xX/h4Y49YYtsb6vTR2UtBlZJhs1c9UJeK8V+bKmekTztJTKwVhiFZtBJGSEIwg2iaVVmm0bNajg2MMH/PpL0NqnD2jTFipFaF9tvGV6TUhVFQZ1x/HyFmmVM1sdoL2PtYj3oyymYzOtuGjv5jaBBtFLkFzq5S/ioGKkoChpYaXMvQRAusCoj8t4vTgwnLRfqIQ/KSN9IEv1jaSgKsHhGEWXECD4rtHCOwZURlzkjQHETa1ooHjyFntmUkYuWtAOwDh3kC7b0Cr+oq1b1ws9mXTvxiLMfRapnxHZsUkYqjEKz7TRNQxBEMTRNsxYjEqdpjgxYx4S9HJv5Rea21BZNkHbbptE0zXMCKwDUJ4oUI4YSoCjeLpKhkMLfpzsbE1jAl9nltmlKVkaEdoeYvipFvcjrR5HoGbGlu5JnpMIoNKbmtIuAIAhCpG80ZbmIy2zTHBFaNIA3P4ppXi2sigBmwNd4EfMtK0QAb8UIm6gZztemEfaleDWDslbNmgUt3OfnlDNS6oWdKeWArjLIDA+zt1Jkjt8Gme5aKlSMCBQyN7GKmwysBEHkQ1RFAGBE4jTNUWGSBvCmjLx53PCL5FmQJ8LDw4q814lGUy/bdYu1afxkjDDYeO+aeS2OPr9CwZZeEL/fVCYbTEqqzTMio2CwB6qRZ6RCKZT6RwZWgiCK0W1LXJWqjPRbi5F8yoITfCdNEb8IYN3xUgixGPHS9mCR8MXaNH6UgHmttQgpwDuWtptZII4JrHKLkSD3x8g6Z8C8jlViHDzljAgUqsgp9IwgiGIwZaQhEcHwREZqMcKUkebaKAbG0r48I0s7CmeMAMKOlyLFCLugefV2sDZNPgOunyV5jLs2rMHxwQks7WjA4X79Z2H1jMhp04RCCqJhBWlVQ0rNlrRZ146YM6JpWqCeEbXADfhkU9IZ3HLLLVAUBTfeeGPex/z4xz+GoiiWP4lEopQvGxiFZsXZDDsZWAmCyAcb610xS1/bLnOahgWeLTMKCrejvYNjaZwYTgIoPtYLeGnTmEWDF28HCz679fE9ePe/bMKN9+/A7iODwnH9t1IaE1FecIlqtmakqBUKtvSK6L+Q2e5gBQOgFyRBjA0nbduGKyGB1Xcx8sILL+Cuu+7CqlWrij62sbERx44d438OHjzo98sGRjar8X0SheLgaVEeQRD5YOmrp/NiRN7WXl6MdLJixJ0y8tZJXRXpbEw4Jp7aYb6LYm2ajMeMEcZVq2dj8Qx90mX/yVH8cudRfP2/XzOPK2n8lrVpNM0snGSO4IpGU1nGWMC81gCsBSTH5wLkDz2rhN00vp65kZERrF+/Hj/4wQ/Q0tJS9PGKoqCzs5P/6ejo8PNlA6XYbgGeM0LKCEEQeTh4St9Lw4qRUUnKyNBEmi/dO82483e7hI+1W9xGx9fEQpZ/l4+0zzHZs+c248m/eRd2/sN7cPN7lwMABsfNok3W+K14UWdKgMyLL/dfiAWDRMWFHTsYZcRoAVV7MXL99dfjiiuuwKWXXurq8SMjI5g/fz7mzp2LK6+8Eq+88krBxyeTSQwNDVn+BA17MQHOvwRkYCUIohATaRXHh/R2yOldTBmR4xlhfpGW2ihmNsQBuC9GWDCXKP8Xwm3oWSpTmsrQXBvDWXObAVgVZ1nqhXhRZ+/bpbSAco4vKiMlTADZYX4UgI0NB+AZEdo/gJy9N6Xi+Qzuv/9+bN++HRs3bnT1+GXLluHuu+/Gww8/jHvuuQfZbBYXXnghDh8+nPffbNy4EU1NTfzP3LlzvZ6mZ8SZeacfDBlYCYIoBDNMNsQjmNNSA0C/sxWnTvzCipGu5hoeGua20GGJpm6VBq/TNF7bNCJOXjxZKoOiKML7tj2zQ64ZVHa7w9GPInlSx9INqDbPyKFDh3DDDTfg3nvvdW1CXbt2LTZs2ICzzz4bF198MR588EHMmDEDd911V95/c/PNN2NwcJD/OXTokJfT9IW1SsxvYJ2gNg1BEA4w8+rc1lo+MQLIGe9lY72zm2vQENfbLW6naVJelZGY29Cz0tspTgtIS5mmsWMmZ9vaNDL9F2pWWsw8P7alaAjGM5Ipcs2bbDyN9m7btg29vb0455xz+MdUVcXTTz+NO+64A8lkEuFw4ajhaDSK1atX480338z7mHg8jng87uXUSob1KUOKLpPlnBMpIwRBFICN9c5vq0U0HEIsEkIqk8VIMoPm2lhJx2bpq76UEY+bdflob7E2jcfjOpGIsJs882ulJHlGACM5eyKT26aRrDKoEgsG8diWFpDM0DM1C1WtrGLE06vokksuwa5du7Bz507+59xzz8X69euxc+fOooUIoBcvu3btwqxZs3yfdBAUk+/sch9BEIQIU0bmtdYC0Ns1gD/fyE+3duOS2zbh9R7dL8cmaWY31/CJmJFkBlnh7jYfbHIi7lIZcd+mKV3B4IqzJSVV/tI53qYJQnXJqIKvQ24xklLlTuo4KS76sctfjHhSRhoaGnDmmWdaPlZXV4e2tjb+8Q0bNmD27NncU/LVr34VF1xwAZYsWYKBgQHceuutOHjwID72sY9J+hbkUMyAFKecEYIgCsDGeue16cVIXTyCvtGUrzbNXZv34UDfGL7+36/hJ9edzz0js1tqeE4HAIymMmhIFJ6SSWU8KiMxd6Fn3IvisshxgrVp1KyGtJpFNBwys0CkJI5a2zRBJaWqsts0AWeYiMcNhxTPO4CCQHoCa3d3N0LCD6S/vx8f//jH0dPTg5aWFqxZswbPPvssTj/9dNlfuiSKRRAnqE1DEEQBWJuGKSN1XMHw9p5xsG8UBwyV5Q97T+K5t/osBtZ4JMTTP0eSLooR40bLrWek1pimyWQ1pDLZvP/ONLCW4hkx1fSJtIpoOGRO00jJ7LBOQQZR6Ij+C2kGVsvYsPxFeZmsxp+TSmjRABKKkU2bNhX8++23347bb7+91C8TOGb1madNQwZWgiDykM1qpmekVQ/0qo/r7xlelZGn3zhh+fs3H3sdx4d0z8js5hooioL6eAT9LBK+qfDx/CojgO4byVeMyPCMiK2jiXQWDQl5OSOAkA9la9NInaYRxmRlTaVY2ikBbAQGTE9QpRQj5R8urhCKGZvIM0IQRD5eOjyAZCaLcEjBrGZ90rDOp2dks1GMXLt2PuKREHZ0DyCr6ReStjrdCMtMrG6W5aU9TtNEwwq/8BVq1cjwjFjHb/WvJXMyxX7sjMQNuE6tFBkx8wAQD5vXG5ktIPE1wDxBleAXAagY4RSb5aZpGoIgnHjurT5s+OFWAMCFi9v4xZkVI16UkVQmiy37+gAAV587F9dcMJ9/rqspwSf96o3xXjfBZ0wZcZsHoigKaqPMxJr/+F6ndPJhj03ISAwm422atHwDa1Bx8OKxU6pqtmkkjvYC5s9WhlFYBpVxFhVAsdQ/bmAlZYQgCIPHdvdgw91bMZzM4LyFrfjuejP2gE/TeNiuu+1gP0ZTKtrrYzh9ViM+9a7FqDPaJrONIDWvx/aqjADuluXx3TQRObHtbLw3mIkXW/y5zEInI3drL+DcppFxbEVReEEylqQ2TUVSrBpPCC8ON+N0BEFMbV4+PIBP37sNqUwWf3J6B/7zL89Do2Am5W2aIuFhIqxF886lMxAKKWirj+Nj71gEAFjR2cgfZ2aNFF/El8x492DUupioSUnyX/AUVt6mkXfxjQvHLhZs6RWnNFNpOSMO4WSyx4bHKswzIn2aplop2qYRXN8pNYtEqHimCkEQU5fn3zqFrAZcsKgV/7b+nBxV1U+bhplX33naDP6xGy5ZinPmt+Ccec38YyxrxE2bxpx6cf+elYgWzxqR16ZhyojNZCpZGbEkjkps06QyWV7oyVZGRM+IDKWIHztpJuxWQhQ8QMUIp5hpyrIBMp21jKQRBDH9GBhPAQCWdTQ4XtzMaRp3PrPe4Qm8ekwPOXv70nb+8VBIwcVCcQLAUworn6bx0E6pddGmYTkjpbZpTM8Ia9MEkTNi3REkc0w2paqIqfr/y/JfOIeeyVVdWKEZrYAleQC1aThqkfnzSEgBey2QiZUgCLb2vilP1LvXaZo/vHESALBydhPa6wuvw2DBZ548Ix4ulLUxY3PvZCgjEauBVVb7BxB9HcG2aVSJrSXAlmESkB+FpmkqhIyaxS2Pvo6/+NFWjCYzReN8FUUReptkYiWI6c7AmF6MNNc4h47Ve2zTcL/Iae1FHuktat7rojzAXZsmJcloGrcty+PKSImKi3jsZCZr2cwu4wIsFgyBxcEH6RlhbZoKKUambZsmEg7hF9sP48RwEm8cH3bVp4xHQhhLqZalTgRBTE+4MpKnGKmLeVNGnntLH+l9x9IZRR4peEZctWmMBFZPyoiLNo3s0d6MNbJdRvtAjGQQ01dlxJ9bls4VCc30ilNsuyzPSNymjMgy3ZbKtFVGAGB5ZwMA4PWeYXNNc4Eq0R4tTBDE9IUVI821eZQRj9t1B4zjzRFGePNRb0ztuGnT+ElKNadpMsioWXzk+1vw4bu2WCYJZcTBA7k5I+mAckYyAakXybT8OPh4OEDPCCtG+GhvZZQBlXEWZYIVI3t6XCojUbPKJghiesPbNPmKEQ9tGk3TzHAyF+0Uc5qm+Ghv2sNxGWKb5snXe/HcW6ewdf8pnBpLmceV5hmZnJwRmYoLYDOZSoxsF48ttsmkeUaM53WUhZ5VSJtmmhcj+tz+a8eGTAmvoDJiVsIEQUxvirZpeDFS/OZF9DPEXYzgNniZpilBGRlLqfjPLQf4x0VVmLV/StnaCyDHiyc3Z0Ro00hUXABzVNo6fiu3GBENxLJbQOMVZmCdtp4RAFgmtGncGJDsvU2CIKYnalbD0AQrRvJN0xijvakMslmNR7k7kRLGTr0oI16maeIeigZWjOw+MogXD/bzj4t+OVnKiHmTZ1VG5OSMCCmpQUW2B3FsPn5r/nxlT+rw0V6Kgy8/S2bWIxxSMDiexhFjRXehHwwpIwRBAHp7RDPEjHzKCCsYNM1Mu8xHKuOxGPGwKM/r1l7AvPESCxHAuRiR5xmxLrOTnTNitn+CmHiRPX6bO8001adppnUxkoiGsahdX/e9+8gggMI/GDKwEgQBmC2a2lg4b/FQEw3zbKJivhFWMIRDiquLg9im0TSt4GP9jPaynBEGGz6ZEG7EZGztBXITWNNSc0bMNg1v/0gqRuIWz4jkOPhJ8IzwaRoqRioD1qphxUihFxNt7iUIAiieMQLo2URux3u9btZtMLb2alrhLBDLsX20aQBg0Yw6LJ5RD8D63mdOvUge7ZU5TRPN9XXImh6JCdeDYutE/B5bVC9kjCNbj11ZnpFpX4ysmKWbWPuNN5dCTmv7CBpBENMTpow0FihGALOdUlQZUfULg9uCIREN8YtIsUIn7SNevUYoRq65YD5qouaIbCnHdSKe06aR104Rp2nSspNMhWV2xdaJ+D12EAUDN7CmyTNSUbDxXkahpUGkjBAEAZiZIPnGehluI+GTHtULRVFcL8tjyogXAys7dm0sjD9bM0dopTh5RmSN9tpyRmSHnkk0xorHtsTBS24BBdFKYVNArECuFGVkWk/TAMDyWY2Wvxcc7WVjYqSMEMS0ZtDI22jOM0nDcDve67VNA+gFw+B42oUy4t3bcd7CVnz43Dm4aEk7GhNRx0lCWXHwdgNrWqLRVAw9C6qVEmQcfBAmU3EjMFA5npFpX4x0NSXQkIjwu4tCo1lkYCUIAiieMcJgm3tHkoXDyfyoF8zEWiz4zI9nJBoO4Z///Cz+97hNvQDMMDVZOSMTxvEykkaGAftuGsk5I05x8JL9KLJMwk7HZlAcfIWgKIqlVVOoGqcEVoIggOLpqwzTwFpEGfEx8eIma0TTNF+hZ3a4EdQxZ6TU0V5rzkhaYjuFKU36bhq5Cazs5jStaoH5URgylRF7wSsrG6VUKuMsygxLYgWKTdOQgZUgCEEZKVKMuI2E96NeuMkaEZNdvRzbTiJiVS/0Y0vyjNgSWN2kYbvFSRmR3UoBBKOpLM9I1KZeSPWMBHfsUqBiBOZ4L1CsTUPKCEEQpoG1aJvG7TSND89Ig4tleWkx2bWEosHZwCqnhWC2gIJLYNU08/jy4uCFYsT4+Urbe2MvGCS2UuxFaaUYWKkYAbBiltCmcbObhjwjBDGtGeQ5I+4MrEVzRkpp0xQ4tpjsWkrR4BRrIKP9Yz220aaROJkitiRYq0yW/0JsT7GE3SBUF0DuZl37sWUl0pYKFSMATusQlJGCnpHcWXuCIKYf7g2s7pQRr6O9gLtleWJropQLpV29EI8di5ToGbG1v3lsu8TRXsBUL2QVDIqi5ASIyTbHMqRO0+T4USqjDKiMsygzDYko5rTUACgSesZ+IalNQxDTmoFxY7S3qIGVTdPIb9O4yRlJ+jiuE3ZfByBM08iKg8+o0DRNMLCWfgEWC4YRY0xWphIQF4LPAInL7Gybm6V6RnJUF1JGKoozunQTa208//puUkYIggDMaZpiyojZppEf2e6mTRPUxAsAnjpasmckavo6UmpW6qI8wFRHmDols+WRazSVO9rLjxugZ6RSRnunfc4I42//ZBkWzajHFStn5X0MGVgJgphIq1whkDZN48czwto0BXJGzOPmv8lygz30TFcw5Hg7EsIFfULYrivrwh6PhDGMDMaSQaSZBnNhz23TBJgzUiHKCBUjBks7GvD5y5cXfAwZWAmCYH6RcEhBQ7zwW2idx9FeT6FnLto06Yx+YY+Vuj/GFnqmZjWwZcGltoBi4RAURVdGkmlVejgZO3emIAWqMki6sDOPj+zUWMBsLZlfqzIaJJVxFlWCU9+UIIjphWheLbZJtd6FyRTw6RmxHXvnoQF887HXLSZTrwv48mG+91mDyYDS2zSKolhMrBlJ7R8Ga6WMplgxEpzKEJTRVGoxEmCGSSmQMuIBJ0c5QRDTC7d+ESDgNo2gjBwdGMe1d2/F4HgaZ3Q14k9XdQEwb5xKzwKxTrykVDkjw4xENITxtIqxdIarAdKKkQhbDBdAmyZnTFZuoTMuORsFMBflMSrFM0LKiAdoNw1BEAPGkjw3xYjrnBFfo7361x+aSOOzP9vJFRv2X8BUMEpXRqw3YmlLMVL6xYwpL2LRJrtNE4SBNcjYdvFnNh08I1SMeCDu4CifqrzeM4SP/ccLeOXoYLlPhSAqCnaxLzbWCwD1xm6atKoVNL4nuWfEvdHUXJSXwfP7T/GPW4LJpCsj1mIkGlaKtqrcwIoRcepI3g4ZazEidbQ3EuAIbkBtmiDNsaVQGWdRJUwnA+uD24/gd6/14idbDpb7VAiionAbeAYAdUJUwGiB8d5SRnsZMxviAPIFk8lSRvTjMWOsvFaKYTKdCEAZYapLSm5KKuA0JitxbDgyOcUIKSNViGhg1TStyKOrG7aW/PWe4TKfCUFUFnxjr4tiJBIO5dyZO5HysXSuNhYGu4584KwuXH5mJwBrMeLHGOuE3bwvKwqeEefKiNlikhYgZm/TVIuBVSxGpHpGKjNnhIoRD4iV6lRXR5hcuvf4MLLZqV14EYQXzI29hffSMNztkPE+9aIoCj5y3jysXdSGf/rgmaix7XgB/BljneDFSE6bRs4lhKVbszHlSEhO+wcw37f5lE6VqAzxSfKMVMqiPJqm8YDYH0xmsvwXdCrC7iJGUyqODIxjbmttmc+IICoDtxt7GfWJCPpGU4WVER9tGgD4xgdX8v9n70fjDsqIrATWiYy1GCk1v8Q8vjU2X+bdut3XUWj/mOdjT9J2XZkFlD3LRqahtxQq4yyqBN2spf//VE9hFe/i9lCrhiA43MDqshipM0yswy7aNF5Cz+w4bdZNS0pgZRf0tKpBzZrpq9ESFRcGK3aYZ0SWeRXIzdUI8thBbdcNclEetWmqEDGcZ6rvpxGNZHuOUzFCEIxBD6O9gNmmeeXIIB7bfQw/f/EQ+kdTlsfI8HbUGBfGIJURQL8RYyPD0to0gSojAaoXwvevKAGGnkk851BIsbSTKsXASm0aj8SNcJ6pHnzG0gqB6aeMaJomrV9NTD0GPIz2AuZEzb/89g3+sQ1r5+OrV57J/+63TSNi93UApjJSiuICgN+EAbrywiPbJV3I2PGHAzCZBjp+G9DEi/3Ysn0dsUgImQCmi0qBlBGP1Bq/8GOpKV6MCJLyG9NIGfm7/3oJ7/6XTUUTM4npi5ecEQC49PQOJKIhdDTGMbu5BgBwbHDC8pikhGKkJlbIM1LaW30opPA79Qlhf0ypxlgGU16GeZsmSGWk8lsp+rHNIkq2r8NaRFVGGUDKiEdqDcl1qhcjomdk34kRpNWs1KjjSuXJ13rRN5rCG8eHsXpeS7lPh6gwslmNFyONLts068+fj/913jwoioJfbDuMv3ngpRxl1c9orx17ZLt+XLYor/Tf3XgkhJSaxURaRUpyzghv0xiRAlKVkQB3sYjR6jK9KPqxA1RdAmoBlcLUv7pIpi7GlJGpe+ecUbP8DS0cUpBWNew/OVrms5oc2EWi0CZUYvoynMzwbbVuPSMAeNuPqRc5xYhMZSTloIxIUDDigkFWTGCVAQ8mMyIFAk1JDWjiReaUzmQeu1I8I1SMeIT9wo9OYWVE/N5WzGoAMH3Cz5hcTsUI4cSgEXhWGwt7im5n2JNMGVI8I2yRZ8YhgVXSMjuAGVglh56xBFYe2R5Mkikgty0RVEqq/djSVZcA20t+oWLEI2xMb3wKKyPsDSEWDmHl7CYAwBt5ipG0msXxoQnHz5Wb40MTnnbrZFRzfTlLoCUIkYFxb5M0dhIOwWSAnNFerro4KCMyvB0JB2VERpEjHnuYt2mC84zIVF2C9F4E6kcRfm6V0n6vjLOoImr5SvDJVUYmM36emTfr4mGc1qErI/nGe2+8fycu2Pgk9p0YmbTzc8sn79mGK+/4I/a6NOCKqbqkjEx9nt13EtfevdVTC9LLXhonnILJAHG0138eCC8WMg45IxKVkYmMyr0o8jwjVmVEqnphC6cMKs00yIIhSNWFlJEqxZymmbyL1fc278Oaf/rdpF3wR3gxEsEyVozkUUZ2HhqApgH7T1SWp0TTNLxyZAiZrIan9vS6+jfi3eoQKSNTnn//w35sfuMEfvPSUdf/hu+lcTlJYyfhYDIFZLVpcj0jSYlTL3GesaQiLdGLApjnzjZPyPWMBJczYmnTkGekJKgY8UhtfPJHe3/36nGcGk1h+8H+Sfl6TBmpj0ewrFMvRrpPjeUUYNmsxls0lbar59Roikvfz+7rc/VvSBmZPmiahpcP6y28kyNJ1//OaxS8HScDazar8fZgScVIzFQumJKaljTaC4ieEfkGVvtqjaB8HYBc/0WgEy/kGSEKwTwjk1mMMGl4YpIu+GIx0lYfR3u9vhDsjeNWZebkaJK/iVZaCJyY47B1/yn+5lkIUkamDz1DE7wIOWlLQy3EEI+Cd7ckz45pYM1dZgfICT3TtNztulI8IxGzkJLvGQlSvZicaRrpnpHw5LSAyDNSpfBpmkkMxeLFyCQVQGxjb53hj2HqiN3E2iNc8CtNGRGLkbGUyu+CAV0Sv/uZ/TleAVJGpg+7hNfDyWEPygiLgvfZpmGbdTPCfhfxdVdaHLywyNNoA8lUMEQDq3zPiLVgkDpNE2DOiFjoBNmmCbQFRMpIdcJzRiZRCeDFyCR9TRY8xHZq5DOxWouRylJGegbHLX/fsu8k//+7Nu/DV3/zKm777R7LY8Tnl6Zppja7jgjFiIc2jSwDK2C+3lJCMVJK0RANh/iFZdx27FLj4MVjTKRVZFgcvLQ2TZAFw+QksFaTydRSRFExUp3wBNZJUkYm0iq/c7I78IOC5YywnRr5TKw9Q5WrjBw1CiVWPDLfSEbN4r6t3QB0X4kIKSPTB7EY6fPQpmHm7oaEv/Bq8QLDCwahlVLqTqQa2+iwTAXDKfRMVpvG3kqRmzMyObtpgtgfwwjSMyKzOCuFyjiLKoJ5RiYr9Iz1qIFcB35QiNM0ALBkZj0A5LQ1LMpIhW0xZuf2vpWzAAAvHuzHRFrFk6/38haOXWmiYmR6oGmapU0zMJZ25SkCzN/BhI/AM8DY/M2MoMaxuHoh0WRqV0bk5IyIoWfBtmmqcWuv7It6kMoItWmmALU+4uD/sPcEfrHtsK+vNygWI5PUCmF+mAajGJnXVgsAODY4bpGUK7lNc8xo07x9aTtmNsSRymSxvbsfP9lykD9m3FZAUZtmenB0cAJ9oylEQgp/I+4bcaeOsNeI3YfghRpb1kgwwWT6sWUmpVo9I8EksDLk5owEd+zJ2tor3TMS4BSQX6gY8YhZjLi/+H72ZzvxNw+8hKMD48UfbMNSjEyagdWqjMyoj6MmGkZWA44I34PYppks1cYtTP2Y1VSDCxe3AQDufb4bz7xpekeSRZSRyQyaIyYPpoqc1tGA1jp9Ksatb4S9Rux38l6wFwxBpaRKPzabpsmIOSPBjPbGJB0XcGoBBaO6yJ94CW5rL4WeTQHquGfEXWGgaRr3JviJTRdHTCdbGWHfq6IomNeqqyMH+8xWTaUqI5qmCcVIAhcubgcA/PfLx/jHgNw2jfj3TFaruAKLkMOuIwMAgFVzmtBeHwfgvhjhykgpse1MGUkxX4f+XxkFg90zItPbERfGkgMf7Q0wZ0TmxdeyP0Zym2bS/CjkGalOzEV57to0KTXLUwVZeqMXRGVkfJKVETZNAwBzjWLk0KkxAPoFv1INrP1jaX5HOLMxjrWGMsL4y4sWAsg1BNu/B2rVTA1eOjRgSS9mY95nzm7iGTonPbZpSlFG4rbYdva6kxrZHoTqEjG9LtXsGZF58Z2sgkF6C0h4DipEGKFixCvmojzVIuMfOjWG//2jrXhWaAMAwETKvMDZpzfcMDhWDgOrNWcEAFdGuo1iZGgiY2lV2S/kalbDT7d2u94LIxPWDmuvjyEeCWNuay3mtNQA0L+P95zeASD3+bS3bSj4rPrpH03h6u9twZ9++xnsPzkKTdOw25ik8aeMlN6mqWEm01RwbRr7pI5Mz0gyo0r3jIhjyTKPC+jKblAX9skqGIKKg4+ElJInuGRBxYhHWBx8JqtZkhMf3X0Mm/ac4GOjDPHuu3/MRzEybiowkzbaKyzKY8xvY20avRixt5zsF/It+/pw84O78KF/exavHh0K8nRz6BH8IgxWgPzlRQu470eMzQZyC6ohmqiperpPjSGlZjGeVvG5n+/Ewb4x9I+lEQ0rWNbZwJWRPs+ekdKTUllrs2o8Iw6jvTL9F4kAL+yWHTJBxcEH2KYJarRXtjG2FKgY8UitcEck+kZYO8XeShF9CKW2aSYr9MycpjGDnezKiJhwCuReyPtG9Tf34WQGG+7eavGaBM0xo1DqNLwhAPD5y5fjF59ai2svXIBELDc2G8h9fmm8t/o5IaSr7ugewOd+vhOAniocj4TRxpURdzcKSe4ZKUUZsWeByGzTWJURmUWDGHqWlhgzzxDVJvljssEkpUbCId7mqKbQM1MZqZwSoHLOpEqIhEP8BymmsA4ZCoZ9yqZ0ZWTyi5ERB2WEjfd2nxqDpmk4nlOM2L5v4Xk4OZLENT/cil4fBl4/HDPaNLOEYiQRDWPN/FY950F4YxLzUcgzMvXoNYoRlpi6vXsAALBydjMAeG/TZJhnpHRlJIg2TY3gGckIfrV42H/xxODKSEZFOiPXMyIeHwCiQSojAbY8gjguENxob6VM0gBUjPiCR8ILKazMX2BvpcgtRoL3jGiaZlmUx5jTUgNF0YutvtEUV0aYzG0/N1Y4nbewFfNaa9F9agz/555tkzIu69SmEYmGlZzYbICUkalI7zALv+vE+1Z28o+vnN0EAJ4MrGpW48ZNv6FngHhRt4WeSc4ZYecKyBnBjQthbemsXM+IeHwgAGUkGkybBjBVl2rKAgmqgCoFKkZ8UOuQwsouXDnjosJj+ke932mLCayT4RkZT6v8bko0sMYjYcxq1JWGg31jfJKGtW/sqgJ7o53TUoP//MvzAOgy+dB48Bd4cazXCV0dsU4dALkpsqSMVD9MGZnRkMDXr1qJmQ1xhBS9SAa8KSOi+ldazojNwCqx5SG2gMSAQpktoImM2aaReQEWCzyZXhTALBgUJbiWR7DTNMHkjJBnpMpxSmFlRUO1t2lYi0ZRzO+TIY73skV0C9rqAOS2acQRyAXtdWg2tpz2TEKrhqWvduYpRth5AdbsFnuOCykj1U/vkF5kzGyIo6Uuhl//1dvx0Kcv4isOWDFyajSFbLawaieqf6WoGPbXXkriaG9c8IwkjfwSWRdg0zOS5W0aWTkjgF29CKZNI9sICpjPgeyCYTLGhskzUuXUOgSfuWnT+DGwiuOlyUy26BtmqYyysd5YJGfka77gG+kx3uSZl8SuKrDvm92pdRqqyrFB7ym0XhADz7rytGmA3L49YH4PrHCiYqT6OTFiFiMA0NGYwFlzm/nnWQKrmtUwMF7495MV2LFwCKESLg5cvTBee0mpnhFx4sUsGGSMb1pbQPLbNIlIkAbW4DwS8YBaHvrYrfH/0ltA+nNNnpEqp84h+Iy1H+yR7eLF7tRYyrNnYtD2Bhl0uJiTX4RhprA6KSP2zA7rCCRTKfyk0HphYCzNz2VmYzzv48xwKGGaxrhTZXfLlDNS/ZwwXm8zGpxfC7FIiJtbi7VqZOylAXJfe0EssxsX2jTyUlKNkWTLbhqJbRrheZWpuACmYhREW4K3aSQfW1GUwAqdNsMr1VIbLfLIyYOKER+w9oVYaLAL11jaml0htlZSmawn30dazea0fYJu1ThN0jDmGYXHm73D6DdUHqaW5G3TRKzKSM+gu6kFvxw1iqS2uljBvn5NLLdNwwqoGUYxQspIdaNpmqmMNOZv2XET63Dh16aMvTTiv7cHk8UkTLzU8IJB/vgt81ml1Cx/LoKappFdNPA2TQDR57EAW0Cr5jSjrS5WsOXsh+WdDfjOutW49eqzpB63FHJvf4mi2A2sYtHAHPds0ZN9yqR/LM3/fTFEVSQSUpDJahhPq2gp+TvIjxtlZLcRYhaPhPibvF0Z4W0a46LPfpl6hkpv0xwZGMcvdxzBNWvnozFhrezZJE2xX16+9CuV6xlpb2DFCCkj1Uz/WJq3KliB6UR7fRz7ToziZJGEZBl7aYDJWZQnKiNBRLaPGIV69eSMBN+mCeLY933sfKRVjb+PykJRFLz/rC6pxyyVkn7it9xyCxRFwY033ljwcQ888ACWL1+ORCKBlStX4pFHHinly5Ydphqw0V77HbSoftiVkH4PkfCsGGlIRMw7+UlTRnKLkflGMaIavpVZTQnzbsnmZzElbbsyUnqb5gdPv4VbH9+DjY+8lvO5Y0XGehlOBlZSRqYWbKy3pTZa8KLJis9iyoiMKHhAWJQX8NZemVM6gLUIY+cuVxkRE0eDmaaRfVwg2DHZSDgkvRCpVHy/kl544QXcddddWLVqVcHHPfvss1i3bh2uu+467NixA1dddRWuuuoq7N692++XLjs1UasyMmTzdYjtm5xixMNEDStGmmqiOdJuUBQqRppro2gQPt7RmODFBgBLPD5/4zZ+UTu4MlJ6m4ZJ7w9uP5Kz76enyFgvwzSw5npGmL+APCPVDUtfndlQ+LXQXseyRooUIxICz/R/b3ovANk5I+bYsKmMyLlIRsKhnAuuTM9IPEgDa5SNsgY4TVMh22+rFV/P3sjICNavX48f/OAHaGkp3DT41re+hcsvvxw33XQTVqxYga997Ws455xzcMcdd/g64UqAKSPjhoHVftGyKCMpezHi/gInFiOiSz5ICrVpFEXh0zOAfsEX30DFiZoJW5uGFQc9EqZpmCKVzGRx73MHLZ876mKsF8jdbgoIykgDKSNTATbWm8+8ymCG5b4iwWdJmw/KLzUx02QKyI2Dr3HIAomVeL4idlVIptF0MjwjQagXlRggVo34eiVdf/31uOKKK3DppZcWfeyWLVtyHnfZZZdhy5Ytef9NMpnE0NCQ5U8lYfeM5LRpRB+CTckY8KCMDFmUkdyLZxCYG3ud38CYbwTQ1Y5ISOG7GUQTaz4Da/9Y2vX38MiuY9j8xomcj48KI9X/seWg5esyZaSruXAxUuPUpuHTNPqd8vBEZlISY4lg6B22jvXmg7dpiigjzBdV8jRNJHjPyERKnKYJZuIFCLJNUx0pqQD4fqMWQ2Ej/OH5J37//fdj+/bt2Lhxo6vH9/T0oKOjw/Kxjo4O9PT05P03GzduRFNTE/8zd+5cr6cZKPbQs5w2TTr/pl17W6EQTBlpTERzlmsFhamMOI98WZSRxoQxfsa2kDq0aYzzFgsqN+O9fSNJfOa+7fi0Q4S8OFJ9ciSJX790jP+dG1gbXXpGHHJG2J20ahiGieqEeUZmFBjxBvTJK8D9aG+pykjCNsklM2dEjJoPYpmdfUFgNDADa1DKiPxWyo2XLsWtf74Kf7pqlvRjTyc8/WQOHTqEG264Affeey8SCbmjRiI333wzBgcH+Z9Dhw4F9rX8YBYjhmfE3qYRfAhMJWFBWl6Cz0RlJD5JnhGzGCmujLBWCN9Z4ZBmygoQRVE8mViPDIwjq+nqk/17Zud4yfKZAIAfPrMfmqZB0zTepinuGTGUpkyuZ6SlNsad8dSqqV563XpGuDJSbJpG0mhvxOpXCqRNk1YDGb+1q0JyPSOCMiK7GOGeEfnKyMyGBK4+d27Jr4vpjqfR3m3btqG3txfnnHMO/5iqqnj66adxxx13IJlMImyble/s7MTx48ctHzt+/Dg6OzuRj3g8jni88N1MOamzJbDa9604TdPMaqrBwFjan4G1NoqaocnxjAwXMLACwPzWOv7/HUZxob+5pi3nxoow8Re0ozGBA8Jem0KIBctIMmMZh2btsY+9YxGe3deH144N4UsP78bBvjF+DsU8I3yiwTaSzc65Ph7B4HgawxNp/n0S1cUJl22aGcJ+Gk3T8qaVygo9Yz6qJG/TGMmuQYWeyVQv7MqIRKXBoowE1aYhX0fF4uknfskll2DXrl3YuXMn/3Puuedi/fr12LlzZ04hAgBr167Fk08+afnYE088gbVr15Z25mWkxpbAaldGxJ01TPqf3Wx6Jtwy6OAZmSxlJF8xIiojbHzWURlJ5xYjpom1eDFyXBizHLGpE+wcZzfX4M/WzAYA3PNcN/6w9yQAYO2itqJ3KXFb28u6BC2EhoT+/Q+RMlK1uC1GWBplMpPl02ROcM9IqW0a2++yVM+I8d6kaebvSVC+jnBIKSkWv9CxA2vT0MRLxeJJGWloaMCZZ55p+VhdXR3a2tr4xzds2IDZs2dzT8kNN9yAiy++GLfddhuuuOIK3H///XjxxRfx/e9/X9K3MPnUGXfp43lGeyfyKCOAv5yRRmG0NzlpbRrnl8bslhosnVmPkKJwbwX7RbdM0/C0SvOX3xzvdVGMCAWLaFjNZjXeHquLh/GZdy/Fwb4xNNVEcd7CVpw7vxXLOhuKHt++xl0891g4hIZEFMA4tWkmifGUipMjSb6MUQa9RaLgGbWxCGpjYYylVJwcSRk/+1zMArtEZcR47WWyGtKq3DwQUbkYCjiYTHYrxbq1N6AttaSMVCzSE1i7u7sREiS2Cy+8EPfddx/+7//9v/jiF7+IpUuX4pe//GVOUVNN1OYoI/mnaXgxwpURfzkjk2VgNadpnF8a4ZCCR294B/9/ADkGVjWr8bu9GuHNi3lG3BhYxYJFvFsdE77/ungEiWgYP7nu/KLHs2Nv0zC/SCSkIBI2lRFKYZ0cPn3vNjy15wS++L7l+MQ7F5d8vNFkhrfzCkXBM9rr4+g+NYa+kSQWttc5PkaaZ0T49xNCOyUu4QIcDSsIhxSoWY3fJMk4LsPq65BbMFgLHbnHPr2rEdGwglVzmqUel5BHycXIpk2bCv4dAK6++mpcffXVpX6piiHXM6L/0iuKLo+KF0x2sWMbZL0YWAcNL8pkhp4VU0aAXKmTKyMZp5ZHbpvmmJs2TZ5ihJ1fOKSUuMbdds62C00jL0aqTxkp5HuoRDRNw4sH+wEA33jkdfQOJfHF960oqQXAzKu1sXDB1zKjrT6G7lNjBSdquCm7xDaN+LqdSGeltmkURUEiEsJoSuXt46D2x8hfZie0aSQrGGd0NWHnP/wJv5EkKg9qoPmAT9MYS/HYL31bnS4HTzjkjLAL8Ugyw998ijHkkMBaztCzfJieEf3cxHO0G1gBawsmH2IxMupQjNTGwiVdcO1KE7vQsAsFk+rzKSP//fIx7Oju9/31g6J/NIWLb92Er/3m1XKfimsGxtKWou/fn9mPG3+20/XviRNu/SIMFnx2osBEjX0TtV8URbHkBsk2mjJPGzPWB9emqR5lBNBvIqupSJ9uUDHiA1aMqFkNyUyWv5F2GHkGTtM0HY0JHg7mNvisHAbW4QJbe/PB7hTZmzW7wMfCIcvyKDbh0juc5Ptt8nFciI23KiP6sb0US47nbFOa7MpIQwFlZP/JUVx/33Z85r4dJZ1DEGw9cArdp8bw+Cv5c3wqjQN9owD0Nt7t//MsREIKfvXSUfz42f2+j8kyRoqN9TLMFNbiyoic2HazGJa/Q8YoRgJRRoQ2TSQ4z0gQI7hEZUPFiA/EMdOxlMoVDHbnzwyWmmaGZtXGw2iq0e+23UzUZFTT2T9ZnpG0akrG/pQR/dzG84xAzqiPI6Toxr2Cb/pp1bKx2FKMpExlpBTi/M7UWkCZyogxTTOe+7Pa06MnAh8bHLcsB6wEDp0aA4CCUyGVxsE+/ZzntdXig6vn4IvvWwEAePyV44X+WUHcRsEzWOpuoTZN0mFCzC/isjweeiapaODKiFGMBBV6Jl8ZCS6Blah86CfuA9GvMJrMcAOrXRlJZrJg4aE10TCPC3ZjYhXvyBsTkZy140EgtkPyGVidiEesLSSnsV5A95qwi0OhiRr76K9Tm6ZUZcRuYDWjvpkywto0TsqIfvHMapW3TI8XI1UUZc+KkQVGuu/lZ+oZRDu6+z2tTxBhnhH3xYj+uEd29eD7T+9zbM/JMrCKx5DtGdGPrR+Ht2lkBpMJBYNsz0iQCaxE5UPFiE/YxXp4IsPvQpkkPGGT/gH9F62l1ihGXIz3MmWgLhbW10hPgmeEfR+xSMjTXY/dwMrOscbhTbvTMPIWMrHap22syoihNMXktGnMc86jjDgUIwdOjvL/9xLvPxl0G8VIxmghVgMHjTbN/DZ9iqWruQbLOhqQ1YCnjewYr/A2TZEoeMbFp81AZ2MCp0ZT+MYjr+PCjb/Hj/5obRMlJW3t1Y9hKiOy2zTs9y4IZSTI8VsqRqY3VIz4hLUJjg+bF057m4YpJNGwgmg4hJZa920a0S8CmHck9i3AMmF+jAaPqoNZjNiVkdyXV6dxcSg03mtXTcTQs2KhbG6xF3dJWy5KIQPr/j6zGPEyqj0ZsGIEqJ5JIOYZWdBmjtS+a9kMAMCmPb2+jnnCZRQ8Y0F7HTb/3bvwz3+2Cktm1mM4mcG//vYNy2PYa6XU0DPAfJ3pCpb+sbhDaKS/YzMDa7DTNLJzRlpqo7hi1Sz8+Zo5Up5jorqgYsQnrBhhLYWaaBiNNdYwtHFbu6K51n2bRgw8Y8cHrFtmZTPi80If5ypD4TYNAFf7aXqHrH17cTHeqA+DrRN2A6upjBQ3sFqVkcpp02SzGg73j/O/V4tvhLVp5gtLGC82ipHNe0748uV4naYB9J/9h982Fz/7xAUAdDO3aLSWFQcP5KoXgHwDKzt1uZ6R4HJGFEXBd//XOfiXq8+SelyiOqBixCesTcAuqo01kRyTqX0/S6vhGXHTB7crIwmbxyEIfBcjPIHV2qZxymNgbZpCxQhTRlgff3gid5qmVGWE3ZmqRgqmXRnhOSNJa7ExmsxwPwLgPlE3o2bx6Xu34Y7f7y3pvAtxYiRpac3YY/QrkaGJNPqM51AsRs6d34r6eAR9oynsPjro+bhePSMi4mtLLITN5Y/yPCOiQVr2aC8jOGWELh+EPOjV5BN2Z86LkYQ58WJv09RwZUQvLNzcTecrRoL0ARTb2JsP+7lxRchh4qWzyYWB1fjc4hl1lvMCzL0/dSVO09hTMHOVEWcD6wGhRQMAp1y2aV47NoxHdvXgtife4CZT2XTbjmsvpIJC0zT855YD+PkL3rdrdxuqSHt9zBLDHouEcNGSNgDAU6+f8HTMVCbLvTxelBFGPBLiLQjxtVeoyPYKe09gv+fhkGIZgy+FhK2okZszEtxmXWJ6Q8WIT2qihjIyxJSRKL8jsUv/7I2HGVj9KCOTMdpbqjIyYfu+7W+KgOmrKVSMsJ0ii2fWA7DupvF7jk7nzPKPxPFKUxkxixFxKuXASesF360ywoooTQPuef5gSeeeD3ZhZ0yGMqJpGr78q1fwDw+/gi88+LLl4u2GAzbzqsi7l80EAGx6w5tvhI3nRkIK/53zgqIo/PUlPodJiW0adgxuMpWoMtiVEZnHJmWECAp6NfmEKSPMiNmYiPA3gZw2TYwVI8zAWvwCNpSjjAQfeubXHJrfwJp7BzlLaNPkGz01lRG9GHGKg68rcZpGj802w9qSeTwjalazPOc5yojLYmRCULR+/sKhgkVln7HK3iuH+m3FSMCekWxWw5ce3o3/3KIXV1nN+3SRk1+EwXwjOw8NeDruCaFF4zdSno2Oi8+hufxRpjISbEqq/GOLoWd0+SDkQa8mnzDPCBtRbSjYptGfZlMZ8d+mmYycEe/TNNY2TdJhSR6jU5g4Gna4WGqaxtNXWZvGabS3VGUEsBZ4dmWkNhbmsrnYqtlvmFe7mrwtPhS9Pv1jafzm5WM5j+kfTeGvfroDa/7pd/i3Tfu8fjs5bZogi5GhiTT+/pe7cM9z3VAU8+7bezGSO0nDmNVUg+WdDdA04A973bdqen2YV+3Yi5GssPzRSfHzit0zEmQxIlPBEKdcZOeMENMbejX5hE3TmFMvEUubRtO03DaNYWB14zNg8m1Trb0YyQYWZlVsY28+chJYuXE39+VVEwtzc6jTjpqBsTR/01/Uzto0ZqtE1jSNfn5mgWf3jCiKwi9I4ngvm6RZPb8FgAdlxFZE/mTLAcvff/tKD95z+9P49UtHAXi7+DKYF4W9NmWP9mqahmf2nsQN9+/A2/7pd/jp1kNQFODWPz8LSzv0n5VbDw3jQAFlBADeZbRqHtmVW7zlgxWI7PfND+xnz15voldLrjIiv01j/70jzwhRDVAx4hO7gVI0sGqa/ubFi5GY1cA6OJ4uupslxzMifL2gTKwjhuHR/zRN8TYNoIdaAbl38oDZommti6GlTv/exQAvroyU2KYBrFkjdmUEcA4+Y22ac+bpxYibzBj9a+jnvXpeM2LhEF46PIiXDg3gcP8Yrr93Oz7xk204OZLkKb5v9o54/n4OndLHelfMagQgXxn57lNv4qM/fB4P7zyKZCaLpTPrcef6c/Dna+bwSTG3HhqGPfDMzp+umgVF0aPhWaFWjHEJrxEx1BCwbqKWs5vG6hmRcUyGXZGUWTQEGQdPTG/o1eSTWtsFu1HYHwPob4j2nBHWptE0550nIvacEVEaDmq811xC5+3Oz96mKTYCubyzAQDw2rGhnM8xD87MhrjlYsIurHxrrwRlhOWjjDsoI4A5UcN+VsMTaZw0trqeM68ZgHtlhL0WZjfX4IpVswAAf/vAS7jkts34713HEFKAT71rMR674Z0AgJMjKU8tj4m0ygu501kxIlEZebN3BN96Uh9L/p/nzsXD11+E3372nbj8TP174enCLoszQDf1spbcgjzKyJmzm/CZdy8BAHzxwV2uJpFYm9Ru5PRCfcKqjLBJmkhIQUTCRXgy2zQyCx0ysBJBQa8mnzgpI5FwiMut42kV4ymrdyIaDnE/Rv9YCkcGxvGdJ/c6vsHyYsS4IEbC5rhhUMFn/kPP7G2awka/07v0i+WrBYqRzqYEQiGFP8/sojAmaTcNYHp5Jhw8I0Bu8BmbpGmvj2Feq37xHBxPI6MWV6rEiPxr1s4HAOztHUEyk8UFi1rx33/9Dnz+8uVoqYthtqEc7T0+7Pp7YWFndbEw5rbq/16WMqJpGv7+oV1Iqxr+x/KZuOXPVuKsuc2WdezcnO2hgGLKWFNNlAcCOnHDJUuxZn4LhpMZ/NVPdyBd5Pkel7BMsT5m9YwUU/u8krC3aQJVRoIJPZN5zgRBryaf1MTsyoj+d9EUaVdGAKDZaD388Jn9uOz2p3HbE2/g20/mBmENjlnbNICZb+BVGdl3YgSf/Mk27D5SODzK7xK6nEV5RXZ4nD6rCQDwytHcYqRnUL9TZkZXu1wua7RXP7/8nhHADD5jUvp+wWzZVBPlo8EDRVQuwJrGu3puMz64ejaWdzbg39afg59+/ALeWgGA0wz/xV4PrRpW0M5trc2bkXJqNIVn3zzp2XP0i+1H8Pz+U0hEQ/jHD5xhKUIYXvxQDFbc5VNFGJFwCN/6yNloTESw89AAbrPFtNuRqYwwH1Wx17RX2Gsvreo/i+rxjAQXB09Mb6gY8YmTMgKYb4DjKTXHwAqYcva9z3fzC+u+E9aLznhK5T4FttocMNsKXpfl/WLbYTz2Sg9+/mLhUCp20WWKgFvsi/KKrVpnysjBvrGc3S/H+YIzvRgR5XJN0/iFRoZnRCxGzK295q/Ewnbdx/DY7h4Apnl1QXsdIuEQLxTdqAGif0hRFNz+P8/GYze+E+9bOSvn4r60Q29jeVFG2FjvvNZaYRLE+tz+/UO78L/+/XlseavP9XFPjabw9f9+FQBw46WnYW6rc+HgxzNSzC8iMqelFt/8s1UAgB/84a2CCb58mWK0dM8Iew6TEvfSALnqhdTI9gCVEauBlS4fhDzo1eQTJ88IIKymT6tcwRDv0GYYEec10TA2GHL9QVtYFTNJNtda5euamL+sEeY9KOYh6DP8EG113kYiE7xNwwys+Ud7Af3CNcsYjX29x3rBZRM2TBnhUw2pDJKZLDKG8VfGNI2TgVW82GxYuwDhkII/7D2JXYcHeTHCipRWDz4JLzL/0pnelREWeDavtVa4q7f+vNlYshdz7O1PvIH+sTSWdzbgurcvzPs4VmR78bmwSZpiygjjvStn4byFrVCzGn66tTvv42S0aRr4NI1td5E0ZSQ49SKn0KHQM6IKoFeTT+xvdEzSZ+2b8ZTqaOS84dKluO7tC/H4je/ETZctAwD0jaYsCkG+7AUzpMtbMcJGHcU9G3Y0TePFSLvHfIa4EB4GiC2J/C+vMwx15BVb64gpI2yqhCkgwxMZrooAZs5LKcSFlprTpuG5rbV4v2E2/bdNb1raNIDQmnBxAS407myHKyNeihGjTTOvrZZfSO3FJ3sd2BcRFmLbwX4Aum+j0MWHKyMe2jTdp9wrI4xrLtAL+J9u7c7rHZHRprG3B3ngWVDKSEAFAyC30ImGQzx/h9o0hEyoGPGJvRhhfXpmihxLCcqI8Oawak4zvvSnp+sXjUQUbcabuKiO7M/TS+cJrx4NrOzOXYxVtzM0kUHKeHNv85jPILZpxHwVu1wswiY+7CZW5hnpyGnTqNzTUhMNS9nj4dimsV1sPvUufZLjsVd6+PTPgnb959LiYQvzRIEgODtLDGXkxHDS1eoAwCxG5rbUCi0GsxjRNI2/Do4XiOK3w1p3HYaSlQ9TGXE/TcM9I+3ulBEAuOyMTrTXx9E7nMQTrx53fAwrhktRz3KnaeR6Ruy/G9WijADm7zspI4RM6NXkE7tnoYErI+YFjiewxvI/zSzsSSxGRG+CiGlg9eYZYWbYQspIn7HPoz4e8TwxIK4sz2Rzw96cYL4R0cSaVrPoGzUMrE3WNs1IMi2YV+XenU6ks3kvNss6G3DpipnQNLP9xJURvvjQizJS/Nzr4xFzokZQR/7+oV341D3bkLVl1Giaxqdp5gqeEdHAOpZSeZicuHW4GEO2qa58iMqIG4NsMqPi6KB+zl6UkVgkhHXnzQUA/GSL844froyU4Blh4+0jttCzavCM2F/DsmPb2WuYihFCJvRq8omojMQjIf4Lyt4ARem/0EWZXdgOnjJ3nrB2wEJ7MWLbfeMWduc+VkAZYfkZomHWLWIfXQ97K77D44wufaJm7/ERfpE8MZyEpunyL/Nj1PGLgmpu7JUwSaOfX+5or9PFhqkjgJF/Ynx9L6ZNZu51o4wApjryhmFifeP4MO59vhuP7u7BkYFxy2P7x8xCbU5LDS+Mk5ksf25F9cZtMZLNajyyn02L5YMF+qlZzRISl4/+0TQ0Td9W61WJW3fePIQUYMtbfXizN9fky4qRkkZ74/r3E5QyYj9OkKFnspURlnlEcfCETOjV5BPRwNoojN8yZWTMIfTMiXlMGRG2weabMmBvAl4MrJqm8V04bpSRtnp/K9cZE3n8F3bYRTOlZrmhsocHniX4gjPxosDGLGX4RQDrJuRC57xmfgvOX9gKwKpWeRln9eIZAYTx3uP6c/OrnWb6qP3nz1o0nY0JJKJhS7HGLqbiPqQTw+7aNCOpDJjIUUwZSUTDfMLMTWuJtX8aExHHUeFCdDXX4NIVHQCAe57LNbLKMLCyIpgVY8UmxLwS5Gbd3Gkaud4OroxEyDNCyIOKEZ+Idx+NwiisGKTlRppnygiboBFTKRfaihH7VmA3jKVU7gUZK5BPcnLUvzKiKAqXmS0x+AW+b0VRcnwjzJPBzKuAIJdPZITAM7nBU+KivHw+ly+8dzk6GuO48uwu/jE+TeOmTePxYrZ0JjOxDkPTNPxKiEK358yYGSN6aycaDvGihykmYivp5EiqaHAYYLZoROWvEM0eJmqG+Rh54SInHyw47hfbDvOCizEqwcDakKOMyNvYC+QaYYPyjMTCIc/FXjFi5BkhAoBeTT4JhxT+hi8qI7XiNE2REVfAVEbY3S0z9bXURvmSPEYi4r0YEeX5QomcJ4f9KyOAqY6IXplib9w8ifXoECbSKu74/ZsAgD85o5M/hpsxUxl+/rKUER4HL/gp8m1kXT2vBc9/8VKsP38+/5ipjLgf7XXbplkqKCMvHR607PGxF5XcL9JiGkGZosSeM7vJ9uRI8VbN0Dhr0bgrGLxM1JjH9vezvGhxOzobExhOZnLC/MZTpStoTBkZS6lQsxpvs8lqp+QoI1InXhQwf3cQEy+sJScjBZkgGFSMlAAzsYoSNrsAi22aQndoTBk5NjiBibRaMAjKVEbcG1hFeT6Vyea9I2bG0XbfxYgZ2878lcXGIJky8srRQfzwmf04NjiB2c01+N8XLuCPEbensouwrDdB++ZUoPAEkJ3WOi+hZ97urJlnpHc4iXuesxo17cUoC+YSi4YGW9aI/RzdjPeKrRQ3mKPOxYsz89j+lJFQSOEKmmjU1YPxJMTBC9/zaCojXRmxFzWxsJzjArrqyF7bQUS2f+G9K/C595yGCxe3Sz82MX2h0rYEauNh9I3aPCMOy9cK3Q231EbRkIhgeCKDQ6fG8ppXAWsuhlvsd6ljKRVNNblvUCeH/bdpAPPN1XphL/xGyEysu48M8qmamy5bZnnDrxcyM0xlRFabJvec8ykjTrR4aNPYNzgXoyERRVdTAkcHJ/Dg9sMA9LvctKrl/PydDJv1tqwRezCbGxPr0HhukVOIVg/7aZjJ1WvarwgfvxW8UMlMlhfDpbRp4pEwf75HJjLSQ88URVdWWZEju2hIRMMYTamBtFLOntuMs+c2Sz8uMb0hZaQEmDIivqHW8jj4jKt2haIofLz3QN+YOdbroIz4a9NYL0JjeUysTBnxmr7Kz812YVeU4pL2kpn1iIYVjKZUjCQzWDm7CR84q8vyGDEzQ/Y0DSsSmXoU9riRlbUlhpMZ3ubJB38teBgNXWKEn2U1fUfRufN1E63dMzJeoBgZ5gZWa4HgJmuEFQxu1Qsvht7hEpURwBqIxxCfm9oSVQxRleMBhpJGewHr+0IQxUgQxyWIoKBXagmwOy9Lm4ZNFIyn+SRCsTs01pI52DdaMAiK773xUIzYL0L5gs9KGe0FzDYNu7AnIuGixrlYJITTjAsuAHzxfSv4FA1DvPtl5y4rZ4S9YQ+Mp4xz9vbr0JiI8t48e56PDY7jZy90WwpGTTPVjESBzBk7pxmtGgB475md3F9h//k7Fb08Et64UDNfC/uRBKOM5CpFg2Np3L+1m7dlzGMzZcR/MWIPJgOAMeO5iEVCngpLJ+qEgo4pGLKUEcBmNJVejIQCOS5BBAW9UkuAe0YEEx57gxHfkItd5Oa3msFnB/JEwYvHSXrwjPTb+vf2yQPGyRJGewHzTZopI25HWFfO1ls1l67owNrFbTmfF9sNo0m5yoiZM1J4kiYfoZBiJo8axchXfvUKPv+LXXh09zH+uJSa5YWpF88BM7ECwAfO6jJbgKl8bRrzeWmwLXpjxRJ7rbkZ7/XvGTFf+997eh++8OAu/McfDzgf26eBFRAD8URlRF4rT1RGkpLj4AHrayEuOwuEKSM08UJUCeQZKYE2Q0WY2WBGZbM3QXZxioWL36GxwuO1Y0P8jtWpGPEz2mv3jDhljSQzKpe6Z5Q4TcOKEbdTI391yVK01cfwFxc5L2GrFxaWsXOXsbEXyC0MvCojgH4B7htN4dRoChk1iz++qW/EFQ2iE0JirtvnBQBWzm4GAMxqSuD8RW349ct6gZOjjDi1aezKiFEgnNbRgAN9Y+4MrBKmaZgXiKWtMoY9toCccCpGeGEmwWgqFsJeFh26Jcg2TZAGVoIIAipGSuCzl56GM7oa8b6V5igqexM4ZbQ93Mi6zDOy49AAAOexXsCai+EWe5vGKYWVLciLhBTfd6pmmyZlOddizG6uwU2XLc/7eaaCpNQsV3nkKSPWc/SqjABiayKNXUcG+YVRVKCY3yASUjwZCk/vasQPrz0X81prEQ4pFnO0iGObJsczoj93yzsb8NtXj7tr03j0dbCRT9GntM8ItBsYs7dpWM5IKeO3uQsBWSuvFPMqQ9x+LDuB1X6soDwjlAVCVAtUjJTAgvY6fOKdiy0fY28Cow5L8vLBPCOqMQZg30ljP3YpBlYnZYQVI231Md8BSXZlxM+F3Yk64aLCNvrWSZqmsf9s/GRItBjjvafGUpZI/xGh6POyl8bOJUbSKGDuOMrfpsmvjDC1YlmnPk7tysA67q2VYo/HH0tleHS9XaHjBlaXqosTDQ7TNONpeVk0onk6KXm0F7C2fKRHtlObhqgy6JUqGXuv2s0d2syGuOUuyZ68yjCVEQ85I+PmpAjgbGBlfhG/GSOAWXyYbRo5L61IOMSLBtZaqChlRLgAb9nXxz8uKiNe01fzUZOnGHXySTTErXf1rGBZ1qkbhk+OJHnxmw+vykirsMU4m9Xw1gmzOMtRRiSM9jpN04xJSF9lNFg8I3JDzwDrOQZlYJW9JI8ggoJeqZKxvwm6UUZCIQXzWs3pmXxbTNmxkj7aNLOMLbhOo72lmlcB028xaPgMZN5B1tm8AbINrPzvfpQR4wLcMzSBFw6c4h8X79ZlSfxioJ5IwWmaZIYXApGQPkauKPq4MBvnTmZUfPGhXXhsd4/luF49IywOPqvphcy+E+bG4XxtmpI8I4n8nhEZ6pnTNI1cz0hwbZoaUkaIKoNeqZKxFx9u37zEAsRprFc/lo/QM0MyZyvpnZWR0sZ6AWGaxqNnxA32XTTSRnsj8pSRTa/3WpJxnZQRL+ZVJ/iqAVehZ/pFfngiw82rzbUxRMMhroAxpenJ13px3/PduO23eyzH9TpNE4uEuJpwajTF/SKAU5tGf36aSmnTCMoFQ0YUPMMxZ0SqZ2QyckZomR1RHVAxIhk/yggALGgzCxCn9FXxWG49Ixk1y+Xw2S16MeKkjPTJaNNErG0amW/a9baLoaxpmlBIsVwEfHlGDDXg6KDuwWAFgVj0Mb9Bqa0D5hnJbdPkXoDFSROmjrUYBtOZDUYxYnhwXjzQDyDXR+I1ZwQwx3v7x1J4U1BGxAWKaTXLCyrZBlaZbRqnaZp4UKFnkhUM9v2TMkJUC/RKlYy9+HD7pjhPUEbytWm8TtOIMeddTYYy4lSMlLCxl8Eu5KMlmDXzYS8+ZLVpAOvPy885M2WE8c6lMwDYsi98pK864ZQzklazyBjeD/F7aRAMrMzEzAoFXowYysi2br0YGRIuutmsxidxvLRSxP00+3pHLZ9j6ojo8Shlz5BTm0Zqzgg/vrnVWWaRLf68ZHpRALPlSNM0RLVAr1TJ2C9oXpWR1rpYXuk6LoR0aVph8yFgTtI0JiJ8IqKQgdVvFDyQe8cot01jL0aC6dv7uSA020awLz1dn36xTHiwAq3EC6RTMSr6R2qcdtMkMzzzhikjHY26f6h3OInxlIpXhK237LUwksrwoDYv6gXbT3NyJIn9xmoDM6VWfz0yxaUuFi4pJVX8Htnvw6hEZaROCI4LRhkJzjOyck4zFAVYNadJ6nEJIihotFcy4ZCCeCTE76Tcxke/bUFr3hRShljYJDPZohd8Ls/XxbiE75TAelIY7fWL/fuUmVQptmkiIUWq9JywKCPejysqI11NCb6J2ClnxI9BVoR7RoQChP1/xNZyElUDlnnDWkpim+blwwNcWQGAE8NJzGmp5QVDPBLyVFgyZeTlw4NIqVnEIyHMbq7BWydHc5SRUsZ6AbMYyWp6gV4TCwuhZ6W/tVknkuQbWIOMg3/P6R3Y9ZXLpG24JoigoVdqANTEwrwYcauMJKJh/Pu15xZ9DGMirRZ9Y2TKSHNtjL8p2ScxAEmjvbY30xoPO1iKIbZl6uIR31koTlilcu8XmhahGFm7uD1PRLmcu3Wn0LN85ljxInS4f8xyrjOYMjKU5C0aBgtD8zpJw2AFz4vGZNHC9jrUxyN46+SoqYxIWJIH6K0YRQE0DRhOplETC0tt07DXnbhSITADawDtFCpEiGqC2jQBIF4YSp2gEImGQ4gYmrcb3wi7E22uiZrGSptnJJvV+LRFKcVIbrR6MG0aWYFnjHiJykhDPMJ/JmsXt/EW0kQ6i4yqF6Sy9prw0DNLmyZjfC43wC0a1s/rECtGbAbW48NJbDtgLUZOsGLE4yQNgylFe41JmiUz6/nIr71NU4p5FdA3XtfHrO1HroxIaOWx192JETOtNjADK+WBENMc+g0IAPHCIKN3LWKmsBYPPhOnKNhdnj0OfmA8zcOv7GZML9iVkaA8IzLNq4C1deLnQqMoCk7vakRtLIx3ntZuOT/mX5CljCQcDKxOe2nYebHn7dApPQWVqRbcMzI0wZUR1l7ixYiPSRrxazAWz6gXYuLltmmA3IkaVqjJXJSXMorJkAJe4MmAihGCMCEdLwBKndAoRCIatuzKKITYpmFvziM2zwgb622qiZb0hphjYJVYhIkX+FrJxYhYIPiV4H/68QswmspgZkMCmqYhElKQyWoYTWbQVBOVtmSNh95lslCzGsIhpWC6a30igv6xNI4NWosRpowcM8aR45EQ3rVsBl49NsRVgCGfi+xa66yPXzyznqs3bLqLqS6lKiOA4Y0ZMl/XfLRXgmfEPlKeiIYDaxHSCC4x3aHfgAAIqk0DeAs+M5WRmKmM2No0MgLPAAdlROKdnhh6Zg9AKxWxdeJXgq+LR/jmZkVRcp7rcUkJrGKOCCtwnALPGCz4jPlT2R4dezvurDnN6DJC8WQrI0tmmG0aFsDnt9Bxwp7OW+j58H7s4CbE9OPpr4dYOCS1yCGIaoSKkQAIsk1TKPjs6MA4DvaZ2Q7MeNdSZ7ZpRm0GVhlR8IDDNI3UNo150ZKRrCkiQxmxY5pYjTaNpARWseBjxyyUONpgU5FYoRCLhCwtuXPmt2CGoZbI8owAgKIAi2bU5Wzz9bqArxANwvgtYBaAMoqReCRsUSxkZ4HwyHZq0RAEFSNBEKwy4lyMZLMarv7eFvzpd57hisjAuBkDzoyfqUwWadX0m5jpq6UqI8HdRdZZlBHJnpFoaZ4RJ9j5svFeWRtfQyHFVMZS1kInX5tGRFQtWKsGANY4FSN+p2mEYmROSw0S0TD/uoPjTBlhbRoZyghrP1qVIlk3AeJrT7YyEqdihCA49FsQAEHcbduPZzew9o2mcGRgHMMTGWzdr49VsumFltqo5c5ZHO812zQlKiP20d6ADKwy7nhFxAJE1s/K3jqQpYyIx3DXpjGft5BiLSxmGiZWwChGjJ//iZEkNE3zPX7bLHyNxTPqLR9jysiwxDYNU824gVXibhrA6leSrYzMakpAUcwllgQxnSEDawDUxoK7m3KaqACAnkFzr8gLB07hT87o5NMLLbUxxIxRz7RqGisBc3NrKemr4nmZfw9mN41sZUQsHGUpI+KCNUDY2iuhkKqJhtGPNC9CCuVqiM9bU00U4ZDpS2DKyKL2OrTWmQbnVEbfZ+S3lRIJh9BUE8XgeNosRgIa7QVMD9GokcIqs02jH988R9m/y13NNXjo0xdZVCqCmK6QMhIAiclo02SsxQibmACArftPQdM0YZpGLzzY3aJoYj0xbCgjDZINrAHtppHtGbEYWGUpI7a0W3M3TenHZ8UT94wUaNOInpEW29g22+K8Zn4L//esODgxnCwpmIzlmSyZWW98bf3vA2MpaJomdbRXTJpNZrLcrBtEMSJbGQGAs+ea5mGCmM6QMhIAFs9IQAbWHGVE2Li6++gQTo6keD4C69nXxyMYHE9b9tPIUkaCNbCKOSOSn89YEJ4Rm4FVoo/BXoy4bdPYp1zWnz8PyUwW68+fxz82syGO4YmMXoz49IwAwOp5LTjcP47zFrYCAJpr9K+dyWoYSWZ8m2OdEFti4u+ErKJVVJdkKyMEQZhQMRIA4oUhqNFelurJOCa0adSsht+/fhyAPjbIzscphbVP2mhvcG0asW8v38AahGfEZmDNyNtrwj0jKfs0TeE2TYttod/MxgS+8N7llo/NaIhj34lRnBhJllQw3Hb1WfjK+89Ak/E1a2Jhvq9pYCzNlREZBtYGIfRszCjQYpGQpSVVCnWWNg0JyQQRFPTbFQDWC9wkKSNGMcLiCh5/RS9GmmqjPMOABYaJKawy9tIAwbZpYpEQnziQHXomI2fETo6BNSXPwGrf3FtwmqaAMuLEDCMrRVdG/OWMAPrUT5Ot+BFTWIcn5I32soJrNJWRupeG0RCgZ4QgCBMqRgJgcuLgnT0jFyxsAwA88+ZJANY74jqbMjKWynCZv5SNvUCw0zSAeWGVHnoWYM4IN7Cyrb0Sjs+eV/ZzGyswPVJfwDPiBJuo6R2awLBx7k0SfB2AWQwd6R/nvg4poWfG9z08keHtx1qpY+XBekYIgtCh364ACLZNY1yMcooRXRm58uwuAOY+jWbhjrjWtlSMqSl1sXDJknlEWOInnqcs2EVRhrQvUupuGifsRR9TRmQ8J+y1NZETelasTeNGGdGLkbdOjkIzCgYZEy+AqYx0n9KX9sXCISkXd9HAKjtjBAh2moYgCBPyjARAkLtp2JKzowPm9IymabwYuWhJO9rrYzw/RFRGmKrApmmY6bVDUs5BPBJCJqUiGlak9ewZn3vPaXh2Xx9Wz22WetwgMmF42m1SRTarcc+IlJwRZmB1E3oWz+8ZcYIVI/uMjbuJaEhagcZMrKwYaayJSIlAF1Wo8TRr08h7W6NihCAmB1JGAoC9acXC8ox0jIXtdQCAt04Ise9jaa6EdDQm+BQDYL0jro1blZHjRjHS2SipGDG+74TENeuM95/VhY0fWomI5IVi4gUmiJwR0Wgs42Jm94wUmqYRVY1mD8rIQVYwSFSh2HgvK0ZkKVz1ooFV4l4afnxxmobaNAQRGPTbFQDsziwI9/3iGXoxcrh/DEnDi8D8Iu31ccQiIbxtgVmMiBch1j7gysigbl6VVowYb9YyN/YGDSucZK6Ht4ybCu00mdM05m6aAqFnwk6fVg+eEdUwdcjIAWE02ZURSe0fXoykMtyYLbMYsXhGSBkhiMCgYiQAmL9B5ps5Y0ZDHPXxCLIa0N2nv7Ez7weLlbYqI7lL5tiUB1NGZsouRqpoBJLljMQj8tbDm0sJM9zbIUsls09Tud1N46VNw5BVMIhf/0i/XjhLU0aMc9Q0fSUCILdN00AGVoKYFMgzEgCnddTjpsuWYXlng/RjK4qChe112HVkEPtOjGJpRwP3i3QaxcjyzkY0JCIYnshY2jTsLpLJ2WabRk4cNWtzBNGmCYr5bXU4a04Tlkn8WdUL7TCzWJBzIfMSelYbDaMhHsF4WsXMhuIFZ2tdDCEF5rSLxGKaGVgzXHWR89ZTEw3zc+4d1l/PMg2sdeQZIYhJgYqRAFAUBde/e0lgx180Qy9G9p/UfSN2ZSQcUvCeFR14cMcRLJ9lXmRrbWFczMDaKcvAalxwZY8zB0k0HMLDn3m71GOam2QzUidpAAcDa4HR3lBIwb9fey7GUmpO7ocT4ZCCtvo439wr0zNi96zIOraiKKiL64V3r3HeUj0jVIwQxKRAxUgVsqhd3/nx1gl96sGujADAN/98FW66fBlmNZl7L+piNmXE+Hcdkto0iSpURoKAXcBSmSxvickq0ETPSFrNcqUh36TO+YvaPB1/hliMSFIvgNzRYlkjw4D+fA9PZHBiSD/voEZ7qU1DEMFBv11VyCLDxPoWU0aG9D68uIo8Gg5ZChHAGgefzWr8TlK2MiJr4Vy1IqoULG5fVt5MjRB6Nyak8Mq6AIu+EbnKiPVYMo/NCoYTRppwbVRioUO7aQhiUpjeV40qhY33sjYNV0YaC2//rBNGTvtGU8hkNShK6VHwDHbnKDvordqIRUKIGSPILG5f1iRGgk9EqbxFEwkpPC6/VMR19kF4RhgylRH2uu412o4ylymKx6omYzZBVBv021WFMGXk1GgKA2OpHM9IPsQwLmZeba+PIyopu4MbWKd5MQKYF7E+oxipkXQhqxXaNMzEKrP4C0wZqbF5RiQWOg18P438BNZ4JMwLS3pdE0RwUDFShdTGIrzw2HFogMv1xdotYs5Iz6DcwDOgOkd7g4IVfieNcVPZBtaJlMrzYmRefC3FiETPSCwS4q8/QG6hU2cz78o0sAJmq4Y8IwQRHPTbVaWwVs2WfX0A9ByHYhc8nsCaUs0oeJnFSJTuIBnMx3BymCkj8g2shfbS+CUoZQSwTtRINbDajlUj0TMCAOvOm4vzFrZKHf8mCMIKTdNUKYtm1OHZfX34o7Gdt7OpsF8EMJWRVCaLI8Zum84mOX4RwGzTTHfPCGAqIyyIS9ZzwhclptSCgWd+mVEfjGcE0H0j7HUn89jixAsgXxm56bLlUo9HEEQunpSRO++8E6tWrUJjYyMaGxuxdu1aPProo3kf/+Mf/xiKolj+JBLy7sSnM2y899VjQwCK+0UA65QHGwvucBGG5Zb3nzULa+a34IpVs6Qds1rhbRrJBlbWkklmsjwvJjhlRO69SktQykjAxQhBEMHj6R1hzpw5uOWWW7B06VJomob/+I//wJVXXokdO3bgjDPOcPw3jY2N2LNnD/+7rMjt6c5Cw8TKVr27Gc9lUx4pNcsX7cna2AsAa+a34hefulDa8aqZem5glauMiBfaU6Np42PyLuwzApqmAWAJXpOqjNjbNFSMEETV4eld7P3vf7/l71//+tdx55134rnnnstbjCiKgs7OTv9nSDiy2FBGGLNcej9q42GkxrI4aOy1kWlgJUzqbHuAZJl6xUC5/jG55lhA3xnzv86fh4mUijYXy/W8wPbTKApQL7GAqstRRqj7TBDVhu/fWlVV8cADD2B0dBRr167N+7iRkRHMnz8f2WwW55xzDr7xjW/kLVwYyWQSyWSS/31oaMjvaU5ZZrfUIBYJIWWsqHcbXFYXi2BgLI2U6u3fEd6wXyBlKSOhkIJ4JIRkJstVF9ltiW98cKXU4zFYm6Y+HkFIwtJARoPtua4jZYQgqg7Pt2u7du1CfX094vE4PvnJT+Khhx7C6aef7vjYZcuW4e6778bDDz+Me+65B9lsFhdeeCEOHz5c8Gts3LgRTU1N/M/cuXO9nuaUJxxSsKCtlv/dnraaD/uFS+Y0DWFi9zHIVC9YG+LUqPxdLEHCt1lLntLJKfyq5PkgCMLEczGybNky7Ny5E88//zw+9alP4dprr8Wrr77q+Ni1a9diw4YNOPvss3HxxRfjwQcfxIwZM3DXXXcV/Bo333wzBgcH+Z9Dhw55Pc1pARvvBdwrHLWWxV8h6SZFQsd+gUxIvEAylaVPcoZJ0DBlRKZ5FXAysNJrmiCqDc+/tbFYDEuW6Btp16xZgxdeeAHf+ta3ihYYABCNRrF69Wq8+eabBR8Xj8cRj8sbOZ2qLJpRD+A4AHfTNIBprAR0vwgZioOh3hZJLnPc2VRGgmnTBAXL6TitQ25eh1iMxCIhhCW2gAiCmBxKvoXIZrMWf0chVFXFrl278L73va/UL0sAWGQoI42JSM6deD7Eu0Zq0QSH/e5cZiotK2yqrRg5c3YTnr7p3eiQmG0DWKdpquW5IAjCiqdi5Oabb8Z73/tezJs3D8PDw7jvvvuwadMmPP744wCADRs2YPbs2di4cSMA4Ktf/SouuOACLFmyBAMDA7j11ltx8OBBfOxjH5P/nUxDVs1pBgBPyZCiuY/Mq8ERlIFVPFa1tWkAYJ7gc5KFuMyutoqeC4IgTDwVI729vdiwYQOOHTuGpqYmrFq1Co8//jje8573AAC6u7sRCpl3gP39/fj4xz+Onp4etLS0YM2aNXj22WfzGl4JbyzrbMAvPnUh5ra6M68CVs8IjfUGx2QYWNkk1XT3SDTETUMsmVcJojrx9C72wx/+sODnN23aZPn77bffjttvv93zSRHuWTO/xdPjRWWE2jTBYV9jL7MYsR9rurcmEtEQQgqQ1XIVKYIgqgNalDfNEN+sqRgJDrsyIrNNYy8+prsaoCgKf75pLxJBVCdUjEwzxHXrMpfkEVZyRnsDMLDm+/t0hBUj010lIohqhYqRaUZtnNo0k0GQQVzUpsmFTdRMd/8MQVQrVIxMM0RlZKbEjb2EFXskubhTplTshc10b9MApjJCzwVBVCdUjEwz2F10e30MsQj9+IMiEg5ZWjMyL5LUpsmljto0BFHV0NVomjG7RR8DXjKzvsgjiVIRTaxxiYWf/YJLrQkzYp6UEYKoTuhdbJpxRlcT7vvY+UaUPBEkdfEITo6kkIiGpMbu2z0jdAEObgkfQRCTAxUj05ALl7SX+xSmBcyfI7uNQm2aXK65YAHSqoarVs8u96kQBOEDKkYIIiBY8JnsuHZRCYmEFPL+ADi9qxH/cvVZ5T4NgiB8Qu9iBBEQdQEFcYnHoxYNQRBTASpGCCIgWDESD1AZoRYNQRBTASpGCCIg6rlnRO6vmViA0CgrQRBTASpGCCIgmDISpGekhsZ6CYKYAlAxQhABUW8YWAP1jEhWXQiCIMoBvZMRRECwfSkJya2UhKVNQ8oIQRDVDxUjBBEQl6zowOp5zfiQ5OyL2hhN0xAEMbWg2yqCCIjFM+rx0Kcvkn7cRJSmaQiCmFqQMkIQVUZYCDqjaRqCIKYCVIwQRBXCFBFq0xAEMRWgYoQgqhBejFCbhiCIKQAVIwRRhbD2DLVpCIKYClAxQhBVSIK3aciDThBE9UPFCEFUIcwrQm0agiCmAlSMEEQV0lYXAwC018fKfCYEQRClQxovQVQhX3zfCrzztBl417KZ5T4VgiCIkqFihCCqkAXtdVjQXlfu0yAIgpACtWkIgiAIgigrVIwQBEEQBFFWqBghCIIgCKKsUDFCEARBEERZoWKEIAiCIIiyQsUIQRAEQRBlhYoRgiAIgiDKChUjBEEQBEGUFSpGCIIgCIIoK1SMEARBEARRVqgYIQiCIAiirFAxQhAEQRBEWaFihCAIgiCIslIVW3s1TQMADA0NlflMCIIgCIJwC7tus+t4PqqiGBkeHgYAzJ07t8xnQhAEQRCEV4aHh9HU1JT384pWrFypALLZLI4ePYqGhgYoiiLtuENDQ5g7dy4OHTqExsZGacetJug5oOcAoOcAoOcAoOdgun//gPznQNM0DA8Po6urC6FQfmdIVSgjoVAIc+bMCez4jY2N0/aFx6DngJ4DgJ4DgJ4DgJ6D6f79A3Kfg0KKCIMMrARBEARBlBUqRgiCIAiCKCvTuhiJx+P48pe/jHg8Xu5TKRv0HNBzANBzANBzANBzMN2/f6B8z0FVGFgJgiAIgpi6TGtlhCAIgiCI8kPFCEEQBEEQZYWKEYIgCIIgygoVIwRBEARBlJVpXYx897vfxYIFC5BIJHD++edj69at5T6lQNi4cSPe9ra3oaGhATNnzsRVV12FPXv2WB4zMTGB66+/Hm1tbaivr8ef/dmf4fjx42U64+C55ZZboCgKbrzxRv6x6fAcHDlyBB/96EfR1taGmpoarFy5Ei+++CL/vKZp+Id/+AfMmjULNTU1uPTSS7F3794ynrFcVFXFl770JSxcuBA1NTVYvHgxvva1r1n2Zky15+Dpp5/G+9//fnR1dUFRFPzyl7+0fN7N93vq1CmsX78ejY2NaG5uxnXXXYeRkZFJ/C5Ko9BzkE6n8fnPfx4rV65EXV0durq6sGHDBhw9etRyjKn8HNj55Cc/CUVR8P/+3/+zfDzI52DaFiM/+9nP8LnPfQ5f/vKXsX37dpx11lm47LLL0NvbW+5Tk87mzZtx/fXX47nnnsMTTzyBdDqNP/mTP8Ho6Ch/zGc/+1n8+te/xgMPPIDNmzfj6NGj+NCHPlTGsw6OF154AXfddRdWrVpl+fhUfw76+/tx0UUXIRqN4tFHH8Wrr76K2267DS0tLfwx//zP/4xvf/vb+N73vofnn38edXV1uOyyyzAxMVHGM5fHN7/5Tdx5552444478Nprr+Gb3/wm/vmf/xnf+c53+GOm2nMwOjqKs846C9/97ncdP+/m+12/fj1eeeUVPPHEE/jNb36Dp59+Gp/4xCcm61somULPwdjYGLZv344vfelL2L59Ox588EHs2bMHH/jAByyPm8rPgchDDz2E5557Dl1dXTmfC/Q50KYp5513nnb99dfzv6uqqnV1dWkbN24s41lNDr29vRoAbfPmzZqmadrAwIAWjUa1Bx54gD/mtdde0wBoW7ZsKddpBsLw8LC2dOlS7YknntAuvvhi7YYbbtA0bXo8B5///Oe1t7/97Xk/n81mtc7OTu3WW2/lHxsYGNDi8bj205/+dDJOMXCuuOIK7S//8i8tH/vQhz6krV+/XtO0qf8cANAeeugh/nc33++rr76qAdBeeOEF/phHH31UUxRFO3LkyKSduyzsz4ETW7du1QBoBw8e1DRt+jwHhw8f1mbPnq3t3r1bmz9/vnb77bfzzwX9HExLZSSVSmHbtm249NJL+cdCoRAuvfRSbNmypYxnNjkMDg4CAFpbWwEA27ZtQzqdtjwfy5cvx7x586bc83H99dfjiiuusHyvwPR4Dn71q1/h3HPPxdVXX42ZM2di9erV+MEPfsA/v3//fvT09Fieg6amJpx//vlT5jm48MIL8eSTT+KNN94AALz00kt45pln8N73vhfA9HgORNx8v1u2bEFzczPOPfdc/phLL70UoVAIzz///KSf82QwODgIRVHQ3NwMYHo8B9lsFtdccw1uuukmnHHGGTmfD/o5qIpFebI5efIkVFVFR0eH5eMdHR14/fXXy3RWk0M2m8WNN96Iiy66CGeeeSYAoKenB7FYjP/iMTo6OtDT01OGswyG+++/H9u3b8cLL7yQ87np8By89dZbuPPOO/G5z30OX/ziF/HCCy/gr//6rxGLxXDttdfy79Pp92KqPAdf+MIXMDQ0hOXLlyMcDkNVVXz961/H+vXrAWBaPAcibr7fnp4ezJw50/L5SCSC1tbWKfmcTExM4POf/zzWrVvHF8VNh+fgm9/8JiKRCP76r//a8fNBPwfTshiZzlx//fXYvXs3nnnmmXKfyqRy6NAh3HDDDXjiiSeQSCTKfTplIZvN4txzz8U3vvENAMDq1auxe/dufO9738O1115b5rObHH7+85/j3nvvxX333YczzjgDO3fuxI033oiurq5p8xwQ+Umn0/jwhz8MTdNw5513lvt0Jo1t27bhW9/6FrZv3w5FUcpyDtOyTdPe3o5wOJwzKXH8+HF0dnaW6ayC5zOf+Qx+85vf4KmnnsKcOXP4xzs7O5FKpTAwMGB5/FR6PrZt24be3l6cc845iEQiiEQi2Lx5M7797W8jEomgo6Njyj8Hs2bNwumnn2752IoVK9Dd3Q0A/Pucyr8XN910E77whS/gIx/5CFauXIlrrrkGn/3sZ7Fx40YA0+M5EHHz/XZ2duYY+zOZDE6dOjWlnhNWiBw8eBBPPPEEV0WAqf8c/OEPf0Bvby/mzZvH3x8PHjyIv/mbv8GCBQsABP8cTMtiJBaLYc2aNXjyySf5x7LZLJ588kmsXbu2jGcWDJqm4TOf+Qweeugh/P73v8fChQstn1+zZg2i0ajl+dizZw+6u7unzPNxySWXYNeuXdi5cyf/c+6552L9+vX8/6f6c3DRRRfljHS/8cYbmD9/PgBg4cKF6OzstDwHQ0NDeP7556fMczA2NoZQyPq2Fw6Hkc1mAUyP50DEzfe7du1aDAwMYNu2bfwxv//975HNZnH++edP+jkHAStE9u7di9/97ndoa2uzfH6qPwfXXHMNXn75Zcv7Y1dXF2666SY8/vjjACbhOSjZAlul3H///Vo8Htd+/OMfa6+++qr2iU98QmtubtZ6enrKfWrS+dSnPqU1NTVpmzZt0o4dO8b/jI2N8cd88pOf1ObNm6f9/ve/11588UVt7dq12tq1a8t41sEjTtNo2tR/DrZu3apFIhHt61//urZ3717t3nvv1Wpra7V77rmHP+aWW27RmpubtYcfflh7+eWXtSuvvFJbuHChNj4+XsYzl8e1116rzZ49W/vNb36j7d+/X3vwwQe19vZ27e/+7u/4Y6baczA8PKzt2LFD27FjhwZA+9d//Vdtx44dfFLEzfd7+eWXa6tXr9aef/557ZlnntGWLl2qrVu3rlzfkmcKPQepVEr7wAc+oM2ZM0fbuXOn5T0ymUzyY0zl58AJ+zSNpgX7HEzbYkTTNO073/mONm/ePC0Wi2nnnXee9txzz5X7lAIBgOOfH/3oR/wx4+Pj2qc//WmtpaVFq62t1T74wQ9qx44dK99JTwL2YmQ6PAe//vWvtTPPPFOLx+Pa8uXLte9///uWz2ezWe1LX/qS1tHRocXjce2SSy7R9uzZU6azlc/Q0JB2ww03aPPmzdMSiYS2aNEi7e///u8tF52p9hw89dRTjr//1157raZp7r7fvr4+bd26dVp9fb3W2Nio/cVf/IU2PDxchu/GH4Weg/379+d9j3zqqaf4Mabyc+CEUzES5HOgaJoQPUgQBEEQBDHJTEvPCEEQBEEQlQMVIwRBEARBlBUqRgiCIAiCKCtUjBAEQRAEUVaoGCEIgiAIoqxQMUIQBEEQRFmhYoQgCIIgiLJCxQhBEARBEGWFihGCIAiCIMoKFSMEQRAEQZQVKkYIgiAIgigrVIwQBEEQBFFW/j8StEupmt1m6wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a54acc-1dce-4de4-9d5f-d0582f5097c5",
   "metadata": {},
   "source": [
    "**To tell if the model is working I'm looking at test_bwd/fwd_pct_correct and seeing if that is doing better than chance (1/batch_size)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "found2",
   "language": "python",
   "name": "found2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
