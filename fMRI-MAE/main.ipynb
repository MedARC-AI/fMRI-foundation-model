{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-kaladin/fmri/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from models import get_vit\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "\n",
    "import lightning as pl\n",
    "from typing import List\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.strategies import DDPStrategy\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, RichModelSummary, RichProgressBar\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4064ce98-5adc-4b87-8d39-e4f6e0be8456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08179db-9c6a-4bc6-a245-79fae6884ca2",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b1c3fe-28ab-40b7-8906-c6a9c8070d00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'test_ddp_lightning6', 'use_cls_token': False, 'use_contrastive_loss': False, 'constrastive_loss_weight': 1.0, 'global_batch_size': 4, 'num_workers': 8, 'max_steps': 10000, 'eval_per_n_steps': 2000, 'limit_val_batches': 800, 'seed': 42, 'max_lr': 3e-05, 'ckpt_saving': True, 'ckpt_interval': 50, 'resume_from_ckpt': False, 'wandb_log': True, 'tube_start_masking_ratio': 0.75, 'tube_end_masking_ratio': 0.75, 'decoder_mask_ratio': 0.75, 'encoder_model': 'vit_base', 'decoder_model': 'vit_small', 'patch_size': 8, 'frame_patch_size': 1, 'use_rope_emb': False, 'masking_strategy': 'MNI', 'img_size': [88, 104, 72], 'num_frames': 4, 'is_s3': False, 'test_urls': '/weka/proj-fmri/souvik/foundational_model/000000.tar', 'train_urls': '/weka/proj-fmri/souvik/foundational_model/000000.tar'}\n",
      "outdir /weka/proj-fmri/souvik/foundational_model/ckpts/test_ddp_lightning6\n",
      "global_batch_size 4\n",
      "use_cls_token False\n",
      "num_patches 5148\n",
      "num_encoder_patches 1287\n",
      "num_decoder_patches 1287\n"
     ]
    }
   ],
   "source": [
    "print(config)\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "if use_contrastive_loss:\n",
    "    global_batch_size = global_batch_size // 2 # contrastive loss doubles the batch size with the same samples and different masks\n",
    "print(\"global_batch_size\", global_batch_size)\n",
    "\n",
    "use_cls_token = True if use_contrastive_loss else use_cls_token\n",
    "print(\"use_cls_token\", use_cls_token)\n",
    "\n",
    "num_patches = int(\n",
    "    (img_size[0] / patch_size)\n",
    "    * (img_size[1] / patch_size)\n",
    "    * (img_size[2] / patch_size)\n",
    "    * num_frames\n",
    ")\n",
    "num_patches_per_timepoint = num_patches // num_frames\n",
    "num_encoder_patches = int(num_patches_per_timepoint * (1 - tube_start_masking_ratio) * num_frames)\n",
    "num_decoder_patches = int(num_patches_per_timepoint * (1 - decoder_mask_ratio) * num_frames)\n",
    "print(\"num_patches\", num_patches)\n",
    "print(\"num_encoder_patches\", num_encoder_patches)\n",
    "print(\"num_decoder_patches\", num_decoder_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5313372-a58c-497d-a265-ef83480ebcaf",
   "metadata": {},
   "source": [
    "# DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b32b46c3-3cbe-45b7-89b3-21caa6ff9d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataloader import fMRIDataModule\n",
    "datamodule = fMRIDataModule(train_urls=train_urls, test_urls=test_urls, batch_size=global_batch_size,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d05a32-12eb-494a-82df-dce4c7e9924c",
   "metadata": {},
   "source": [
    "# Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4963686-7d07-448f-b519-d93a8c71848d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FMRITrainer(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder_model: str, \n",
    "        decoder_model: str,\n",
    "        img_size: List[int],\n",
    "        patch_size: int,\n",
    "        num_frames: int,\n",
    "        frame_patch_size: int,\n",
    "        use_rope_emb: bool,\n",
    "        use_cls_token: bool,\n",
    "        masking_strategy: str,\n",
    "        max_steps: int,\n",
    "        tube_start_masking_ratio: float,\n",
    "        tube_end_masking_ratio: float,\n",
    "        use_contrastive_loss: bool,\n",
    "        decoder_mask_ratio: float,\n",
    "        constrastive_loss_weight: float,\n",
    "        max_lr: float,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(img_size) == 3 # 3D volumes\n",
    "        self.max_steps = max_steps\n",
    "        self.tube_start_masking_ratio = tube_start_masking_ratio\n",
    "        self.tube_end_masking_ratio = tube_end_masking_ratio\n",
    "        self.masking_strategy = masking_strategy\n",
    "        self.num_frames = num_frames\n",
    "        self.decoder_mask_ratio = decoder_mask_ratio\n",
    "        self.constrastive_loss_weight = constrastive_loss_weight\n",
    "        self.max_lr = max_lr\n",
    "        self.batch_size = batch_size\n",
    "        self.use_contrastive_loss = use_contrastive_loss\n",
    "        self.use_cls_token = use_cls_token\n",
    "    \n",
    "        self.model = get_vit(\n",
    "            size={\"encoder\": encoder_model, \"decoder\": decoder_model},\n",
    "            image_size=img_size,  # depth, height, width\n",
    "            image_patch_size=(patch_size,patch_size,patch_size),  # depth, height, width patch size\n",
    "            frames=num_frames,\n",
    "            frame_patch_size=frame_patch_size,\n",
    "            channels=1,\n",
    "            use_rope_emb=use_rope_emb,\n",
    "            use_cls_token=use_cls_token,\n",
    "        )\n",
    "        self.aug_transform = utils.DataPrepper(\n",
    "            num_frames=num_frames,\n",
    "            masking_strategy=masking_strategy,\n",
    "            patch_depth=patch_size,\n",
    "            patch_height=patch_size,\n",
    "            patch_width=patch_size,\n",
    "            frame_patch_size=frame_patch_size,\n",
    "        )\n",
    "        self.num_patches = int(\n",
    "            (img_size[0] / patch_size)\n",
    "            * (img_size[1] / patch_size)\n",
    "            * (img_size[2] / patch_size)\n",
    "            * (num_frames/frame_patch_size)\n",
    "        )\n",
    "        \n",
    "        if self.masking_strategy==\"MNI\":\n",
    "            MNI_brain = nib.load(\"/weka/proj-fmri/paulscotti/old_fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "            brain_pos_voxels = MNI_brain[6:94,8:112,10:82]\n",
    "            self.brain_pos_pats = self.model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "            self.brain_pos_pats_vit = rearrange(self.brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "            \n",
    "        ## LOSS\n",
    "        self.mse = nn.MSELoss()\n",
    "        if use_contrastive_loss:\n",
    "            self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))  # learned logit scale\n",
    "\n",
    "\n",
    "    def shared_step(self, batch, batch_idx, phase: str):\n",
    "        tube_mask_ratio = utils.get_masking_ratio(\n",
    "            current_epoch=self.global_step, \n",
    "            total_epochs=self.max_steps, \n",
    "            start_masking_ratio=self.tube_start_masking_ratio, \n",
    "            end_masking_ratio=self.tube_end_masking_ratio\n",
    "        )\n",
    "        \n",
    "        input_func = batch['func.npy'].to(self.device)\n",
    "        if self.masking_strategy==\"MNI\":\n",
    "            func, _ = self.aug_transform(input_func)\n",
    "            brain_pos_pats = self.brain_pos_pats\n",
    "            brain_pos_pats_vit = self.brain_pos_pats_vit\n",
    "        else:\n",
    "            func, brain_pos_voxels = self.aug_transform(input_func)\n",
    "            brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "            brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "        \n",
    "        if self.use_contrastive_loss and phase==\"train\":  # create positive pairs by duplicating the batch\n",
    "            func = torch.cat([func, func], dim=0)\n",
    "            brain_pos_pats = torch.cat([brain_pos_pats, brain_pos_pats], dim=0)\n",
    "            brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "        \n",
    "        func = func.unsqueeze(1)\n",
    "        batch_size=func.shape[0]\n",
    "        # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "        tube_mask = torch.zeros(self.num_patches // self.num_frames).to(torch.bool).to(self.device)\n",
    "        batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "        mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "        # check if there's not enough brain left for code to continue\n",
    "        if len(mask_idx_candidates) < (\n",
    "            int(self.num_patches/self.num_frames*(1-tube_mask_ratio))+int(self.num_patches/self.num_frames*(1-self.decoder_mask_ratio))):\n",
    "            print(\"Brain volume skipped due to not enough brain-positive patches remaining...\")\n",
    "            return\n",
    "        mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "        tube_idx = mask_idx_candidates[:int(self.num_patches / self.num_frames * (1 - tube_mask_ratio))]\n",
    "        tube_mask[tube_idx] = True\n",
    "        tube_mask = tube_mask.tile(self.num_frames)\n",
    "        \n",
    "        # create decoder mask\n",
    "        decoder_mask = torch.zeros(self.num_patches // self.num_frames).to(torch.bool).to(self.device)\n",
    "        remaining_mask_idx = mask_idx_candidates[int(self.num_patches / self.num_frames * (1 - tube_mask_ratio)):]\n",
    "        decoder_mask_idx = remaining_mask_idx[:int(self.num_patches / self.num_frames * (1 - self.decoder_mask_ratio))]\n",
    "        decoder_mask[decoder_mask_idx] = True\n",
    "        decoder_mask = decoder_mask.tile(self.num_frames)\n",
    "        \n",
    "        # encode the tube patches\n",
    "        encoder_out = self.model(func.to(self.device), encoder_mask=tube_mask.to(self.device))\n",
    "        if self.use_cls_token:\n",
    "            enc_cls_token = encoder_out[:,:1,:]\n",
    "\n",
    "        # decode both the encoder_out patches and masked decoder patches\n",
    "        decoder_out = self.model(encoder_out, encoder_mask=tube_mask, decoder_mask=decoder_mask)\n",
    "        # subset only the reconstructed decoder patches\n",
    "        output = decoder_out[:, -decoder_mask.sum():]\n",
    "        \n",
    "        # compare to ground truth and calculate loss\n",
    "        target_patches = self.model.patchify(func)\n",
    "        target_patches_vit = rearrange(target_patches, \"b ... d -> b (...) d\")\n",
    "        target = target_patches_vit[:, decoder_mask]\n",
    "        loss = self.mse(output, target)\n",
    "        self.log(f\"{phase}/recon_loss\", loss, on_step=True, on_epoch=False, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        # contrastive loss\n",
    "        if self.use_contrastive_loss and phase==\"train\":\n",
    "            n_b = len(func) // 2\n",
    "            cls_token1 = enc_cls_token[:n_b, 0, :]  # first half of batch, cls_token shape B, 1, d_model\n",
    "            cls_token2 = enc_cls_token[n_b:, 0, :]\n",
    "            contrastive_loss = utils.contrastive_loss(cls_token1, cls_token2, temperature=self.logit_scale)\n",
    "            cnt_loss = self.constrastive_loss_weight * contrastive_loss\n",
    "            self.log(f\"{phase}/contrastive_loss\", cnt_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "            loss += cnt_loss\n",
    "        self.log(f\"{phase}/loss\", loss, on_step=True, on_epoch=False, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        return loss\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, batch_idx, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, batch_idx, \"val\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, batch_idx, \"test\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        opt_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)]},\n",
    "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)]},\n",
    "        ]\n",
    "        self.optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=self.max_lr)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.max_lr,\n",
    "            total_steps=self.max_steps\n",
    "        )\n",
    "        # return self.optimizer\n",
    "        return [self.optimizer], [self.lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c721e70f-a053-43eb-b34a-ce7c6920e3a0",
   "metadata": {},
   "source": [
    "# Callbacks and trainers\n",
    "> Currently using\n",
    "> - Save Checkpoint\n",
    "> - LR monitor to log lr\n",
    "> - Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "143cd8ae-4faa-43e7-9243-10e91a037071",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-kaladin/fmri/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.11 /admin/home-kaladin/fmri/lib/python3.11/site-pac ...\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.abspath(f'../ckpts/{model_name}'),\n",
    "    filename='{epoch}-{val/loss:.5f}',\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='val/loss',\n",
    "    mode='min',\n",
    ")\n",
    "callbacks = [\n",
    "    LearningRateMonitor(),\n",
    "    checkpoint_callback,\n",
    "    RichModelSummary(max_depth=2),\n",
    "    RichProgressBar()\n",
    "]\n",
    "trainer = Trainer(\n",
    "    devices=\"auto\",\n",
    "    num_nodes=int(os.getenv('NUM_NODES', 1)),\n",
    "    precision=\"16-mixed\",\n",
    "    logger=WandbLogger(project='found', id=model_name, name=model_name),\n",
    "    callbacks=callbacks,\n",
    "    max_steps=max_steps,\n",
    "    ddp = DDPStrategy(process_group_backend=\"gloo\"),\n",
    "    # val_check_interval=eval_per_n_steps,\n",
    "    # limit_train_batches=100, \n",
    "    # max_epochs=20,\n",
    "    log_every_n_steps=10,\n",
    "    # limit_val_batches=100\n",
    "    # overfit_batches=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "088d211f-583d-4d0e-bbef-e217d5996d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "module = FMRITrainer(\n",
    "    encoder_model=encoder_model, \n",
    "    decoder_model=decoder_model, \n",
    "    img_size=img_size, \n",
    "    patch_size=patch_size, \n",
    "    num_frames=num_frames, \n",
    "    masking_strategy=masking_strategy,\n",
    "    tube_start_masking_ratio=tube_start_masking_ratio, \n",
    "    tube_end_masking_ratio=tube_end_masking_ratio,\n",
    "    decoder_mask_ratio=decoder_mask_ratio, \n",
    "    frame_patch_size=frame_patch_size, \n",
    "    use_rope_emb=use_rope_emb, \n",
    "    use_cls_token=use_cls_token,\n",
    "    max_lr=max_lr, \n",
    "    max_steps=max_steps,\n",
    "    batch_size=global_batch_size,\n",
    "    use_contrastive_loss=use_contrastive_loss,\n",
    "    constrastive_loss_weight=constrastive_loss_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b11e418f-2705-4e9a-8c70-c1e4ea1e01a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmandalsouvik76\u001b[0m (\u001b[33msouvikmandal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240318_191518-MAE_2node_2gpu_32gbs_d_lit</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/souvikmandal/found/runs/MAE_2node_2gpu_32gbs_d_lit' target=\"_blank\">MAE_2node_2gpu_32gbs_d_lit</a></strong> to <a href='https://wandb.ai/souvikmandal/found' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/souvikmandal/found' target=\"_blank\">https://wandb.ai/souvikmandal/found</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/souvikmandal/found/runs/MAE_2node_2gpu_32gbs_d_lit' target=\"_blank\">https://wandb.ai/souvikmandal/found/runs/MAE_2node_2gpu_32gbs_d_lit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding decoder ImageHandler to decoders.\n",
      "Adding decoder ImageHandler to decoders.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1,2,5,6]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                 </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model                     │ VisionTransformerMAE │  107 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ model.encoder_transformer │ Transformer          │ 85.0 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ model.decoder_transformer │ Transformer          │ 21.3 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ model.patchify            │ Rearrange            │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ model.unpatchify          │ Sequential           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ model.patch_to_emb        │ Sequential           │  396 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ model.encoder_to_decoder  │ Linear               │  294 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>│ model.decoder_proj        │ Sequential           │  197 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span>│ mse                       │ MSELoss              │      0 │\n",
       "└───┴───────────────────────────┴──────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model                     │ VisionTransformerMAE │  107 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ model.encoder_transformer │ Transformer          │ 85.0 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ model.decoder_transformer │ Transformer          │ 21.3 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ model.patchify            │ Rearrange            │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ model.unpatchify          │ Sequential           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ model.patch_to_emb        │ Sequential           │  396 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ model.encoder_to_decoder  │ Linear               │  294 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m7\u001b[0m\u001b[2m \u001b[0m│ model.decoder_proj        │ Sequential           │  197 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m8\u001b[0m\u001b[2m \u001b[0m│ mse                       │ MSELoss              │      0 │\n",
       "└───┴───────────────────────────┴──────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 107 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 107 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 428                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 107 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 107 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 428                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4334a861b91648d592f6a09a5a50eda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">####################################################################################################\n",
       "</pre>\n"
      ],
      "text/plain": [
       "####################################################################################################\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Building dataloader with the following parameters\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Building dataloader with the following parameters\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">batch_size: 32, num_workers: 0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "batch_size: 32, num_workers: 0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">####################################################################################################\n",
       "</pre>\n"
      ],
      "text/plain": [
       "####################################################################################################\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/weka/proj-fmri/souvik/foundational_model/found/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/d\n",
       "ata_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider \n",
       "increasing the value of the `num_workers` argument` to `num_workers=39` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/weka/proj-fmri/souvik/foundational_model/found/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/d\n",
       "ata_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider \n",
       "increasing the value of the `num_workers` argument` to `num_workers=39` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">####################################################################################################\n",
       "</pre>\n"
      ],
      "text/plain": [
       "####################################################################################################\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Building dataloader with the following parameters\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Building dataloader with the following parameters\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">batch_size: 32, num_workers: 0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "batch_size: 32, num_workers: 0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">####################################################################################################\n",
       "</pre>\n"
      ],
      "text/plain": [
       "####################################################################################################\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/weka/proj-fmri/souvik/foundational_model/found/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/d\n",
       "ata_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider \n",
       "increasing the value of the `num_workers` argument` to `num_workers=39` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/weka/proj-fmri/souvik/foundational_model/found/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/d\n",
       "ata_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider \n",
       "increasing the value of the `num_workers` argument` to `num_workers=39` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/weka/proj-fmri/souvik/foundational_model/found/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: \n",
       "Detected KeyboardInterrupt, attempting graceful shutdown...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/weka/proj-fmri/souvik/foundational_model/found/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: \n",
       "Detected KeyboardInterrupt, attempting graceful shutdown...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(module, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75489b08-edfb-4652-a6d9-2d24fc833fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
