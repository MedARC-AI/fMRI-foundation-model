{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d078f7e7-ccbc-4a88-9d12-93510fe1814b",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56bbd92-27d1-4f94-a579-029661eb72f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from subprocess import call\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import webdataset as wds\n",
    "import nibabel as nib\n",
    "import pickle as pkl\n",
    "from einops import rearrange\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "# import torchio as tio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c1ba3-39ff-458c-bc3c-e452445eb10c",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46adfbe0-91ef-43ae-87a9-c2e4ea89033a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape_to_2d(tensor):\n",
    "    return rearrange(tensor, 'b h w c -> (b h) (c w)')\n",
    "\n",
    "def reshape_to_original(tensor_2d, h=64, w=64, c=48):\n",
    "    return rearrange(tensor_2d, \"(tr h) (c w) -> tr h w c\", h=h, w=w, c=c)\n",
    "\n",
    "def header_to_dict(header):\n",
    "    readable_header = {}\n",
    "    for key, value in header.items():\n",
    "        readable_header[key] = value\n",
    "    return readable_header\n",
    "\n",
    "def temporal_interp1d(fmri_data, change_TR):\n",
    "    original_time_points = np.arange(fmri_data.shape[0])  # Time points: 0, 1, 2, ..., T-1\n",
    "    new_time_points = np.arange(0, fmri_data.shape[0], change_TR)  # New time points: 0, 2, 4, ...\n",
    "\n",
    "    reshaped_data = fmri_data.reshape(fmri_data.shape[0], -1)  # Reshape to (T, X*Y*Z)\n",
    "    interpolate = interp1d(original_time_points, reshaped_data, kind='linear', axis=0, bounds_error=False, fill_value=\"extrapolate\")\n",
    "    resampled_fmri_data = interpolate(new_time_points).reshape((len(new_time_points),) + fmri_data.shape[1:])\n",
    "    return resampled_fmri_data\n",
    "\n",
    "def list_folders(bucket, prefix='', delimiter='/'):\n",
    "    folder_names = []\n",
    "    continuation_token = None\n",
    "    while True:\n",
    "        # Include the continuation token in the request if it exists\n",
    "        kwargs = {'Bucket': bucket, 'Prefix': prefix, 'Delimiter': delimiter}\n",
    "        if continuation_token:\n",
    "            kwargs['ContinuationToken'] = continuation_token\n",
    "\n",
    "        response = s3.list_objects_v2(**kwargs)\n",
    "        folder_names.extend([x['Prefix'].split('/')[-2] for x in response.get('CommonPrefixes', [])])\n",
    "\n",
    "        # Check if more items are available to retrieve\n",
    "        if 'NextContinuationToken' in response:\n",
    "            continuation_token = response['NextContinuationToken']\n",
    "        else:\n",
    "            break\n",
    "    return folder_names\n",
    "\n",
    "def list_all_objects(bucket, prefix):\n",
    "    continuation_token = None\n",
    "    while True:\n",
    "        if continuation_token:\n",
    "            response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, ContinuationToken=continuation_token)\n",
    "        else:\n",
    "            response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "        for content in response.get('Contents', []):\n",
    "            yield content\n",
    "\n",
    "        continuation_token = response.get('NextContinuationToken')\n",
    "        if not continuation_token:\n",
    "            break\n",
    "\n",
    "def torchio_slice(data,xslice=None,yslice=None,zslice=None):    \n",
    "    if xslice is None: xslice = data.shape[1] // 2\n",
    "    if yslice is None: yslice = data.shape[2] // 2\n",
    "    if zslice is None: zslice = data.shape[3] // 2\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(5,5))\n",
    "\n",
    "    # Plot the three different slices\n",
    "    axs[0].imshow(data[0, xslice], cmap='gray')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(f'Slice [0, {xslice}]', fontsize=8)\n",
    "\n",
    "    axs[1].imshow(data[0, :, yslice], cmap='gray')\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title(f'Slice [0, :, {yslice}]', fontsize=8)\n",
    "\n",
    "    axs[2].imshow(data[0, :, :, zslice], cmap='gray')\n",
    "    axs[2].axis('off')\n",
    "    axs[2].set_title(f'Slice [0, :, :, {zslice}]', fontsize=8)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8f6f9-ce59-46fc-9b77-43d6ce5a3920",
   "metadata": {},
   "source": [
    "## Create dir to save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9852d060-955e-47ec-ad04-1a285ee38059",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/weka/proj-fmri/paulscotti/fMRI-foundation-model/dataset_creation/temp_NSD_MNIs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/weka/proj-fmri/paulscotti/fMRI-foundation-model/dataset_creation/temp_NSD_MNIs/*': No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = \"sub-01\"\n",
    "proj_name = \"NSD_MNI\"\n",
    "outpath=f\"/weka/proj-fmri/paulscotti/fMRI-foundation-model/dataset_creation/{proj_name}\"\n",
    "os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
    "os.makedirs(f\"{outpath}/{subject}\", exist_ok=True)\n",
    "\n",
    "temp_folder = os.getcwd()+'/temp_NSD_MNIs'\n",
    "os.makedirs(temp_folder, exist_ok=True)\n",
    "print(temp_folder)\n",
    "\n",
    "# delete saved files\n",
    "command = f\"rm {temp_folder}/*\"\n",
    "call(command,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c684fb6-30f5-4413-97bc-91eac75e1aa7",
   "metadata": {},
   "source": [
    "## Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041e2607-8045-48e6-901a-fcf01e33042e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(file_name_list) = 3696\n",
      "['fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-01_bold_MNI.nii.gz', 'fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-02_bold_MNI.nii.gz', 'fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-03_bold_MNI.nii.gz', 'fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-04_bold_MNI.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'proj-fmri'\n",
    "prefix = 'fmri_foundation_datasets/NSD_MNI/'\n",
    "\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "file_name_list = []\n",
    "for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "    for obj in page.get('Contents', []):\n",
    "        file_name = obj['Key']\n",
    "        file_name_list.append(file_name)\n",
    "print(\"len(file_name_list) =\", len(file_name_list))\n",
    "print(file_name_list[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20dbcc6e-d071-4911-96f6-940f3466c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a dictionary to hold the categorized file paths\n",
    "datasets = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "for file_name in file_name_list:\n",
    "    parts = file_name.split('/')\n",
    "    dataset_id = parts[2]\n",
    "    subject_parts = parts[3].split('_')\n",
    "    subject_id = subject_parts[0]  # Extract the subject identifier\n",
    "\n",
    "    # Check for session identifier, default to \"ses-01\" if not present\n",
    "    session_id = \"ses-01\"\n",
    "    for part in subject_parts:\n",
    "        if part.startswith(\"ses-\"):\n",
    "            session_id = part\n",
    "            break\n",
    "    \n",
    "    datasets[dataset_id][subject_id][session_id].append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad98a0a3-cfc9-4614-a0a8-e390f109e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_split_by_node(urls):\n",
    "    return urls\n",
    "\n",
    "# prep hdf5 creation\n",
    "N = 188*12*40 # number of TRs in entire NSD subj01 dataset\n",
    "R = 12*40 # number of runs in entire NSD subj01 dataset\n",
    "\n",
    "# Open an HDF5 file\n",
    "f = h5py.File('subj01_mnidata.h5', 'w')\n",
    "\n",
    "# Create datasets\n",
    "global_trs_dset = f.create_dataset('global_trs', shape=(N,), dtype=np.int64)\n",
    "funcs_dset = f.create_dataset('funcs', shape=(N, 88, 104, 72), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd67c0-011b-4d49-9fc6-4c14a21d41f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ses-nsd01 sub-01 ses-nsd01 | global_tr_cnt 0\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-01_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd01_sub-01_ses-nsd01_task-nsdcore_run-01_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-02_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd01_sub-01_ses-nsd01_task-nsdcore_run-02_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-03_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd01_sub-01_ses-nsd01_task-nsdcore_run-03_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-04_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd01_sub-01_ses-nsd01_task-nsdcore_run-04_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-05_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd01_sub-01_ses-nsd01_task-nsdcore_run-05_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-06_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd01_sub-01_ses-nsd01_task-nsdcore_run-06_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-07_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd01_sub-01_ses-nsd01_task-nsdcore_run-07_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-08_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd01_sub-01_ses-nsd01_task-nsdcore_run-08_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-09_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd01_sub-01_ses-nsd01_task-nsdcore_run-09_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-10_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd01_sub-01_ses-nsd01_task-nsdcore_run-10_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-11_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd01_sub-01_ses-nsd01_task-nsdcore_run-11_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd01/sub-01_ses-nsd01_task-nsdcore_run-12_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd01_sub-01_ses-nsd01_task-nsdcore_run-12_bold_MNI.nii.gz\n",
      "min = 0.0 | max = 1073.5833333333333\n",
      "1 188\n",
      "2 376\n",
      "3 564\n",
      "4 752\n",
      "5 940\n",
      "6 1128\n",
      "7 1316\n",
      "8 1504\n",
      "9 1692\n",
      "10 1880\n",
      "11 2068\n",
      "12 2256\n",
      "Processing ses-nsd02 sub-01 ses-nsd02 | global_tr_cnt 2256\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd02/sub-01_ses-nsd02_task-nsdcore_run-01_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd02_sub-01_ses-nsd02_task-nsdcore_run-01_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd02/sub-01_ses-nsd02_task-nsdcore_run-02_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd02_sub-01_ses-nsd02_task-nsdcore_run-02_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd02/sub-01_ses-nsd02_task-nsdcore_run-03_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd02_sub-01_ses-nsd02_task-nsdcore_run-03_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd02/sub-01_ses-nsd02_task-nsdcore_run-04_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd02_sub-01_ses-nsd02_task-nsdcore_run-04_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd02/sub-01_ses-nsd02_task-nsdcore_run-05_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd02_sub-01_ses-nsd02_task-nsdcore_run-05_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd02/sub-01_ses-nsd02_task-nsdcore_run-06_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd02_sub-01_ses-nsd02_task-nsdcore_run-06_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd02/sub-01_ses-nsd02_task-nsdcore_run-07_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd02_sub-01_ses-nsd02_task-nsdcore_run-07_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd02/sub-01_ses-nsd02_task-nsdcore_run-08_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd02_sub-01_ses-nsd02_task-nsdcore_run-08_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd02/sub-01_ses-nsd02_task-nsdcore_run-09_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd02_sub-01_ses-nsd02_task-nsdcore_run-09_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd02/sub-01_ses-nsd02_task-nsdcore_run-10_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd02_sub-01_ses-nsd02_task-nsdcore_run-10_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd02/sub-01_ses-nsd02_task-nsdcore_run-11_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd02_sub-01_ses-nsd02_task-nsdcore_run-11_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd02/sub-01_ses-nsd02_task-nsdcore_run-12_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd02_sub-01_ses-nsd02_task-nsdcore_run-12_bold_MNI.nii.gz\n",
      "min = 0.0 | max = 1003.1666666666666\n",
      "13 2444\n",
      "14 2632\n",
      "15 2820\n",
      "16 3008\n",
      "17 3196\n",
      "18 3384\n",
      "19 3572\n",
      "20 3760\n",
      "21 3948\n",
      "22 4136\n",
      "23 4324\n",
      "24 4512\n",
      "Processing ses-nsd03 sub-01 ses-nsd03 | global_tr_cnt 4512\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd03/sub-01_ses-nsd03_task-nsdcore_run-01_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd03_sub-01_ses-nsd03_task-nsdcore_run-01_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd03/sub-01_ses-nsd03_task-nsdcore_run-02_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd03_sub-01_ses-nsd03_task-nsdcore_run-02_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd03/sub-01_ses-nsd03_task-nsdcore_run-03_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd03_sub-01_ses-nsd03_task-nsdcore_run-03_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd03/sub-01_ses-nsd03_task-nsdcore_run-04_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd03_sub-01_ses-nsd03_task-nsdcore_run-04_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd03/sub-01_ses-nsd03_task-nsdcore_run-05_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd03_sub-01_ses-nsd03_task-nsdcore_run-05_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd03/sub-01_ses-nsd03_task-nsdcore_run-06_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd03_sub-01_ses-nsd03_task-nsdcore_run-06_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd03/sub-01_ses-nsd03_task-nsdcore_run-07_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd03_sub-01_ses-nsd03_task-nsdcore_run-07_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd03/sub-01_ses-nsd03_task-nsdcore_run-08_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd03_sub-01_ses-nsd03_task-nsdcore_run-08_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd03/sub-01_ses-nsd03_task-nsdcore_run-09_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd03_sub-01_ses-nsd03_task-nsdcore_run-09_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd03/sub-01_ses-nsd03_task-nsdcore_run-10_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd03_sub-01_ses-nsd03_task-nsdcore_run-10_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd03/sub-01_ses-nsd03_task-nsdcore_run-11_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd03_sub-01_ses-nsd03_task-nsdcore_run-11_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd03/sub-01_ses-nsd03_task-nsdcore_run-12_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd03_sub-01_ses-nsd03_task-nsdcore_run-12_bold_MNI.nii.gz\n",
      "min = 0.0 | max = 1065.6666666666667\n",
      "25 4700\n",
      "26 4888\n",
      "27 5076\n",
      "28 5264\n",
      "29 5452\n",
      "30 5640\n",
      "31 5828\n",
      "32 6016\n",
      "33 6204\n",
      "34 6392\n",
      "35 6580\n",
      "36 6768\n",
      "Processing ses-nsd04 sub-01 ses-nsd04 | global_tr_cnt 6768\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd04/sub-01_ses-nsd04_task-nsdcore_run-01_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd04_sub-01_ses-nsd04_task-nsdcore_run-01_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd04/sub-01_ses-nsd04_task-nsdcore_run-02_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd04_sub-01_ses-nsd04_task-nsdcore_run-02_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd04/sub-01_ses-nsd04_task-nsdcore_run-03_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd04_sub-01_ses-nsd04_task-nsdcore_run-03_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd04/sub-01_ses-nsd04_task-nsdcore_run-04_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd04_sub-01_ses-nsd04_task-nsdcore_run-04_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd04/sub-01_ses-nsd04_task-nsdcore_run-05_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd04_sub-01_ses-nsd04_task-nsdcore_run-05_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd04/sub-01_ses-nsd04_task-nsdcore_run-06_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd04_sub-01_ses-nsd04_task-nsdcore_run-06_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd04/sub-01_ses-nsd04_task-nsdcore_run-07_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd04_sub-01_ses-nsd04_task-nsdcore_run-07_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd04/sub-01_ses-nsd04_task-nsdcore_run-08_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd04_sub-01_ses-nsd04_task-nsdcore_run-08_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd04/sub-01_ses-nsd04_task-nsdcore_run-09_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd04_sub-01_ses-nsd04_task-nsdcore_run-09_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd04/sub-01_ses-nsd04_task-nsdcore_run-10_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd04_sub-01_ses-nsd04_task-nsdcore_run-10_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd04/sub-01_ses-nsd04_task-nsdcore_run-11_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd04_sub-01_ses-nsd04_task-nsdcore_run-11_bold_MNI.nii.gz\n",
      "download: s3://proj-fmri/fmri_foundation_datasets/NSD_MNI/ses-nsd04/sub-01_ses-nsd04_task-nsdcore_run-12_bold_MNI.nii.gz to temp_NSD_MNIs/ses-nsd04_sub-01_ses-nsd04_task-nsdcore_run-12_bold_MNI.nii.gz\n",
      "min = 0.0 | max = 1090.8333333333333\n",
      "37 6956\n",
      "38 7144\n"
     ]
    }
   ],
   "source": [
    "target_subject = 'sub-01'\n",
    "global_tr_cnt = 0\n",
    "global_run_cnt = 0\n",
    "TRs_per_sample = 1\n",
    "\n",
    "MNI_mask = nib.load(\"/weka/proj-fmri/paulscotti/fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "MNI_mask[MNI_mask>0]=1\n",
    "MNI_mask = MNI_mask.astype(bool)\n",
    "\n",
    "for dataset_id in list(datasets.keys()):\n",
    "    if dataset_id == \"ses-prffloc\":\n",
    "        continue\n",
    "    for subject_id in list(datasets[dataset_id].keys()):\n",
    "        if subject_id != target_subject:\n",
    "            continue\n",
    "        for session_id in list(datasets[dataset_id][subject_id].keys()):\n",
    "            print(f\"Processing {dataset_id} {subject_id} {session_id} | global_tr_cnt {global_tr_cnt}\")\n",
    "            \n",
    "            # first get min and max values across all runs in session\n",
    "            run_count = 0\n",
    "            for file_name in datasets[dataset_id][subject_id][session_id]:\n",
    "                temp_file_path = temp_folder + '/' + file_name.split('/')[2] + '_' + file_name.split('/')[-1]\n",
    "            \n",
    "                if not os.path.exists(temp_file_path):\n",
    "                    # s3.download_file(bucket_name, file_name, temp_file_path)\n",
    "                    command = f\"aws s3 cp s3://proj-fmri/{file_name} {temp_file_path}\"\n",
    "                    call(command,shell=True)\n",
    "\n",
    "                if not os.path.exists(temp_file_path):\n",
    "                    try:\n",
    "                        time.sleep(5)\n",
    "                        command = f\"aws s3 cp s3://proj-fmri/{file_name} {temp_file_path}\"\n",
    "                        call(command,shell=True)\n",
    "                    except:\n",
    "                        print(f\"s3 file not found: {temp_file_path}\")\n",
    "                if not os.path.exists(temp_file_path):\n",
    "                    continue\n",
    "            \n",
    "                func_nii = nib.load(temp_file_path).get_fdata()\n",
    "                func_nii = np.moveaxis(func_nii, -1, 0)\n",
    "                data = func_nii[:,MNI_mask] # find normalization values only inside of the MNI brain mask\n",
    "\n",
    "                # ignore outliers via standard deviation exclusion\n",
    "                low = data.mean() - 2 * data.std()\n",
    "                high = data.mean() + 2 * data.std()\n",
    "                filtered_data = data[(data > low) & (data < high)]\n",
    "                min_val = np.min(filtered_data)\n",
    "                max_val = np.max(filtered_data)\n",
    "                \n",
    "                run_count +=1\n",
    "                if run_count==1: \n",
    "                    min = min_val\n",
    "                    max = max_val\n",
    "                else:\n",
    "                    min += min_val\n",
    "                    max += max_val\n",
    "                    \n",
    "            min /= run_count\n",
    "            max /= run_count\n",
    "            print(f\"min = {min} | max = {max}\")\n",
    "\n",
    "            for file_name in datasets[dataset_id][subject_id][session_id]:\n",
    "                temp_file_path = temp_folder + '/' + file_name.split('/')[2] + '_' + file_name.split('/')[-1]\n",
    "                if \"task-nsdcore\" not in temp_file_path:\n",
    "                    continue\n",
    "                if not os.path.exists(temp_file_path):\n",
    "                    print(f\"skipped {temp_file_path}\")\n",
    "                    continue\n",
    "    \n",
    "                func_nii = nib.load(temp_file_path).get_fdata()\n",
    "                func_nii = np.moveaxis(func_nii, -1, 0)\n",
    "                func_nii = func_nii[:,6:94,8:112,10:82].astype(np.float16) # [T, 97, 115, 97] to [T, 88, 104, 72]\n",
    "    \n",
    "                # normalize by min max\n",
    "                func_nii = (func_nii - min) / (max - min)       \n",
    "    \n",
    "                global_run_cnt += 1\n",
    "\n",
    "                # create samples of TRs_per_sample TRs\n",
    "                for batch in range(0,len(func_nii),TRs_per_sample):\n",
    "                    out = func_nii[[batch]]\n",
    "                    funcs_dset[global_tr_cnt] = out\n",
    "                    global_trs_dset[global_tr_cnt] = global_tr_cnt\n",
    "                    global_tr_cnt += 1\n",
    "                    \n",
    "                print(global_run_cnt, global_tr_cnt)\n",
    "\n",
    "            # delete saved files\n",
    "            command = f\"rm {temp_folder}/*\"\n",
    "            call(command,shell=True)\n",
    "###\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8c5f8-137f-4aca-8829-9e7b136ab2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014c553-2c8e-4706-b441-c952ade78d29",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Matrix of behavioral/stimuli info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1deeaa3-90bc-4466-89a7-faca765e4dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TR = 1.6\n",
    "TRs_per_run = 188\n",
    "print(\"TR =\",TR)\n",
    "\n",
    "shared1000 = np.load('/weka/proj-fmri/shared/mindeyev2_dataset/shared1000.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401771a1-9757-458b-999d-def515a6cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the bucket name and folder name\n",
    "bucket_name = 'natural-scenes-dataset'\n",
    "folder_names = list_folders(bucket_name)\n",
    "\n",
    "folder_name = \"nsddata_rawdata\"\n",
    "print(f\"Processing dataset: {folder_name}.\")\n",
    "\n",
    "stim = {}\n",
    "run_cnt = 0\n",
    "last_trial = 0\n",
    "\n",
    "pattern = r\"ses-nsd(\\d+)/.*?run-(\\d+)\"\n",
    "\n",
    "# List all objects in the folder\n",
    "all_objects = list_all_objects(bucket_name, folder_name)\n",
    "for obj in all_objects:\n",
    "    obj_key = obj['Key']\n",
    "\n",
    "    if '_events.tsv' in obj_key and 'nsdcore_run' in obj_key and subject in obj_key:\n",
    "        # print(obj_key)\n",
    "        \n",
    "        filename = os.getcwd()+f'/{proj_name}/'+obj_key\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        if not os.path.exists(filename):\n",
    "            s3.download_file(bucket_name, obj_key, filename)\n",
    "\n",
    "        events = pd.read_csv(filename, delimiter='\\t')\n",
    "        \n",
    "        match = re.search(pattern, filename)\n",
    "        ses, run = match.groups()\n",
    "        \n",
    "        if stim=={}:\n",
    "            stim['run_TR_onsets'] = events['onset'].values / TR\n",
    "            stim['global_TR_onsets'] = (events['onset'].values / TR) + (run_cnt*188)\n",
    "            stim['coco73k'] = events['73k_id'].values - 1\n",
    "            stim['run_trial'] = events['trial_number'].values\n",
    "            stim['global_trial'] = events['trial_number'].values + last_trial\n",
    "            stim['global_sess'] = np.repeat(int(ses), len(events['73k_id'].values))\n",
    "            stim['global_run'] = np.repeat(int(run) + (run_cnt*12), len(events['73k_id'].values))\n",
    "            stim['trial_type'] = events['trial_type'].values\n",
    "        else:\n",
    "            stim['run_TR_onsets'] = np.append(stim['run_TR_onsets'], events['onset'].values / TR)\n",
    "            stim['global_TR_onsets'] = np.append(stim['global_TR_onsets'],(events['onset'].values / TR) + (run_cnt*188))\n",
    "            stim['coco73k'] = np.append(stim['coco73k'], events['73k_id'].values - 1)\n",
    "            stim['run_trial'] = np.append(stim['run_trial'], events['trial_number'].values)\n",
    "            stim['global_trial'] = np.append(stim['global_trial'], events['trial_number'].values + last_trial)\n",
    "            stim['global_sess'] = np.append(stim['global_sess'], np.repeat(int(ses), len(events['73k_id'].values)))\n",
    "            stim['global_run'] = np.append(stim['global_run'], np.repeat(int(run) + (run_cnt*12), len(events['73k_id'].values)))\n",
    "            stim['trial_type'] = np.append(stim['trial_type'], events['trial_type'].values)\n",
    "            \n",
    "        last_trial = stim['global_trial'][-1]\n",
    "        run_cnt += 1\n",
    "\n",
    "stim['shared1000'] = shared1000[stim['coco73k']]\n",
    "        \n",
    "# convert to 0-index\n",
    "stim['run_trial'] = stim['run_trial']-1\n",
    "stim['global_trial'] = stim['global_trial']-1\n",
    "stim['global_run'] = stim['global_run']-1\n",
    "stim['global_sess'] = stim['global_sess']-1\n",
    "\n",
    "df = pd.DataFrame(stim)\n",
    "df.to_csv(f'{proj_name}/{subject}/nsddata_rawdata.csv', index=False)\n",
    "df.to_csv(f'nsddata_rawdata.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb56ca2-7da1-4668-bdf2-d5bd16e343e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Send to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d956d-e757-402f-92d6-4f8540ddcc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to aws s3\n",
    "command = f\"aws s3 sync {outpath}/{subject} s3://proj-fmri/fmri_foundation_datasets/{proj_name} --region us-west-2\"\n",
    "call(command,shell=True)\n",
    "\n",
    "# # delete tars\n",
    "# command = f\"rm {outpath}/{subject}/*.tar\"\n",
    "# call(command,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b619e-a8b5-4e44-94d4-52aef5311621",
   "metadata": {},
   "source": [
    "# Making tars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3aeca2-a512-403d-a88a-9cea8d2bb52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_tr_cnt = 0\n",
    "# TRs_per_sample = 24\n",
    "# max_samples_per_tar = 320 # translates to around 1 Gb per tar\n",
    "# max_TRs_per_tar = max_samples_per_tar * TRs_per_sample\n",
    "\n",
    "# # Set the bucket name and folder name\n",
    "# bucket_name = 'natural-scenes-dataset'\n",
    "# folder_names = list_folders(bucket_name)\n",
    "\n",
    "# print(f\"TRs_per_sample: {TRs_per_sample} max_samples_per_tar: {max_samples_per_tar} max_TRs_per_tar: {max_TRs_per_tar}\")\n",
    "\n",
    "# # If you want to start over webdataset creation from the beginning, uncomment these:\n",
    "# tar_count = 0\n",
    "# TR_count = 0\n",
    "# subj_count = 0\n",
    "# dataset_count = 0\n",
    "# dataset_list = []\n",
    "# obj_key_list = []\n",
    "\n",
    "# sample_idx = 0\n",
    "# current_dataset = None\n",
    "# current_subject = None\n",
    "# sink = wds.TarWriter(f\"{outpath}/sub-01/{tar_count:06d}.tar\")\n",
    "\n",
    "# tio_transforms = tio.Compose(\n",
    "#                 (\n",
    "#                     tio.ToCanonical(), # make sure orientation of brains are consistent (RAS+ orientation)\n",
    "#                     tio.RescaleIntensity(out_min_max=(0, 1)),\n",
    "#                     tio.Resample(3, image_interpolation='nearest'), # rescale voxels to #mm isotropic\n",
    "#                     tio.CropOrPad((64, 64, 48)),\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "# folder_name = \"nsddata_rawdata\"\n",
    "\n",
    "# print(f\"Processing dataset: {folder_name}.\")\n",
    "# all_objects = list_all_objects(bucket_name, folder_name)\n",
    "# for obj in all_objects:\n",
    "#     obj_key = obj['Key']\n",
    "\n",
    "#     if '_bold.nii.gz' in obj_key and 'nsdcore_run' in obj_key and subject in obj_key:\n",
    "#         print(obj_key)\n",
    "#         func_subj = obj_key.split('/')[1]\n",
    "\n",
    "#         # if metadata file shows you already processed this file, skip it\n",
    "#         if np.isin(obj_key, obj_key_list):\n",
    "#             continue\n",
    "\n",
    "#         filename = os.getcwd()+f'/{proj_name}/'+obj_key\n",
    "#         os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "#         if not os.path.exists(filename):\n",
    "#             s3.download_file(bucket_name, obj_key, filename)\n",
    "\n",
    "#         func_nii = nib.load(filename)\n",
    "#         try:\n",
    "#             print(obj_key, func_nii.get_fdata().shape, \"| TRs:\", TR_count, \"samp:\", sample_idx)\n",
    "#         except Exception as e:\n",
    "#             print(f\"get_fdata() error occurred: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         try:\n",
    "#             tio_image = tio.ScalarImage(tensor=np.moveaxis(func_nii.get_fdata(),-1,0).astype(np.float32), \n",
    "#                                     affine=func_nii.affine, \n",
    "#                                     dtype=np.float32)\n",
    "#             out = tio_transforms(tio_image)['data']\n",
    "#         except Exception as e: # this can happen if the func is actually 3d and not 4d\n",
    "#             print(f\"tio processing error occurred: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         # zscore across the run\n",
    "#         out_shape = out.shape\n",
    "#         out = out.reshape(len(out),-1)\n",
    "#         scalar = StandardScaler(with_mean=True, with_std=True).fit(out)\n",
    "#         mean = scalar.mean_\n",
    "#         sd = scalar.scale_\n",
    "#         out = (out - mean) / sd\n",
    "#         mean = mean.reshape([out_shape[1],out_shape[2],out_shape[3]])\n",
    "#         sd = sd.reshape([out_shape[1],out_shape[2],out_shape[3]])\n",
    "#         meansd = np.array([mean,sd])\n",
    "#         out = out.reshape(out_shape)\n",
    "\n",
    "#         # create 16-bit png of mean and sd volumes\n",
    "#         meansd_images = reshape_to_2d(meansd)\n",
    "#         meansd_images = torch.Tensor(meansd_images)\n",
    "#         min_meansd, max_meansd = meansd_images.min(), meansd_images.max()\n",
    "#         minmax_meansd_images = (meansd_images - min_meansd) / (max_meansd - min_meansd) # first you need to rescale to 0 to 1\n",
    "#         rescaled_images = (minmax_meansd_images * 65535).to(torch.int16) # then multiply by constant prior to numpy uint16\n",
    "#         rescaled_images_numpy = rescaled_images.numpy().astype(np.uint16)\n",
    "#         meansd_PIL_image = Image.fromarray(rescaled_images_numpy, mode='I;16')\n",
    "        \n",
    "#         global_tr_cnt += len(out)\n",
    "\n",
    "#         # create samples of TRs_per_sample TRs\n",
    "#         for batch in range(0,len(out),TRs_per_sample):\n",
    "#             if len(out[batch:batch+TRs_per_sample])<TRs_per_sample:\n",
    "#                 continue\n",
    "#             images = reshape_to_2d(out[batch:batch+TRs_per_sample])\n",
    "#             images = torch.Tensor(images)\n",
    "\n",
    "#             # convert tensor to something compatible with 16-bit png\n",
    "#             min_, max_ = images.min(), images.max()\n",
    "#             minmax_images = (images - min_) / (max_ - min_) # first you need to rescale to 0 to 1\n",
    "#             rescaled_images = (minmax_images * 65535).to(torch.int16) # then multiply by constant prior to numpy uint16\n",
    "#             rescaled_images_numpy = rescaled_images.numpy().astype(np.uint16)\n",
    "#             PIL_image = Image.fromarray(rescaled_images_numpy, mode='I;16')\n",
    "\n",
    "#             sink.write({\n",
    "#                 \"__key__\": \"%06d\" % sample_idx,\n",
    "#                 \"dataset.txt\": obj_key,\n",
    "#                 \"header.npy\": np.array(header_to_dict(func_nii.header)),\n",
    "#                 \"minmax.npy\": np.array([min_, max_, min_meansd, max_meansd]),\n",
    "#                 \"meansd.png\": meansd_PIL_image,\n",
    "#                 \"func.png\": PIL_image, # 27M for 8-bit png vs 48M for 16-bit png vs 144M for numpy\n",
    "#                 \"run_TR.npy\": np.arange(len(out))[batch:batch+TRs_per_sample],\n",
    "#                 \"global_TR.npy\": np.arange(len(out))[batch:batch+TRs_per_sample] + (global_tr_cnt - len(out)),\n",
    "#             })\n",
    "\n",
    "#             if current_dataset != obj['Key'].split('/')[0]:\n",
    "#                 print(obj_key, \"| TR_count:\", TR_count, \"sample_idx:\", sample_idx)\n",
    "#                 dataset_list.append(obj['Key'].split('/')[0])\n",
    "#                 dataset_count += 1\n",
    "#                 if is_interactive(): # dont want to plot unless you are in interactive notebook\n",
    "#                     torchio_slice(out) # plot normalized slices\n",
    "#                     torchio_slice((out * sd) + mean) # plot unnormalized slices\n",
    "\n",
    "#             if current_subject != obj['Key'].split('/')[1]:\n",
    "#                 subj_count += 1\n",
    "#             current_dataset = obj['Key'].split('/')[0]\n",
    "#             current_subject = obj['Key'].split('/')[1]\n",
    "\n",
    "#             TR_count += TRs_per_sample\n",
    "#             sample_idx += 1\n",
    "\n",
    "#             if sample_idx >= max_samples_per_tar:\n",
    "#                 print(\"HIT MAX SAMPLES PER TAR\")\n",
    "#                 sink.close()\n",
    "#                 sample_idx = 0 \n",
    "\n",
    "#                 # make metadata file and save progress to aws s3\n",
    "#                 data = {\n",
    "#                     \"TR_count\": TR_count,\n",
    "#                     \"subj_count\": subj_count,\n",
    "#                     \"tar_count\": tar_count,\n",
    "#                     \"dataset_count\": dataset_count,\n",
    "#                     \"datasets\": dataset_list,\n",
    "#                     \"obj_key_list\": obj_key_list,\n",
    "#                 }\n",
    "#                 with open(f\"{outpath}/{subject}/metadata.json\", \"w\") as file:\n",
    "#                     json.dump(data, file)\n",
    "\n",
    "#                 tar_count += 1\n",
    "#                 sink = wds.TarWriter(f\"{outpath}/{subject}/{tar_count:06d}.tar\")\n",
    "\n",
    "#         obj_key_list.append(obj['Key'])\n",
    "\n",
    "# print(\"TR_count\",TR_count)\n",
    "# print(\"subj_count\",subj_count)\n",
    "# print(\"dataset_count\",dataset_count)\n",
    "# print(\"tar_count\", tar_count)   \n",
    "\n",
    "# try:\n",
    "#     sink.close()\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# data = {\n",
    "#     \"TR_count\": TR_count,\n",
    "#     \"subj_count\": subj_count,\n",
    "#     \"tar_count\": tar_count,\n",
    "#     \"datasets\": dataset_list,\n",
    "#     \"obj_key_list\": obj_key_list,          \n",
    "# }\n",
    "\n",
    "# with open(f\"{outpath}/{subject}/metadata.json\", \"w\") as file:\n",
    "#     json.dump(data, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "found",
   "language": "python",
   "name": "found"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
