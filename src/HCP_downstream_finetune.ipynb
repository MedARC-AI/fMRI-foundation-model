{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import webdataset as wds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from mae_utils.flat_models import *\n",
    "import h5py\n",
    "from mae_utils import flat_models\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import argparse\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# following fixes a Conv3D CUDNN_NOT_SUPPORTED error\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44bcbd9d-d802-4bb9-af40-b6f0b0746227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name_suffix: testing\n",
      "--found_model_name=BOTHflat_small_gsrFalse_sourceadd_cont_91113 --epoch_checkpoint epoch0.pth                     --hcp_flat_path=/weka/proj-medarc/shared/HCP-Flat                     --target=trial_type                     --model_suffix=testing                     --batch_size=16                     --max_lr=3e-4 --num_epochs=20 --no-save_ckpt --no-wandb_log --num_workers=10                     --weight_decay=1e-5                     --global_pool\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name_suffix = \"testing\"\n",
    "    print(\"model_name_suffix:\", model_name_suffix)\n",
    "    batch_size = 16\n",
    "    # global_batch_size and batch_size should already be defined in the 2nd cell block\n",
    "    jupyter_args = f\"--found_model_name=BOTHflat_small_gsrFalse_sourceadd_cont_91113 --epoch_checkpoint epoch0.pth \\\n",
    "                    --hcp_flat_path=/weka/proj-medarc/shared/HCP-Flat \\\n",
    "                    --target=trial_type \\\n",
    "                    --model_suffix={model_name_suffix} \\\n",
    "                    --batch_size={batch_size} \\\n",
    "                    --max_lr=3e-4 --num_epochs=20 --no-save_ckpt --no-wandb_log --num_workers=10 \\\n",
    "                    --weight_decay=1e-5 \\\n",
    "                    --global_pool\"\n",
    "    # --multisubject_ckpt=../train_logs/multisubject_subj01_1024_24bs_nolow\n",
    "    # suggested hyperparameters for trial_type: wd = 1e-5, max_lr = 3e-4\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe5db9ac-76fb-4eb8-8074-43268b246606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ ARGS ------- \n",
      " Namespace(found_model_name='BOTHflat_small_gsrFalse_sourceadd_cont_91113', epoch_checkpoint='epoch0.pth', model_suffix='testing', hcp_flat_path='/weka/proj-medarc/shared/HCP-Flat', nsd_flat_path='/weka/proj-medarc/shared/NSD-Flat', batch_size=16, wandb_log=False, num_epochs=20, lr_scheduler_type='cycle', save_ckpt=False, seed=42, max_lr=0.0003, target='trial_type', num_workers=10, weight_decay=1e-05, global_pool=True)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--found_model_name\", type=str, default=\"Testing_flat\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--epoch_checkpoint\", type=str, default=\"epoch99.pth\",\n",
    "    help=\"the epoch number of the found_model_name checkpoint\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_suffix\", type=str, default=\"Testing_flat\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hcp_flat_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--nsd_flat_path\", type=str, default='/weka/proj-medarc/shared/NSD-Flat',\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=128,\n",
    "    help=\"Batch size can be increased by 10x if only training retreival submodule and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=150,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_ckpt\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--target\",type=str,default='trial_type',choices=['trial_type','sex','age'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_workers\",type=int,default=10,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weight_decay\",type=float,default=1e-5,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--global_pool\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"not implemented yet\",\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "print(f\"------ ARGS ------- \\n {args}\")\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e82ea6-3ee2-456b-be1e-a9460fb20ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## MODEL TO LOAD ##\n",
    "# if utils.is_interactive():\n",
    "#     model_name = \"HCPflat_large_gsrFalse_\"\n",
    "# else:\n",
    "#     model_name = sys.argv[1]\n",
    "    \n",
    "# target = 'sex' # This can be 'trial_type' 'age' 'sex'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f8f866d-c68f-4a35-bb51-398b2b2f8313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir /weka/proj-fmri/ckadirt/fMRI-foundation-model/src/checkpoints/BOTHflat_small_gsrFalse_sourceadd_cont_91113\n",
      "Loaded config.yaml from ckpt folder /weka/proj-fmri/ckadirt/fMRI-foundation-model/src/checkpoints/BOTHflat_small_gsrFalse_sourceadd_cont_91113\n",
      "\n",
      "__CONFIG__\n",
      "base_lr = 0.001\n",
      "batch_size = 16\n",
      "ckpt_interval = 25\n",
      "ckpt_saving = True\n",
      "cls_embed = True\n",
      "cls_forward = False\n",
      "contrastive_loss_weight = 1.0\n",
      "datasets_to_include = BOTH\n",
      "decoder_embed_dim = 512\n",
      "global_pool = False\n",
      "grad_accumulation_steps = 1\n",
      "grad_clip = 1.0\n",
      "gsr = False\n",
      "hcp_flat_path = /weka/proj-medarc/shared/HCP-Flat\n",
      "mask_ratio = 0.75\n",
      "model_name = BOTHflat_small_gsrFalse_sourceadd_cont\n",
      "model_size = small\n",
      "no_qkv_bias = False\n",
      "norm_pix_loss = False\n",
      "nsd_flat_path = /weka/proj-medarc/shared/NSD-Flat\n",
      "num_epochs = 100\n",
      "num_frames = 16\n",
      "num_samples_per_epoch = 200000\n",
      "num_workers = 8\n",
      "patch_size = 16\n",
      "pct_masks_to_decode = 1\n",
      "plotting = True\n",
      "pred_t_dim = 8\n",
      "print_interval = 20\n",
      "probe_base_lr = 0.0003\n",
      "probe_batch_size = 8\n",
      "probe_num_epochs = 30\n",
      "probe_num_samples_per_epoch = 100000\n",
      "resume_from_ckpt = True\n",
      "seed = 42\n",
      "sep_pos_embed = True\n",
      "source_embed_mode = add\n",
      "source_embed_train_mode = ce\n",
      "t_patch_size = 2\n",
      "test_num_samples_per_epoch = 50000\n",
      "test_set = False\n",
      "trunc_init = False\n",
      "use_contrastive_loss = True\n",
      "use_decoder_contrastive_loss = False\n",
      "use_source_embeds = True\n",
      "wandb_log = True\n",
      "wandb_rand = 0\n",
      "\n",
      "\n",
      "WORLD_SIZE=1\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "PID of this process = 243901\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# outdir = os.path.abspath(f'checkpoints/{model_name}')\n",
    "outdir = os.path.abspath(f'checkpoints/{found_model_name}')\n",
    "\n",
    "print(\"outdir\", outdir)\n",
    "# Load previous config.yaml if available\n",
    "if os.path.exists(f\"{outdir}/config.yaml\"):\n",
    "    config = yaml.load(open(f\"{outdir}/config.yaml\", 'r'), Loader=yaml.FullLoader)\n",
    "    print(f\"Loaded config.yaml from ckpt folder {outdir}\")\n",
    "    # create global variables from the config\n",
    "    print(\"\\n__CONFIG__\")\n",
    "    for attribute_name in config.keys():\n",
    "        print(f\"{attribute_name} = {config[attribute_name]}\")\n",
    "        globals()[attribute_name] = config[f'{attribute_name}']\n",
    "    print(\"\\n\")\n",
    "\n",
    "world_size = os.getenv('WORLD_SIZE')\n",
    "if world_size is None: \n",
    "    world_size = 1\n",
    "else:\n",
    "    world_size = int(world_size)\n",
    "print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "# batch_size = probe_batch_size\n",
    "# num_epochs = probe_num_epochs\n",
    "\n",
    "data_type = torch.float32 # change depending on your mixed_precision\n",
    "global_batch_size = batch_size * world_size\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# hcp_flat_path = \"/weka/proj-medarc/shared/HCP-Flat\"\n",
    "# seed = 42\n",
    "# num_frames = 16\n",
    "# gsr = False\n",
    "# num_workers = 10\n",
    "# batch_size = 128\n",
    "# save_ckpt = True\n",
    "# wandb_log = True\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "utils.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "128b23ed-5094-4f7d-8305-e5196d14e21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_pool = False\n",
      "gsr = False\n"
     ]
    }
   ],
   "source": [
    "# if os.getenv('global_pool') == \"False\":\n",
    "#     global_pool = False\n",
    "# else:\n",
    "#     global_pool = True\n",
    "print(f\"global_pool = {global_pool}\")\n",
    "\n",
    "try:\n",
    "    gsr\n",
    "except:\n",
    "    gsr = True\n",
    "    print(\"set gsr to True\")\n",
    "print(f\"gsr = {gsr}\")\n",
    "\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc00235a-a9e1-4dc4-8fd3-359c1d91bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### UNCOMMENT THIS TO SAVE THE HCP-FLAT IN HDF5 FORMAT\n",
    "\n",
    "\n",
    "# from torch.utils.data import default_collate\n",
    "# from mae_utils.flat import load_hcp_flat_mask\n",
    "# from mae_utils.flat import create_hcp_flat\n",
    "# from mae_utils.flat import batch_unmask\n",
    "# import mae_utils.visualize as vis\n",
    "\n",
    "\n",
    "# batch_size = 1\n",
    "# print(f\"changed batch_size to {batch_size}\")\n",
    "# load_file_frames = num_frames * 2\n",
    "# print(f\"Calculating with {load_file_frames} frames, doubling to approximate TR\")\n",
    "\n",
    "# ## Test ##\n",
    "# datasets_to_include = \"HCP\"\n",
    "# assert \"HCP\" in datasets_to_include\n",
    "# test_dataset = create_hcp_flat(root=hcp_flat_path, \n",
    "#                 clip_mode=\"event\", frames=load_file_frames, shuffle=False, gsr=gsr, sub_list = 'test')\n",
    "# test_dl = wds.WebLoader(\n",
    "#     test_dataset.batched(batch_size, partial=False, collation_fn=default_collate),\n",
    "#     batch_size=None,\n",
    "#     shuffle=False,\n",
    "#     num_workers=num_workers,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "\n",
    "# ## Train ##\n",
    "# assert \"HCP\" in datasets_to_include\n",
    "# train_dataset = create_hcp_flat(root=hcp_flat_path, \n",
    "#                 clip_mode=\"event\", frames=load_file_frames, shuffle=False, gsr=gsr, sub_list = 'train')\n",
    "# train_dl = wds.WebLoader(\n",
    "#     train_dataset.batched(batch_size, partial=True, collation_fn=default_collate),\n",
    "#     batch_size=None,\n",
    "#     shuffle=False,\n",
    "#     num_workers=num_workers,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "\n",
    "# def flatten_meta(meta_dict):\n",
    "#     \"\"\"\n",
    "#     Flatten the meta dictionary by:\n",
    "#     - Replacing single-item lists with the item itself.\n",
    "#     - Converting tensors to scalar numbers.\n",
    "#     \"\"\"\n",
    "#     flattened = {}\n",
    "#     for key, value in meta_dict.items():\n",
    "#         if isinstance(value, list):\n",
    "#             if len(value) == 1:\n",
    "#                 flattened[key] = value[0]  # Replace list with its single item\n",
    "#             else:\n",
    "#                 flattened[key] = value  # Keep as is if multiple items\n",
    "#         elif isinstance(value, torch.Tensor):\n",
    "#             # Convert tensor to scalar\n",
    "#             if value.numel() == 1:\n",
    "#                 flattened[key] = value.item()\n",
    "#             else:\n",
    "#                 flattened[key] = value.tolist()  # Convert multi-element tensor to list\n",
    "#         else:\n",
    "#             flattened[key] = value  # Keep the value as is\n",
    "#     return flattened\n",
    "\n",
    "\n",
    "# import h5py\n",
    "# meta_array = np.array([], dtype=object)\n",
    "# # Open an HDF5 file in write mode\n",
    "# with h5py.File(f'test_HCP_raw_flatmaps_{load_file_frames}f.hdf5', 'w') as h5f:\n",
    "#     flatmaps_dset = None\n",
    "    \n",
    "#     total_samples = 0\n",
    "\n",
    "#     for i, batch in tqdm(enumerate(test_dl), total = 12000):\n",
    "#         images = batch[0]\n",
    "#         meta = batch[1]\n",
    "#         batch_size = images.shape[0]\n",
    "#         meta_serializable = meta.copy()\n",
    "        \n",
    "        \n",
    "#         # Step 2: Serialize the dictionary to a JSON string\n",
    "#         meta_str = json.dumps(flatten_meta(meta_serializable), indent=4)\n",
    "#         meta_array = np.append(meta_array, meta_str)\n",
    "#         if flatmaps_dset is None:\n",
    "#             # Initialize datasets with unlimited (None) maxshape along the first axis\n",
    "#             flatmaps_shape = (0,) + images.shape[1:]\n",
    "#             flatmaps_maxshape = (None,) + images.shape[1:]\n",
    "\n",
    "#             flatmaps_dset = h5f.create_dataset(\n",
    "#                 'flatmaps',\n",
    "#                 shape=flatmaps_shape,\n",
    "#                 maxshape=flatmaps_maxshape,\n",
    "#                 dtype=np.float16,\n",
    "#                 chunks=True  # Enable chunking for efficient resizing\n",
    "#             )\n",
    "\n",
    "#         # Resize datasets to accommodate new data\n",
    "#         flatmaps_dset.resize(total_samples + batch_size, axis=0)\n",
    "\n",
    "#         # Write data to the datasets\n",
    "#         flatmaps_dset[total_samples:total_samples + batch_size] = images.numpy().astype(np.float16)\n",
    "\n",
    "#         total_samples += batch_size\n",
    "        \n",
    "#     print(f\"Processed {total_samples} samples\")\n",
    "# np.save(f'metadata_test_HCP_raw_flatmaps_{load_file_frames}f.npy', meta_array)\n",
    "\n",
    "\n",
    "# import h5py\n",
    "# meta_array = np.array([], dtype=object)\n",
    "# # Open an HDF5 file in write mode\n",
    "# with h5py.File(f'train_HCP_raw_flatmaps_{load_file_frames}f.hdf5', 'w') as h5f:\n",
    "#     flatmaps_dset = None\n",
    "    \n",
    "#     total_samples = 0\n",
    "\n",
    "#     for i, batch in tqdm(enumerate(train_dl), total = 120000):\n",
    "#         images = batch[0]\n",
    "#         meta = batch[1]\n",
    "#         batch_size = images.shape[0]\n",
    "#         meta_serializable = meta.copy()\n",
    "        \n",
    "        \n",
    "#         # Step 2: Serialize the dictionary to a JSON string\n",
    "#         meta_str = json.dumps(flatten_meta(meta_serializable), indent=4)\n",
    "#         meta_array = np.append(meta_array, meta_str)\n",
    "#         if flatmaps_dset is None:\n",
    "#             # Initialize datasets with unlimited (None) maxshape along the first axis\n",
    "#             flatmaps_shape = (0,) + images.shape[1:]\n",
    "#             flatmaps_maxshape = (None,) + images.shape[1:]\n",
    "\n",
    "#             flatmaps_dset = h5f.create_dataset(\n",
    "#                 'flatmaps',\n",
    "#                 shape=flatmaps_shape,\n",
    "#                 maxshape=flatmaps_maxshape,\n",
    "#                 dtype=np.float16,\n",
    "#                 chunks=True  # Enable chunking for efficient resizing\n",
    "#             )\n",
    "\n",
    "#         # Resize datasets to accommodate new data\n",
    "#         flatmaps_dset.resize(total_samples + batch_size, axis=0)\n",
    "\n",
    "#         # Write data to the datasets\n",
    "#         flatmaps_dset[total_samples:total_samples + batch_size] = images.numpy().astype(np.float16)\n",
    "\n",
    "#         total_samples += batch_size\n",
    "        \n",
    "#     print(f\"Processed {total_samples} samples\")\n",
    "# np.save(f'metadata_train_HCP_raw_flatmaps_{load_file_frames}f.npy', meta_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75be5d80-64ef-4e07-a15d-054dbc8b4d8a",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f04009-13f1-473d-9808-181a46d3e97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "if target == \"trial_type\":\n",
    "    \n",
    "    INCLUDE_CONDS = {\n",
    "        \"fear\",\n",
    "        \"neut\",\n",
    "        \"math\",\n",
    "        \"story\",\n",
    "        \"lf\",\n",
    "        \"lh\",\n",
    "        \"rf\",\n",
    "        \"rh\",\n",
    "        \"t\",\n",
    "        \"match\",\n",
    "        \"relation\",\n",
    "        \"mental\",\n",
    "        \"rnd\",\n",
    "        \"0bk_body\",\n",
    "        \"2bk_body\",\n",
    "        \"0bk_faces\",\n",
    "        \"2bk_faces\",\n",
    "        \"0bk_places\",\n",
    "        \"2bk_places\",\n",
    "        \"0bk_tools\",\n",
    "        \"2bk_tools\",\n",
    "    }\n",
    "    \n",
    "    # test_data = []\n",
    "    \n",
    "    # # Iterate over the DataLoader with a progress bar\n",
    "    # for sample in tqdm(train_dl, desc=\"Processing samples\"):\n",
    "    #     x = sample['image']\n",
    "    #     y = sample['meta']['trial_type']\n",
    "    #     key = sample['meta']['key']\n",
    "    #     print(x.shape, y, key)\n",
    "    #     break\n",
    "    # Initialize the label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(sorted(INCLUDE_CONDS))  # Ensure consistent ordering\n",
    "    \n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e66e2c0-6a73-42d7-8414-3b8acafdb0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded flatmaps\n"
     ]
    }
   ],
   "source": [
    "hdf5_base_path_raw_file = '.'  # use this /weka/proj-fmri/ckadirt/fMRI-foundation-model/src if you don't want to store them again\n",
    "load_file_frames = num_frames * 2\n",
    "\n",
    "try:\n",
    "    f_train = h5py.File(f'{hdf5_base_path_raw_file}/train_HCP_raw_flatmaps_{load_file_frames}f.hdf5', 'r')\n",
    "    flatmaps_train = f_train['flatmaps']\n",
    "    \n",
    "    f_test = h5py.File(f'{hdf5_base_path_raw_file}/test_HCP_raw_flatmaps_{load_file_frames}f.hdf5', 'r')\n",
    "    flatmaps_test = f_test['flatmaps']\n",
    "    \n",
    "    metadata_train = np.load(f'{hdf5_base_path_raw_file}/metadata_train_HCP_raw_flatmaps_{load_file_frames}f.npy', allow_pickle=True)\n",
    "    metadata_test = np.load(f'{hdf5_base_path_raw_file}/metadata_test_HCP_raw_flatmaps_{load_file_frames}f.npy', allow_pickle=True)\n",
    "    print(\"Loaded flatmaps\")\n",
    "except:\n",
    "    print(f\"Make sure you have the raw flatmaps precomputed for this num frames: {load_file_frames}. You can do it uncommenting the cell above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbe36239-bab4-4ff8-af07-443384b7eab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened length: 87748\n",
      "First few items:\n",
      "[{'key': 'sub-285446_mod-tfMRI_task-MOTOR_mag-3T_dir-RL', 'sub': '285446', 'mod': 'tfMRI', 'task': 'MOTOR', 'mag': '3T', 'dir': 'RL', 'start': 19, 'trial_type': 'lh'}, {'key': 'sub-123521_mod-tfMRI_task-SOCIAL_mag-3T_dir-LR', 'sub': '123521', 'mod': 'tfMRI', 'task': 'SOCIAL', 'mag': '3T', 'dir': 'LR', 'start': 15, 'trial_type': 'mental'}, {'key': 'sub-116221_mod-tfMRI_task-SOCIAL_mag-3T_dir-RL', 'sub': '116221', 'mod': 'tfMRI', 'task': 'SOCIAL', 'mag': '3T', 'dir': 'RL', 'start': 15, 'trial_type': 'mental'}, {'key': 'sub-456346_mod-tfMRI_task-RELATIONAL_mag-3T_dir-LR', 'sub': '456346', 'mod': 'tfMRI', 'task': 'RELATIONAL', 'mag': '3T', 'dir': 'LR', 'start': 15, 'trial_type': 'relation'}, {'key': 'sub-142828_mod-tfMRI_task-EMOTION_mag-3T_dir-RL', 'sub': '142828', 'mod': 'tfMRI', 'task': 'EMOTION', 'mag': '3T', 'dir': 'RL', 'start': 19, 'trial_type': 'neut'}]\n",
      "Flattened length: 9498\n",
      "First few items:\n",
      "[{'key': 'sub-102109_mod-tfMRI_task-WM_mag-3T_dir-LR', 'sub': '102109', 'mod': 'tfMRI', 'task': 'WM', 'mag': '3T', 'dir': 'LR', 'start': 15, 'trial_type': '2bk_tools'}, {'key': 'sub-731140_mod-tfMRI_task-WM_mag-3T_dir-RL', 'sub': '731140', 'mod': 'tfMRI', 'task': 'WM', 'mag': '3T', 'dir': 'RL', 'start': 15, 'trial_type': '2bk_body'}, {'key': 'sub-149539_mod-tfMRI_task-MOTOR_mag-3T_dir-RL', 'sub': '149539', 'mod': 'tfMRI', 'task': 'MOTOR', 'mag': '3T', 'dir': 'RL', 'start': 19, 'trial_type': 'lh'}, {'key': 'sub-376247_mod-tfMRI_task-WM_mag-3T_dir-RL', 'sub': '376247', 'mod': 'tfMRI', 'task': 'WM', 'mag': '3T', 'dir': 'RL', 'start': 15, 'trial_type': '2bk_body'}, {'key': 'sub-164939_mod-tfMRI_task-EMOTION_mag-3T_dir-LR', 'sub': '164939', 'mod': 'tfMRI', 'task': 'EMOTION', 'mag': '3T', 'dir': 'LR', 'start': 19, 'trial_type': 'neut'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "fields = [\"key\", \"sub\", \"mod\", \"task\", \"mag\", \"dir\", \"start\", \"trial_type\"]\n",
    "\n",
    "flattened = []\n",
    "\n",
    "for idx, json_string in enumerate(metadata_train):\n",
    "    try:\n",
    "        data_dict = json.loads(json_string)  # parse JSON\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"[SKIP] Index {idx}: JSON parse error -> {e}\")\n",
    "        continue\n",
    "\n",
    "    # check if it's an array\n",
    "    if isinstance(data_dict['key'], list):\n",
    "        for i in range(len(data_dict['key'])):\n",
    "            n_key = data_dict['key'][i]\n",
    "            n_sub = data_dict['sub'][i]\n",
    "            n_mod = data_dict['mod'][i]\n",
    "            n_task = data_dict['task'][i]\n",
    "            n_mag = data_dict['mag'][i]\n",
    "            n_dir = data_dict['dir'][i]\n",
    "            n_start = data_dict['start'][i]\n",
    "            n_trial_type = data_dict['trial_type'][i]\n",
    "\n",
    "            flattened.append({\n",
    "                \"key\": n_key,\n",
    "                \"sub\": n_sub,\n",
    "                \"mod\": n_mod,\n",
    "                \"task\": n_task,\n",
    "                \"mag\": n_mag,\n",
    "                \"dir\": n_dir,\n",
    "                \"start\": n_start,\n",
    "                \"trial_type\": n_trial_type\n",
    "            })\n",
    "    else:\n",
    "        flattened.append({\n",
    "            \"key\": data_dict['key'],\n",
    "            \"sub\": data_dict['sub'],\n",
    "            \"mod\": data_dict['mod'],\n",
    "            \"task\": data_dict['task'],\n",
    "            \"mag\": data_dict['mag'],\n",
    "            \"dir\": data_dict['dir'],\n",
    "            \"start\": data_dict['start'],\n",
    "            \"trial_type\": data_dict['trial_type']\n",
    "        })\n",
    "\n",
    "        \n",
    "\n",
    "print(f\"Flattened length: {len(flattened)}\")\n",
    "print(\"First few items:\")\n",
    "print(flattened[:5])\n",
    "metadata_train = flattened\n",
    "\n",
    "import json\n",
    "\n",
    "fields = [\"key\", \"sub\", \"mod\", \"task\", \"mag\", \"dir\", \"start\", \"trial_type\"]\n",
    "\n",
    "flattened = []\n",
    "\n",
    "for idx, json_string in enumerate(metadata_test):\n",
    "    try:\n",
    "        data_dict = json.loads(json_string)  # parse JSON\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"[SKIP] Index {idx}: JSON parse error -> {e}\")\n",
    "        continue\n",
    "\n",
    "    # check if it's an array\n",
    "    if isinstance(data_dict['key'], list):\n",
    "        for i in range(len(data_dict['key'])):\n",
    "            n_key = data_dict['key'][i]\n",
    "            n_sub = data_dict['sub'][i]\n",
    "            n_mod = data_dict['mod'][i]\n",
    "            n_task = data_dict['task'][i]\n",
    "            n_mag = data_dict['mag'][i]\n",
    "            n_dir = data_dict['dir'][i]\n",
    "            n_start = data_dict['start'][i]\n",
    "            n_trial_type = data_dict['trial_type'][i]\n",
    "\n",
    "            flattened.append({\n",
    "                \"key\": n_key,\n",
    "                \"sub\": n_sub,\n",
    "                \"mod\": n_mod,\n",
    "                \"task\": n_task,\n",
    "                \"mag\": n_mag,\n",
    "                \"dir\": n_dir,\n",
    "                \"start\": n_start,\n",
    "                \"trial_type\": n_trial_type\n",
    "            })\n",
    "    else:\n",
    "        flattened.append({\n",
    "            \"key\": data_dict['key'],\n",
    "            \"sub\": data_dict['sub'],\n",
    "            \"mod\": data_dict['mod'],\n",
    "            \"task\": data_dict['task'],\n",
    "            \"mag\": data_dict['mag'],\n",
    "            \"dir\": data_dict['dir'],\n",
    "            \"start\": data_dict['start'],\n",
    "            \"trial_type\": data_dict['trial_type']\n",
    "        })\n",
    "\n",
    "        \n",
    "\n",
    "print(f\"Flattened length: {len(flattened)}\")\n",
    "print(\"First few items:\")\n",
    "print(flattened[:5])\n",
    "metadata_test = flattened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b52b668f-c143-40fe-b3d4-c21c7a406d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets\n",
      "Datasets ready\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HCPFlatDataset(Dataset):\n",
    "    def __init__(self, flatmaps, metadata):\n",
    "        self.flatmaps = flatmaps\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.flatmaps[idx], self.metadata[idx]#json.loads(self.metadata[idx])\n",
    "print(\"Creating datasets\")\n",
    "# Loading to cpu for faster training, this can take several minutes.  Remove this [:] if you want to move one at the time.\n",
    "train_dataset = HCPFlatDataset(flatmaps_train, metadata_train)\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_dataset = HCPFlatDataset(flatmaps_test, metadata_test)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "print(\"Datasets ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cee7abc3-3be5-451c-96fb-775415fa25da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file containing subject information\n",
    "if target == \"age\" or target == \"sex\":\n",
    "    subject_information_HCP_path = os.path.join(hcp_flat_path, \"subjects_data_restricted.csv\")\n",
    "    try:\n",
    "        subject_information_HCP = pd.read_csv(subject_information_HCP_path)\n",
    "    except:\n",
    "        try:\n",
    "            subject_information_HCP = pd.read_csv('./unrestricted_clane9_4_23_2024_13_28_14.csv')   \n",
    "        except:\n",
    "            assert False, \"Subject information file not found\"\n",
    "\n",
    "    ###### This is for unrestricted\n",
    "    # age_related_columns = [\n",
    "    #     'Age', 'PicSeq_AgeAdj', 'CardSort_AgeAdj', 'Flanker_AgeAdj',\n",
    "    #     'ReadEng_AgeAdj', 'PicVocab_AgeAdj', 'ProcSpeed_AgeAdj',\n",
    "    #     'CogFluidComp_AgeAdj', 'CogEarlyComp_AgeAdj', 'CogTotalComp_AgeAdj',\n",
    "    #     'CogCrystalComp_AgeAdj', 'Endurance_AgeAdj', 'Dexterity_AgeAdj',\n",
    "    #     'Strength_AgeAdj', 'Odor_AgeAdj', 'Taste_AgeAdj'\n",
    "    # ]\n",
    "    \n",
    "    # sex_related_columns = [\n",
    "    #     'Gender'\n",
    "    # ]\n",
    "\n",
    "    ###### This is for restricted\n",
    "    gender_related_columns = [\n",
    "        'Gender'\n",
    "    ]\n",
    "\n",
    "    age_related_columns = [\n",
    "        'Age_in_Yrs',\n",
    "        'Menstrual_AgeBegan',\n",
    "        'Menstrual_AgeIrreg',\n",
    "        'Menstrual_AgeStop',\n",
    "        'SSAGA_Alc_Age_1st_Use',\n",
    "        'SSAGA_TB_Age_1st_Cig',\n",
    "        'SSAGA_Mj_Age_1st_Use',\n",
    "        'Endurance_AgeAdj',\n",
    "        'Dexterity_AgeAdj',\n",
    "        'Strength_AgeAdj',\n",
    "        'PicSeq_AgeAdj',\n",
    "        'CardSort_AgeAdj',\n",
    "        'Flanker_AgeAdj',\n",
    "        'ReadEng_AgeAdj',\n",
    "        'PicVocab_AgeAdj',\n",
    "        'ProcSpeed_AgeAdj',\n",
    "        'Odor_AgeAdj',\n",
    "        'Taste_AgeAdj'\n",
    "    ]\n",
    "\n",
    "    # # show the first few rows of the subject information\n",
    "    # subject_information_HCP[age_related_columns + sex_related_columns].head()\n",
    "\n",
    "    # Handle missing values (e.g., impute with mean)\n",
    "    mean_age = subject_information_HCP['Age_in_Yrs'].mean()\n",
    "    \n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Perform z-score normalization\n",
    "    subject_information_HCP['Age_in_Yrs_z'] = scaler.fit_transform(subject_information_HCP[['Age_in_Yrs']])\n",
    "\n",
    "\n",
    "    \n",
    "def get_label_unrestricted(subject_id: List[str], target: str, method_for_age: str = 'mean') -> List:\n",
    "    \"\"\"\n",
    "    Get the label for the given subject id and target.\n",
    "\n",
    "    For sex 0 is F and 1 is M\n",
    "    \"\"\"\n",
    "\n",
    "    # convert to list of ints\n",
    "    subject_id = [int(x) for x in subject_id]\n",
    "\n",
    "    if target == \"age\":\n",
    "        age_array = []\n",
    "        for subject in subject_id:\n",
    "            c_age = subject_information_HCP[subject_information_HCP['Subject'] == subject]['Age'].values\n",
    "            # if the subject is not in the subject information file trigger an error\n",
    "            if len(c_age) == 0:\n",
    "                assert False, f\"Subject {subject} not found in subject information file\"\n",
    "            if len(c_age) > 1:\n",
    "                print(f\"Warning: Multiple entries for subject {subject}\")\n",
    "\n",
    "            c_age = c_age[0].split('-')\n",
    "            if len(c_age) < 2:\n",
    "                c_age = c_age[0].split('+')\n",
    "                age_array.append(int(c_age[0]))\n",
    "            else:\n",
    "                if method_for_age == 'mean':\n",
    "                    age_array.append(np.mean([int(x) for x in c_age]))\n",
    "                elif method_for_age == 'min':\n",
    "                    age_array.append(np.min([int(x) for x in c_age]))\n",
    "                elif method_for_age == 'max':\n",
    "                    age_array.append(np.max([int(x) for x in c_age]))\n",
    "                else:\n",
    "                    assert False, f\"Method {method_for_age} not recognized\"\n",
    "\n",
    "        return np.array(age_array)  \n",
    "    \n",
    "    elif target == 'sex':\n",
    "        sex_array = []\n",
    "        for subject in subject_id:\n",
    "            c_sex = subject_information_HCP[subject_information_HCP['Subject'] == subject]['Gender'].values\n",
    "            # if the subject is not in the subject information file trigger an error\n",
    "            if len(c_sex) == 0:\n",
    "                assert False, f\"Subject {subject} not found in subject information file\"\n",
    "            if len(c_sex) > 1:\n",
    "                print(f\"Warning: Multiple entries for subject {subject}\")\n",
    "            sex_array.append(int(c_sex[0] == 'M'))\n",
    "        return sex_array\n",
    "\n",
    "def get_label_restricted(subject_id: List[str], target: str, normalized: bool = True) -> List:\n",
    "    \"\"\"\n",
    "    Get the label for the given subject id and target.\n",
    "\n",
    "    For sex 0 is F and 1 is M\n",
    "    \"\"\"\n",
    "\n",
    "    # convert to list of ints\n",
    "    subject_id = [int(x) for x in subject_id]\n",
    "\n",
    "    if target == \"age\":\n",
    "        age_array = []\n",
    "        for subject in subject_id:\n",
    "            c_age = subject_information_HCP[subject_information_HCP['Subject'] == subject]['Age_in_Yrs' if not normalized else 'Age_in_Yrs_z'].values\n",
    "            # if the subject is not in the subject information file trigger an error\n",
    "            if len(c_age) == 0:\n",
    "                assert False, f\"Subject {subject} not found in subject information file\"\n",
    "            if len(c_age) > 1:\n",
    "                print(f\"Warning: Multiple entries for subject {subject}\")\n",
    "\n",
    "            age_array.append(np.int8(c_age[0]))\n",
    "\n",
    "        return np.array(age_array)  \n",
    "    \n",
    "    elif target == 'sex':\n",
    "        sex_array = []\n",
    "        for subject in subject_id:\n",
    "            c_sex = subject_information_HCP[subject_information_HCP['Subject'] == subject]['Gender'].values\n",
    "            # if the subject is not in the subject information file trigger an error\n",
    "            if len(c_sex) == 0:\n",
    "                assert False, f\"Subject {subject} not found in subject information file\"\n",
    "            if len(c_sex) > 1:\n",
    "                print(f\"Warning: Multiple entries for subject {subject}\")\n",
    "            sex_array.append(int(c_sex[0] == 'M'))\n",
    "        return sex_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a535979-1fc4-4e72-9fe2-96bb8897534b",
   "metadata": {},
   "source": [
    "### Creating and loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b92beee5-c65a-4714-b640-ae545dc1b120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_size (144, 320) patch_size (16, 16) frames 16 t_patch_size 2\n",
      "model initialized\n"
     ]
    }
   ],
   "source": [
    "from mae_utils.flat import load_hcp_flat_mask, load_nsd_flat_mask\n",
    "from mae_utils.flat import create_hcp_flat\n",
    "from mae_utils.flat import batch_unmask\n",
    "import mae_utils.visualize as vis\n",
    "\n",
    "\n",
    "if \"HCP\" in datasets_to_include:\n",
    "    flat_mask = load_hcp_flat_mask()\n",
    "    nsd_mask = None\n",
    "    hcp_mask = None\n",
    "elif \"NSD\" in datasets_to_include:\n",
    "    flat_mask = load_nsd_flat_mask()\n",
    "    nsd_mask = None\n",
    "    hcp_mask = None\n",
    "elif \"BOTH\" in datasets_to_include:\n",
    "    flat_mask = None\n",
    "    nsd_mask = load_nsd_flat_mask()\n",
    "    hcp_mask = load_hcp_flat_mask()\n",
    "\n",
    "assert model_size in {\"huge\", \"large\", \"small\"}, \"undefined model_size\"\n",
    "\n",
    "if model_size==\"huge\":\n",
    "    mae_model = flat_models.mae_vit_huge_fmri(\n",
    "        patch_size=patch_size,\n",
    "        decoder_embed_dim=decoder_embed_dim,\n",
    "        t_patch_size=t_patch_size,\n",
    "        pred_t_dim=pred_t_dim,\n",
    "        decoder_depth=4,\n",
    "        cls_embed=cls_embed,\n",
    "        norm_pix_loss=norm_pix_loss,\n",
    "        no_qkv_bias=no_qkv_bias,\n",
    "        sep_pos_embed=sep_pos_embed,\n",
    "        trunc_init=trunc_init,\n",
    "        pct_masks_to_decode=pct_masks_to_decode,\n",
    "        img_mask=flat_mask,\n",
    "        nsd_mask=nsd_mask,\n",
    "        hcp_mask=hcp_mask,\n",
    "        use_source_embeds=use_source_embeds,\n",
    "        use_decoder_contrastive_loss=use_decoder_contrastive_loss,\n",
    "        source_embed_train_mode=source_embed_train_mode,\n",
    "        source_embed_mode=source_embed_mode,\n",
    "        use_contrastive_loss=use_contrastive_loss\n",
    "    )\n",
    "elif model_size==\"large\":\n",
    "    mae_model = flat_models.mae_vit_large_fmri(\n",
    "        patch_size=patch_size,\n",
    "        decoder_embed_dim=decoder_embed_dim,\n",
    "        t_patch_size=t_patch_size,\n",
    "        pred_t_dim=pred_t_dim,\n",
    "        decoder_depth=4,\n",
    "        cls_embed=cls_embed,\n",
    "        norm_pix_loss=norm_pix_loss,\n",
    "        no_qkv_bias=no_qkv_bias,\n",
    "        sep_pos_embed=sep_pos_embed,\n",
    "        trunc_init=trunc_init,\n",
    "        pct_masks_to_decode=pct_masks_to_decode,\n",
    "        img_mask=flat_mask,\n",
    "        nsd_mask=nsd_mask,\n",
    "        hcp_mask=hcp_mask,\n",
    "        use_source_embeds=use_source_embeds,\n",
    "        use_decoder_contrastive_loss=use_decoder_contrastive_loss,\n",
    "        source_embed_train_mode=source_embed_train_mode,\n",
    "        source_embed_mode=source_embed_mode,\n",
    "        use_contrastive_loss=use_contrastive_loss\n",
    "    )\n",
    "elif model_size==\"small\":\n",
    "    mae_model = flat_models.mae_vit_small_fmri(\n",
    "        patch_size=patch_size,\n",
    "        decoder_embed_dim=decoder_embed_dim,\n",
    "        t_patch_size=t_patch_size,\n",
    "        pred_t_dim=pred_t_dim,\n",
    "        decoder_depth=4,\n",
    "        cls_embed=cls_embed,\n",
    "        norm_pix_loss=norm_pix_loss,\n",
    "        no_qkv_bias=no_qkv_bias,\n",
    "        sep_pos_embed=sep_pos_embed,\n",
    "        trunc_init=trunc_init,\n",
    "        pct_masks_to_decode=pct_masks_to_decode,\n",
    "        img_mask=flat_mask,\n",
    "        nsd_mask=nsd_mask,\n",
    "        hcp_mask=hcp_mask,\n",
    "        use_source_embeds=use_source_embeds,\n",
    "        use_decoder_contrastive_loss=use_decoder_contrastive_loss,\n",
    "        source_embed_train_mode=source_embed_train_mode,\n",
    "        source_embed_mode=source_embed_mode,\n",
    "        use_contrastive_loss=use_contrastive_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54cc1fdc-e43d-44ae-a562-a492a59580a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest_checkpoint: epoch0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_243901/2629719241.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded checkpoint epoch0.pth from /weka/proj-fmri/ckadirt/fMRI-foundation-model/src/checkpoints/BOTHflat_small_gsrFalse_sourceadd_cont_91113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint_files = [f for f in os.listdir(outdir) if f.endswith('.pth')]\n",
    "\n",
    "latest_checkpoint = epoch_checkpoint\n",
    "    \n",
    "print(f\"latest_checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = os.path.join(outdir, latest_checkpoint)\n",
    "\n",
    "state = torch.load(checkpoint_path)\n",
    "mae_model.load_state_dict(state[\"model_state_dict\"], strict=False)\n",
    "mae_model.to(device)\n",
    "\n",
    "print(f\"\\nLoaded checkpoint {latest_checkpoint} from {outdir}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fde712b-36a7-46e7-a39a-7679e6cbe070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: 384\n"
     ]
    }
   ],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the input except for the batch dimension\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.linear(x)\n",
    "        return out  # Raw logits\n",
    "\n",
    "# Determine the input dimension from a single sample\n",
    "# Assuming images are of shape [1, 16, 144, 320]\n",
    "input_dim = np.prod(mae_model(torch.randn(1,1,16,144,320).to(device),forward_features=True, global_pool=global_pool, cls_forward=cls_forward).shape[1:])\n",
    "print(f\"Input dimension: {input_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8576297-ac37-40d9-b871-f23812c16502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, False, True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_model.n_mask_patches, cls_forward, global_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a2c6527-fa75-42ba-be43-3d753ed8100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self, lc_model, mae_model):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.lc_model = lc_model\n",
    "        self.mae_model = mae_model\n",
    "        \n",
    "        \n",
    "    def forward(self, x, gsr):\n",
    "        x = self.mae_model(x, forward_features=True, global_pool=global_pool, cls_forward=cls_forward)\n",
    "        x = self.lc_model(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8983a0b-c810-4424-8c31-32b9921ebd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 109700\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "\n",
    "if target == \"trial_type\":\n",
    "    lc_model = LinearClassifier(input_dim=input_dim, num_classes=num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "elif target == \"age\":\n",
    "    lc_model = LinearClassifier(input_dim=input_dim, num_classes=1)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "elif target == \"sex\":\n",
    "    lc_model = LinearClassifier(input_dim=input_dim, num_classes=1)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# lc_model = LinearClassifier(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "model = FullModel(lc_model, mae_model)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer with L2 regularization (weight_decay)\n",
    "# learning_rate = 1e-4\n",
    "# weight_decay = 1e-5  # Adjust based on your needs\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "\n",
    "num_iterations_per_epoch = math.ceil(flatmaps_train.shape[0]/batch_size)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# num_epochs = 20  # Adjust as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98629867-6d64-45ef-823d-5f4fdb279ec7",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b42844f-0b96-4e68-ae38-3af24720a803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'70c25fb6-e0d4-463b-bcd3-31625ba592ee'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "myuuid = uuid.uuid4()\n",
    "str(myuuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63ce8f26-a1b9-4e29-badc-7b9b30d0d3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in interactive notebook. Disabling W&B and ckpt saving.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "if utils.is_interactive():\n",
    "    print(\"Running in interactive notebook. Disabling W&B and ckpt saving.\")\n",
    "    wandb_log = False\n",
    "    save_ckpt = False\n",
    "\n",
    "if wandb_log:\n",
    "    wandb_project = 'fMRI-foundation-model'\n",
    "    wandb_config = {\n",
    "        \"model_name\": f'{found_model_name}_HCP_FT_{target}',\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"seed\": seed,\n",
    "        \"lr_scheduler_type\": lr_scheduler_type,\n",
    "        \"save_ckpt\": save_ckpt,\n",
    "        \"seed\": seed,\n",
    "        \"max_lr\": max_lr,\n",
    "        \"target\": target,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"weight_decay\": weight_decay\n",
    "    }\n",
    "    print(\"wandb_config:\\n\", wandb_config)\n",
    "    random_id = random.randint(0, 100000)\n",
    "    wandb_id = f\"{found_model_name}_{model_suffix}_{target}_HCPFT_{myuuid}\"\n",
    "    print(\"wandb_id:\", wandb_id)\n",
    "    wandb.init(\n",
    "        id=wandb_id,\n",
    "        project=wandb_project,\n",
    "        name=f\"{found_model_name}_{model_suffix}_{target}_HCPFT\",\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f43535f4-0f6e-4655-b4f0-926416685358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a4bf9c704d41b39fdbf57a29d55159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20 - Training:   0%|          | 0/5485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f1e9c54f740>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/logging/__init__.py\", line 237, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    mse_age_train = 0.0\n",
    "    total_train = 0\n",
    "    step = 0\n",
    "\n",
    "    # with torch.amp.autocast(device_type='cuda'):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        images = batch[0].to(device).float()  # Shape: [batch_size, 1, 16, 144, 320]\n",
    "\n",
    "        images_shape = images.shape\n",
    "        images_reshaped = images.view(len(images), 2, images_shape[2]//2, images_shape[3], images_shape[4])\n",
    "        images = images_reshaped.mean(dim=1).unsqueeze(1) \n",
    "        \n",
    "        # Prepare labels based on target type\n",
    "        if target == \"trial_type\":\n",
    "            labels = batch[1]['trial_type']  # List of labels\n",
    "            labels = label_encoder.transform(labels)\n",
    "            labels = torch.tensor(labels, dtype=torch.long).to(device)  # Shape: [batch_size]\n",
    "        elif target == \"age\":\n",
    "            labels = get_label_restricted(batch[1]['sub'], 'age')\n",
    "            labels = torch.tensor(labels, dtype=torch.float).to(device)  # Shape: [batch_size]\n",
    "        elif target == \"sex\":\n",
    "            labels = get_label_restricted(batch[1]['sub'], 'sex')\n",
    "            labels = torch.tensor(labels, dtype=torch.float).to(device)  # Shape: [batch_size]\n",
    "        # labels = labels.unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images, gsr=gsr)  # Shape: [num_train_samples, num_classes]\n",
    "        \n",
    "        # Compute loss\n",
    "        if target in [\"trial_type\", \"sex\"]:\n",
    "            # For classification, ensure outputs are logits\n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "        elif target == \"age\":\n",
    "            # For regression, ensure outputs are single values\n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        running_train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        \n",
    "       # Calculate and accumulate metrics\n",
    "        if target == \"trial_type\":\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        elif target == \"age\":\n",
    "            mse_age_train += (torch.sum((outputs.squeeze() - labels) ** 2).item())\n",
    "        elif target == \"sex\":\n",
    "            threshold = 0.5\n",
    "            predicted = (torch.sigmoid(outputs) > threshold).float().squeeze()\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        total_train += labels.size(0)\n",
    "        step += 1\n",
    "\n",
    "        # Print intermediate metrics every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            if target in [\"trial_type\", \"sex\"]:\n",
    "                current_accuracy = 100 * correct_train / total_train if total_train > 0 else 0.0\n",
    "                print(f\"Step [{step}/{len(train_dl)}] - Training Loss: {loss.item():.4f} - Training Accuracy: {current_accuracy:.2f}%\")\n",
    "            elif target == \"age\":\n",
    "                current_mse = mse_age_train / total_train if total_train > 0 else 0.0\n",
    "                print(f\"Step [{step}/{len(train_dl)}] - Training Loss: {loss.item():.4f} - Training MSE: {current_mse:.4f}\")\n",
    "        if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "\n",
    "    # Calculate epoch-level metrics\n",
    "    epoch_train_loss = running_train_loss / total_train if total_train > 0 else 0.0\n",
    "\n",
    "    if target in [\"trial_type\", \"sex\"]:\n",
    "        train_accuracy = 100 * correct_train / total_train if total_train > 0 else 0.0\n",
    "    elif target == \"age\":\n",
    "        train_mse = mse_age_train / total_train if total_train > 0 else 0.0\n",
    "    \n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    mse_age_val = 0.0\n",
    "    total_val = 0\n",
    "    step_val = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dl, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            \n",
    "            images = batch[0].to(device).float()\n",
    "            labels = batch[1]['trial_type']\n",
    "\n",
    "            images_shape = images.shape\n",
    "            images_reshaped = images.view(len(images), 2, images_shape[2]//2, images_shape[3], images_shape[4])\n",
    "            images = images_reshaped.mean(dim=1).unsqueeze(1) \n",
    "            \n",
    "            # Prepare labels based on target type\n",
    "            if target == \"trial_type\":\n",
    "                labels = batch[1]['trial_type']  # List of labels\n",
    "                labels = label_encoder.transform(labels)\n",
    "                labels = torch.tensor(labels, dtype=torch.long).to(device)  # Shape: [batch_size]\n",
    "            elif target == \"age\":\n",
    "                labels = get_label_restricted(batch[1]['sub'], 'age')\n",
    "                labels = torch.tensor(labels, dtype=torch.float).to(device)  # Shape: [batch_size]\n",
    "            elif target == \"sex\":\n",
    "                labels = get_label_restricted(batch[1]['sub'], 'sex')\n",
    "                labels = torch.tensor(labels, dtype=torch.float).to(device)  # Shape: [batch_size]\n",
    "\n",
    "            # labels = labels.unsqueeze(1)\n",
    "            # Forward pass\n",
    "            outputs = model(images, gsr=gsr)\n",
    "            \n",
    "            # Compute loss\n",
    "            if target in [\"trial_type\", \"sex\"]:\n",
    "                loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "            elif target == \"age\":\n",
    "                loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "\n",
    "            # Accumulate loss\n",
    "            running_val_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Calculate and accumulate metrics\n",
    "            if target == \"trial_type\":\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "            elif target == \"age\":\n",
    "                mse_age_val += (torch.sum((outputs.squeeze() - labels) ** 2).item())\n",
    "            elif target == \"sex\":\n",
    "                threshold = 0.5\n",
    "                predicted = (torch.sigmoid(outputs) > threshold).float().squeeze()\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            total_val += labels.size(0)\n",
    "            step_val += 1\n",
    "\n",
    "    # Calculate epoch-level validation metrics\n",
    "    epoch_val_loss = running_val_loss / total_val if total_val > 0 else 0.0\n",
    "\n",
    "    if target in [\"trial_type\", \"sex\"]:\n",
    "        val_accuracy = 100 * correct_val / total_val if total_val > 0 else 0.0\n",
    "    elif target == \"age\":\n",
    "        val_mse = mse_age_val / total_val if total_val > 0 else 0.0\n",
    "\n",
    "    # Print epoch-level metrics\n",
    "    if target in [\"trial_type\", \"sex\"]:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"- Training Loss: {epoch_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}% \"\n",
    "              f\"- Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    elif target == \"age\":\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"- Training Loss: {epoch_train_loss:.4f}, Training MSE: {train_mse:.4f} \"\n",
    "              f\"- Validation Loss: {epoch_val_loss:.4f}, Validation MSE: {val_mse:.4f}\")\n",
    "\n",
    "    # Log metrics with wandb\n",
    "    if wandb_log:\n",
    "        log_dict = {\n",
    "            \"epoch_train_loss\": epoch_train_loss,\n",
    "            \"epoch_val_loss\": epoch_val_loss,\n",
    "        }\n",
    "        if target in [\"trial_type\", \"sex\"]:\n",
    "            log_dict.update({\n",
    "                f\"train_accuracy_{target}\": train_accuracy,\n",
    "                f\"val_accuracy_{target}\": val_accuracy,\n",
    "            })\n",
    "        elif target == \"age\":\n",
    "            log_dict.update({\n",
    "                f\"train_mse_{target}\": train_mse,\n",
    "                f\"val_mse_{target}\": val_mse,\n",
    "            })\n",
    "        wandb.log(log_dict)\n",
    "        \n",
    "    if save_ckpt:\n",
    "        outdir = os.path.abspath(f'checkpoints/{f\"{found_model_name}_{model_suffix}_{target}_HCPFT\"}')\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        print(\"outdir\", outdir)\n",
    "        # Save model and config\n",
    "        torch.save(model.state_dict(), f\"{outdir}/model.pth\")\n",
    "        with open(f\"{outdir}/config.yaml\", 'w') as f:\n",
    "            yaml.dump(wandb_config, f)\n",
    "        print(f\"Saved model and config to {outdir}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation_env",
   "language": "python",
   "name": "foundation_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
