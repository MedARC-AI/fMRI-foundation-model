{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0ba2f3-4e5b-4d03-89b4-9b6868536f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CUDA devices: 1\n",
      "LOCAL RANK=0\n",
      "NUM GPUS=1\n",
      "NODE=0\n",
      "GLOBAL RANK=0\n",
      "WORLD_SIZE=1\n",
      "NOT distributed\n",
      "PID of this process = 143404\n",
      "device = cuda distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "import gc\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from models import *\n",
    "from mindeye_models import *\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "# from accelerate import Accelerator\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "### Multi-GPU config ###\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"Number of available CUDA devices: {device_count}\")\n",
    "\n",
    "local_rank = os.getenv('LOCAL_RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(f\"LOCAL RANK={local_rank}\")\n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(f\"NUM GPUS={num_devices}\")\n",
    "distributed = True if num_devices>1 else False\n",
    "if distributed: assert device_count==num_devices\n",
    "\n",
    "node = os.getenv('SLURM_NODEID')\n",
    "if node is None:\n",
    "    node = 0\n",
    "else:\n",
    "    node = int(node)\n",
    "print(f\"NODE={node}\")\n",
    "\n",
    "global_rank = os.getenv('RANK')\n",
    "if global_rank is None:\n",
    "    global_rank = 0\n",
    "else:\n",
    "    global_rank = int(global_rank)\n",
    "print(f\"GLOBAL RANK={global_rank}\")\n",
    "\n",
    "world_size = os.getenv('WORLD_SIZE')\n",
    "if world_size is None: \n",
    "    world_size = 1\n",
    "else:\n",
    "    world_size = int(world_size)\n",
    "print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# FSDP Setup\n",
    "if distributed:\n",
    "    import torch.distributed as dist\n",
    "    import torch.multiprocessing as mp\n",
    "    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "    from torch.distributed.fsdp.api import BackwardPrefetch, CPUOffload, ShardingStrategy\n",
    "    import functools\n",
    "    from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\n",
    "    print(\"starting init_process_group...\")\n",
    "    dist.init_process_group(\"nccl\", rank=global_rank, world_size=world_size)\n",
    "    print(f\"setting device to cuda:{local_rank}\")\n",
    "    try:\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.cuda.current_device() #torch.device('cuda',local_rank)\n",
    "        print(f\"\\nSuccessfully set cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "    except Exception as error:        \n",
    "        print(f\"\\nFAILED TO SET DEVICE cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "        print(\"An exception occurred:\", error)\n",
    "    dist.barrier()\n",
    "    print(\"passed barrier\\n\")\n",
    "else:\n",
    "    print(\"NOT distributed\")\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "print(\"device =\", device, \"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed487017-385c-4e79-b005-71eda9a4c960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load VJEPA parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "\n",
    "# Load MindEye parameters from yaml config (will override any params with same name)\n",
    "mindeye_config = yaml.load(open('mindeye_config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in mindeye_config.keys():\n",
    "    globals()[attribute_name] = mindeye_config[f'{attribute_name}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0abbeb-6d4e-4359-b955-ba23a0072767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  8\n",
      "global_batch_size:  8\n"
     ]
    }
   ],
   "source": [
    "# First use \"accelerate config\" in terminal for setup\n",
    "global_batch_size = batch_size * num_devices\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "# accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "global_batch_size = batch_size * num_devices\n",
    "print(\"batch_size: \", batch_size)\n",
    "print(\"global_batch_size: \", global_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3eb3e-e718-4741-a88b-4e858efcf1ab",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b7b1894-b35e-4acc-9281-8025dddb09c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vjepa config\n",
      "\n",
      " {'model_name': 's3_jepa_small_1node_FSDP_1worker_clamped', 'use_cls_token': False, 'use_contrastive_loss': False, 'constrastive_loss_weight': 1.0, 'batch_size': 1, 'num_workers': 1, 'num_epochs': 200, 'seed': 42, 'max_lr': 3e-05, 'num_samples_per_epoch': 1024, 'ema': [0.998, 1.0], 'ipe_scale': 1.25, 'ckpt_saving': True, 'ckpt_interval': 50, 'resume_from_ckpt': False, 'wandb_log': True, 'x_encoder_start_masking_ratio': 0.95, 'x_encoder_end_masking_ratio': 0.95, 'y_encoder_mask_ratio': 0.95, 'patch_size': 8, 'frame_patch_size': 1, 'use_rope_emb': False, 'masking_strategy': 'MNI', 'embed_dim': 384, 'mlp_dim': 1536, 'depth': 12, 'num_heads': 6, 'dim_head': 64, 'image_size': [88, 104, 72], 'num_frames': 4, 'is_s3': True, 'train_urls': ['/scratch/fmri_foundation_datasets/NSD_MNI_wds/{000000..000494}.tar', '/scratch/fmri_foundation_datasets/NSD_MNI_wds/{000496..000740}.tar'], 's3_train_urls': ['s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_wds/{000000..000494}.tar', 's3://proj-fmri/fmri_foundation_datasets/NSD_MNI_wds/{000496..000740}.tar']}\n",
      "mindeye_config\n",
      " {'model_name': 'jepa_NOpretrain_bs8_xc', 'mae_model_name': 's3_jepa_mini_4gpu_b2', 'batch_size': 8, 'num_workers': 10, 'mixed_precision': 'fp16', 'num_epochs': 50, 'seed': 42, 'max_lr': 0.0003, 'ckpt_saving': False, 'ckpt_interval': 50, 'resume_from_ckpt': False, 'wandb_log': True, 'in_dim': 1236480, 'hidden_dim': 512, 'drop': 0.15, 'mixup_pct': 0.33, 'nsd_wds_path': '/weka/proj-fmri/shared/mindeyev2_dataset/wds', 'nsd_raw_path': '/weka/proj-fmri/shared/mindeyev2_dataset', 'nsd_image_path': '/weka/proj-fmri/shared/mindeyev2_dataset', 'num_sessions': 5}\n",
      "outdir /weka/proj-fmri/paulscotti/fMRI-foundation-model/ckpts/s3_jepa_small_1node_FSDP_1worker_clamped\n",
      "num_patches 5148\n"
     ]
    }
   ],
   "source": [
    "print(\"vjepa config\\n\\n\",config)\n",
    "print(\"mindeye_config\\n\",mindeye_config)\n",
    "\n",
    "if utils.is_interactive():\n",
    "    ckpt_saving = False\n",
    "    wandb_log = False\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{config[\"model_name\"]}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "num_patches = int(\n",
    "    (image_size[0] / patch_size)\n",
    "    * (image_size[1] / patch_size)\n",
    "    * (image_size[2] / patch_size)\n",
    "    * num_frames\n",
    ")\n",
    "print(\"num_patches\", num_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441c6fcd-1d17-4283-8c97-e8a10b932136",
   "metadata": {},
   "source": [
    "# Load VJEPA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f03af316-fea0-4b50-b5ac-2ccdd3d08178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_encoder\n",
      "param counts:\n",
      "21,275,904 total\n",
      "21,275,904 trainable\n",
      "21275904\n",
      "y_encoder\n",
      "param counts:\n",
      "21,275,904 total\n",
      "21,275,904 trainable\n",
      "21275904\n",
      "predictor\n",
      "param counts:\n",
      "21,275,904 total\n",
      "21,275,904 trainable\n",
      "21275904\n"
     ]
    }
   ],
   "source": [
    "image_depth, image_height, image_width = image_size\n",
    "image_patch_size=(patch_size,patch_size,patch_size)\n",
    "patch_depth, patch_height, patch_width = image_patch_size\n",
    "x_encoder = Transformer(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"x_encoder\")\n",
    "print(utils.count_params(x_encoder))\n",
    "y_encoder = Transformer(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"y_encoder\")\n",
    "print(utils.count_params(y_encoder))\n",
    "predictor = Transformer(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"predictor\")\n",
    "print(utils.count_params(predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4278478f-378f-42cb-91bf-eae684ac997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "64,026,880 total\n",
      "64,026,880 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64026880"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleViT(\n",
    "    x_encoder=x_encoder,\n",
    "    y_encoder=y_encoder,\n",
    "    predictor=predictor,\n",
    "    image_size=image_size, \n",
    "    image_patch_size=image_patch_size, \n",
    "    num_frames=num_frames,\n",
    "    frame_patch_size=frame_patch_size,\n",
    "    channels=1,\n",
    "    use_rope_emb=use_rope_emb,\n",
    "    use_cls_token=use_cls_token,\n",
    ")\n",
    "utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212b974d-90b0-4d4b-888d-1137f6e26861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load from ckpt\n",
    "# mae_ckpt_pth = os.path.abspath(f'../ckpts/{mae_model_name}/last.pth')\n",
    "# print(\"mae_ckpt_pth\", mae_ckpt_pth)\n",
    "\n",
    "# checkpoint = torch.load(mae_ckpt_pth)\n",
    "# state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b845aa6d-a8b6-4f71-a029-ebb1429d3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set foundation model to evaluation\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd412c-ac9e-482a-8c13-5e0a7967ca8a",
   "metadata": {},
   "source": [
    "# Setup MindEye Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88f2266c-8278-470c-90fa-6aa77eb284f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nsddata_raw_stimuli = pd.read_csv(f\"{nsd_raw_path}/nsddata_rawdata.csv\")\n",
    "TR_delay = 3 # to account for bold hrf\n",
    "train_TRs = np.round(nsddata_raw_stimuli[nsddata_raw_stimuli['shared1000'] == False]['global_TR_onsets'].values + TR_delay).astype(np.int32)\n",
    "test_TRs = np.round(nsddata_raw_stimuli[nsddata_raw_stimuli['shared1000'] == True]['global_TR_onsets'].values + TR_delay).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369253a2-ba58-4362-a599-d5b2b21a2c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{nsd_image_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:] \n",
    "images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images!\", images.shape)\n",
    "\n",
    "# Load MindEye hdf5\n",
    "f = h5py.File(f'{nsd_raw_path}/subj01_mnidata.h5', 'r') #subj01_rawdata_old.h5\n",
    "mindeye_global_trs = f['global_trs'][:]\n",
    "mindeye_funcs = f['funcs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ccbc02-1111-4e02-8809-68c319a1d037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=\"ViT-bigG-14\",\n",
    "    version=\"laion2b_s39b_b160k\",\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "clip_img_embedder.to(device)\n",
    "clip_seq_dim, clip_emb_dim = 256, 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce30ef3-e898-412e-a6d7-2ab002dcf9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subj = s = 1\n",
    "subj_list = [subj]\n",
    "\n",
    "num_samples_per_epoch = (750*num_sessions) // num_devices\n",
    "num_iterations_per_epoch = num_samples_per_epoch // batch_size\n",
    "print(\"num_iterations_per_epoch: \", num_iterations_per_epoch)\n",
    "train_data = {}\n",
    "train_dl = {}\n",
    "\n",
    "print(f\"Training with {num_sessions} sessions\")\n",
    "train_url = f\"{nsd_wds_path}/subj0{s}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "print(train_url)\n",
    "    \n",
    "train_data[f'subj0{s}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=utils.my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "# train_dl[f'subj0{s}'] = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "train_dl[f'subj0{s}'] = wds.WebLoader(\n",
    "    train_data[f'subj0{s}'].batched(batch_size), \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    batch_size=None,\n",
    "    num_workers=num_workers, \n",
    "    persistent_workers=num_workers>0,\n",
    ").with_epoch(num_iterations_per_epoch)\n",
    "\n",
    "if global_rank==0:\n",
    "    print(\"Loaded all subj train dls and betas!\\n\")\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{nsd_wds_path}/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "    print(test_url)\n",
    "    test_data = wds.WebDataset(test_url,resampled=True,nodesplitter=utils.my_split_by_node)\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    # test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "    test_dl = wds.WebLoader(\n",
    "        test_data.batched(num_test),\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        batch_size=None,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=num_workers>0,\n",
    "    ).with_epoch(10)\n",
    "    print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f49b37-0aa8-47f1-987f-e2f53000a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate.state import AcceleratorState\n",
    "# try:\n",
    "#     AcceleratorState().deepspeed_plugin.deepspeed_config['train_micro_batch_size_per_gpu'] = global_batch_size\n",
    "#     print(\"deepspeed reconfigured, train_micro_batch_size_per_gpu = \", global_batch_size)\n",
    "# except:\n",
    "#     print(\"skipping deepspeed reconfiguration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0444190e-33ff-4903-bdf2-0845f450d04a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mindeye = MindEyeModule()\n",
    "mindeye.ridge = RidgeRegression(np.array([in_dim]), out_features=hidden_dim)\n",
    "mindeye.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, n_blocks=4, drop=drop,\n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim, clip_scale=1)\n",
    "utils.count_params(mindeye.ridge)\n",
    "utils.count_params(mindeye.backbone)\n",
    "utils.count_params(mindeye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84127f7a-2108-4e5d-8ac6-6149b13c1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed: \n",
    "    my_auto_wrap_policy = functools.partial(\n",
    "        size_based_auto_wrap_policy, min_num_params=200000\n",
    "    )\n",
    "    print(f\"\\nPrepping FSDP on {global_rank} {node}...\\n\")\n",
    "    mindeye = FSDP(\n",
    "        mindeye,\n",
    "        sharding_strategy=ShardingStrategy.HYBRID_SHARD,\n",
    "        auto_wrap_policy=my_auto_wrap_policy,\n",
    "        use_orig_params=False,\n",
    "        cpu_offload=None, #CPUOffload(offload_params=True)\n",
    "        sync_module_states=True,\n",
    "        limit_all_gathers=True, # See https://github.com/pytorch/pytorch/issues/91165\n",
    "        device_id=device,\n",
    "    )\n",
    "    print(f\"\\nSuccessfully loaded FSDP model to device on global_rank {global_rank}\\n\")\n",
    "    dist.barrier()\n",
    "    print(f\"\\nSuccessfully passed barrier! {global_rank}\\n\")\n",
    "else:\n",
    "    mindeye=mindeye.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad1b981-00f4-4072-8402-c0775881edd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in mindeye.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in mindeye.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in mindeye.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "total_steps = num_epochs * num_iterations_per_epoch\n",
    "print(\"total_steps\", total_steps)\n",
    "pct_start = 2/num_epochs if num_epochs>1 else 1.\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    ")\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55228b07-e8ae-448c-972e-800aa6dc7ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_ckpt(tag=\"last\"):\n",
    "    ckpt_path = outdir+f'/{tag}/downstream'\n",
    "    os.makedirs(ckpt_path,exist_ok=True)\n",
    "    accelerator.save_model(model, ckpt_path, max_shard_size=\"2GB\", safe_serialization=True)\n",
    "    print(f\"\\n---saved {ckpt_path}!---\\n\")\n",
    "        \n",
    "def save_progress(tag=\"last\"):\n",
    "    if accelerator.is_main_process:\n",
    "        ckpt_path = outdir+f'/{tag}/downstream'\n",
    "        torch.save(\n",
    "                {\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"scheduler\": lr_scheduler.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"losses\": losses,\n",
    "                    \"test_losses\": test_losses,\n",
    "                    \"lrs\": lrs,\n",
    "                },\n",
    "                os.path.join(ckpt_path, f\"params.pt\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290b05d-9985-4a68-919e-2baeccc4af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if global_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found_downstream'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"mae_model_name\": mae_model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"in_dim\": in_dim,\n",
    "      \"hidden_dim\": hidden_dim,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_params\": num_params,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2a116-fc5d-4be9-8f9c-01b00618c8f9",
   "metadata": {},
   "source": [
    "# Train MindEye with Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee95c4-2011-4187-96e7-4fa840458507",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9624a-5f39-46b9-83d3-5c8288511490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "MNI_brain = nib.load(\"/weka/proj-fmri/paulscotti/fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "brain_pos_voxels = MNI_brain[6:94,8:112,10:82]\n",
    "brain_pos_voxels[6:94,:(112-60),10:62] = 0\n",
    "brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "tube_mask = (brain_pos_pats_vit > 0).tile(num_frames)\n",
    "print(tube_mask.sum(), tube_mask.sum() / len(tube_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eafa9d-8e17-436e-80b0-831996536452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume from ckpt (e.g., if you are resuming from a run that got pre-empted)\n",
    "load_progress = False\n",
    "if wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "        load_progress = True\n",
    "elif resume_from_ckpt: # if resuming without using wandb\n",
    "    load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "    load_progress = True\n",
    "    \n",
    "if load_progress:\n",
    "    ckpt_path = outdir+'/last'\n",
    "    prev_params = torch.load(ckpt_path+\"/params.pt\")\n",
    "    optimizer.load_state_dict(prev_params[\"optimizer\"])\n",
    "    lr_scheduler.load_state_dict(prev_params[\"scheduler\"])\n",
    "    epoch = prev_params[\"epoch\"]\n",
    "    losses = prev_params[\"losses\"]\n",
    "    test_losses = prev_params[\"test_losses\"]\n",
    "    lrs = prev_params[\"lrs\"]\n",
    "    print(\"Loaded model params from\", ckpt_path, \"at epoch\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27dfcf-804f-4145-8c21-73740b280920",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "# mindeye, optimizer, *train_dls, lr_scheduler = accelerator.prepare(\n",
    "#     mindeye, optimizer, *train_dls, lr_scheduler\n",
    "# )\n",
    "# # skipping test_dl because we just use local_rank=0 for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8d10d-63da-4ae9-8888-d7ee0db9995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch, num_epochs), disable=global_rank!=0)\n",
    "mse = nn.MSELoss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "test_image=None\n",
    "num_test_eval=batch_size # should instead be 300 to mimic MindEye2 retrieval evaluation, but this leads to OOM\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    print(f\"epoch {epoch}\")\n",
    "    mindeye.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    loss_clip_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    annot_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    for s, train_dl in enumerate(train_dls):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            for iter, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(train_dl):\n",
    "                image0 = images[behav0[:,0,0].cpu().long()].float()\n",
    "                image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "\n",
    "                if epoch==0 and iter==0: print(\"\\nreached start of train!\\n\")\n",
    "\n",
    "                # if images are not fully preloaded, then can do this inefficient but more memory friendly approach\n",
    "                # for ib,b in enumerate(behav0[:,0,0].cpu().long()):\n",
    "                #     if ib==0:\n",
    "                #         image0 = torch.Tensor(images[[b]])\n",
    "                #     else:\n",
    "                #         image0 = torch.vstack((image0, torch.Tensor(images[[b]])))\n",
    "                # image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "                \n",
    "                # get the corresponding raw voxel time series\n",
    "                for ib,b in enumerate(behav0[:,0,5].cpu().long().numpy()):\n",
    "                    tr = (nsddata_raw_stimuli[nsddata_raw_stimuli['global_trial'].isin([b.item()])]['global_TR_onsets'].values + TR_delay).astype(np.int32).item()\n",
    "                    if ib==0:\n",
    "                        voxels_raw = mindeye_funcs[tr-2:tr+2][None][None]\n",
    "                    else:\n",
    "                        voxels_raw = np.vstack((voxels_raw, mindeye_funcs[tr-2:tr+2][None][None]))\n",
    "                voxels_raw = torch.Tensor(voxels_raw).to(device)\n",
    "                \n",
    "                ## Process it through pretrained VJEPA Y-Encoder (encodes the entire voxels_raw) ##\n",
    "                encoder_out = model(voxels_raw, encoder_mask=tube_mask, encoder_type = \"y\")\n",
    "                voxel0 = encoder_out.flatten(1).unsqueeze(1).cpu()\n",
    "                \n",
    "                assert len(voxel0) == batch_size\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                    perm_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = perm\n",
    "                    betas_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = betas\n",
    "                    select_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = select\n",
    "\n",
    "                voxel_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = voxel0\n",
    "\n",
    "                if iter >= num_iterations_per_epoch:\n",
    "                    break\n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i in range(num_iterations_per_epoch):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss=0.\n",
    "            if train_i==0 and epoch==0: print(f\"\\nreached start of {train_i} train loop!\\n\")\n",
    "\n",
    "            voxel_list = [voxel_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "            image = image_iters[train_i].detach()\n",
    "            image = image.to(device)\n",
    "\n",
    "            clip_target = clip_img_embedder(image)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                perm_list = [perm_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                perm = torch.cat(perm_list, dim=0)\n",
    "                betas_list = [betas_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                betas = torch.cat(betas_list, dim=0)\n",
    "                select_list = [select_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                select = torch.cat(select_list, dim=0)\n",
    "\n",
    "            voxel_ridge_list = [mindeye.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "            voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "            backbone, clip_voxels = mindeye.backbone(voxel_ridge)\n",
    "\n",
    "            clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "            clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):                \n",
    "                loss_clip = utils.mixco_nce(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006,\n",
    "                    perm=perm, betas=betas, select=select)\n",
    "            else:\n",
    "                epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=epoch_temp)\n",
    "\n",
    "            loss_clip_total += loss_clip.item()\n",
    "            loss += loss_clip\n",
    "\n",
    "            # forward and backward top 1 accuracy        \n",
    "            labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "            fwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "            bwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            utils.check_loss(loss)\n",
    "            loss.backward()\n",
    "            # accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    mindeye.eval()\n",
    "    if global_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):  \n",
    "                loss=0.     \n",
    "                if test_i==0 and epoch==0: print(f\"\\nreached start of {test_i} test loop!\\n\")\n",
    "\n",
    "                coco_idx = behav[:,0,0].cpu().long()\n",
    "                _,test_indices = np.unique(coco_idx, return_index=True)\n",
    "                test_indices = np.random.permutation(test_indices)[:num_test_eval]\n",
    "                image = images[coco_idx[test_indices]].float().to(device)\n",
    "                \n",
    "                # get the corresponding raw voxel time series\n",
    "                for ib,b in enumerate(behav[test_indices,0,5].cpu().long().numpy()):\n",
    "                    tr = (nsddata_raw_stimuli[nsddata_raw_stimuli['global_trial'].isin([b.item()])]['global_TR_onsets'].values + TR_delay).astype(np.int32).item()\n",
    "                    if ib==0:\n",
    "                        voxels_raw = mindeye_funcs[tr-2:tr+2][None][None]\n",
    "                    else:\n",
    "                        voxels_raw = np.vstack((voxels_raw, mindeye_funcs[tr-2:tr+2][None][None]))\n",
    "                voxels_raw = torch.Tensor(voxels_raw).to(device)\n",
    "                \n",
    "                ## Process it through pretrained MAE ##\n",
    "                encoder_out = model(voxels_raw, encoder_mask=tube_mask, encoder_type = \"y\")\n",
    "                voxel = encoder_out.flatten(1).unsqueeze(1)\n",
    "\n",
    "                assert len(image) == num_test_eval\n",
    "\n",
    "                clip_target = clip_img_embedder(image.float())\n",
    "\n",
    "                voxel_ridge = mindeye.ridge(voxel,0) # 0th index of subj_list\n",
    "                backbone, clip_voxels = mindeye.backbone(voxel_ridge)\n",
    "\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006)\n",
    "\n",
    "                test_loss_clip_total += loss_clip.item()\n",
    "                loss += loss_clip\n",
    "\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                test_fwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                test_bwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "            \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "            \"train/lr\": lrs[-1],\n",
    "            \"train/num_steps\": len(losses),\n",
    "            \"test/num_steps\": len(test_losses),\n",
    "            \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "            \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "            \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "            \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "            \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "            \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        if not distributed: print(logs)\n",
    "        if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt()\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    # accelerator.wait_for_everyone()\n",
    "    if distributed: dist.barrier()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46b8a2-ac00-489d-8ab1-18b5b590d6ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training losses\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(test_losses)\n",
    "plt.title(\"Test losses\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
