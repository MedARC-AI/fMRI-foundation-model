{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchio as tio\n",
    "import nibabel as nib\n",
    "import utils\n",
    "from models import *\n",
    "\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "if utils.is_interactive():\n",
    "    %load_ext autoreload\n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc6090a-e05d-4ea7-b6f3-1f98058e1ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### class token config ###\n",
    "use_cls_token = True\n",
    "\n",
    "### Loss Config ###\n",
    "use_contrastive_loss = True\n",
    "constrastive_loss_weight = 1.0\n",
    "use_cls_token = (\n",
    "    True if use_contrastive_loss else use_cls_token\n",
    ")  # if using contrastive loss, we need to add a class token\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv(\"RANK\")\n",
    "if local_rank is None:\n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK\", local_rank)\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices == 0:\n",
    "    num_devices = 1\n",
    "\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "global_batch_size = 8\n",
    "if use_contrastive_loss:\n",
    "    global_batch_size = (\n",
    "        global_batch_size / 2\n",
    "    )  # contrastive loss doubles the batch size with the same samples and different masks\n",
    "print(\"GLOBAL BATCH SIZE\", global_batch_size)\n",
    "\n",
    "print(\"PID of this process =\", os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\", device)\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == \"NO\"\n",
    "print(\n",
    "    \"distributed =\",\n",
    "    distributed,\n",
    "    \"num_devices =\",\n",
    "    num_devices,\n",
    "    \"local rank =\",\n",
    "    local_rank,\n",
    "    \"world size =\",\n",
    "    world_size,\n",
    ")\n",
    "print = accelerator.print  # only print if local_rank=0\n",
    "\n",
    "# set data_type to match your mixed precision\n",
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    data_type = torch.bfloat16\n",
    "elif accelerator.mixed_precision == \"fp16\":\n",
    "    data_type = torch.float16\n",
    "else:\n",
    "    data_type = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8419d-988f-42c6-acb6-b258e0694eee",
   "metadata": {},
   "source": [
    "# Prep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93ecf3-cc80-4f38-a72e-474ccbd10656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = int(global_batch_size / num_devices)\n",
    "print(\"batch_size\", batch_size)\n",
    "num_epochs = 30\n",
    "tube_mask_ratio = 0.75\n",
    "decoder_mask_ratio = 0.75\n",
    "input_size = [64, 64, 48]\n",
    "print(\"input_size\", input_size)\n",
    "seed = 42\n",
    "num_frames = 4\n",
    "tubelet_size = 1\n",
    "\n",
    "img_size = (64, 64, 48)\n",
    "patch_size = 8\n",
    "frame_patch_size = 1\n",
    "num_patches = int(\n",
    "    (img_size[0] / patch_size)\n",
    "    * (img_size[1] / patch_size)\n",
    "    * (img_size[2] / patch_size)\n",
    "    * num_frames\n",
    ")\n",
    "num_patches_per_timepoint = num_patches // num_frames\n",
    "num_encoder_patches = int(\n",
    "    num_patches_per_timepoint * (1 - tube_mask_ratio) * num_frames\n",
    ")\n",
    "num_decoder_patches = int(\n",
    "    num_patches_per_timepoint * (1 - decoder_mask_ratio) * num_frames\n",
    ")\n",
    "print(\"num_patches\", num_patches)\n",
    "print(\"num_encoder_patches\", num_encoder_patches)\n",
    "print(\"num_decoder_patches\", num_decoder_patches)\n",
    "\n",
    "max_lr = 3e-5  # 3e-5 seems to be working best? original videomae used 1.5e-4\n",
    "train_urls = \"s3://proj-fmri/fmri_foundation_datasets/openneuro/{000001..000664}.tar\"\n",
    "test_urls = \"s3://proj-fmri/fmri_foundation_datasets/openneuro/000000.tar\"\n",
    "num_samples_per_epoch = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e37b6b9-5b91-4c4a-af85-ac2af69704e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SimpleViT(\n",
    "    image_size=img_size,  # depth, height, width\n",
    "    image_patch_size=(\n",
    "        patch_size,\n",
    "        patch_size,\n",
    "        patch_size,\n",
    "    ),  # depth, height, width patch size\n",
    "    frames=num_frames,\n",
    "    frame_patch_size=frame_patch_size,\n",
    "    depth=12,\n",
    "    heads=12,\n",
    "    dim=512,\n",
    "    mlp_dim=512,  # TODO: right now dim needs to equal mlp_dim, and both need to be 512\n",
    "    num_encoder_patches=num_encoder_patches,\n",
    "    num_decoder_patches=num_decoder_patches,\n",
    "    channels=1,\n",
    "    use_rope_emb=False,\n",
    "    use_cls_token=use_cls_token,\n",
    ")\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works without error\n",
    "model = model.to(device)\n",
    "encoder_mask = torch.zeros(num_patches).to(device).to(torch.bool)\n",
    "encoder_mask[:num_encoder_patches] = True\n",
    "decoder_mask = torch.zeros(num_patches).to(device).to(torch.bool)\n",
    "decoder_mask[-num_decoder_patches:] = True\n",
    "with torch.no_grad():\n",
    "    print(\"\\nencoder\")\n",
    "    encoder_out = model(\n",
    "        torch.randn(6, 1, 4, 64, 64, 48).to(device),\n",
    "        encoder_mask=encoder_mask,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(\"\\ndecoder\")\n",
    "    decoder_out = model(\n",
    "        encoder_out, encoder_mask=encoder_mask, decoder_mask=decoder_mask, verbose=True\n",
    "    )\n",
    "    if use_cls_token:\n",
    "        enc_cls_token = encoder_out[:, :1, :]\n",
    "        encoder_patches = encoder_out[:, 1:, :]\n",
    "        dec_cls_token = decoder_out[:, :1, :]\n",
    "        decoder_patches = decoder_out[:, 1:, :]\n",
    "        print(\"enc_cls_token\", enc_cls_token.shape)\n",
    "        print(\"encoder_patches\", encoder_patches.shape)\n",
    "        print(\"dec_cls_token\", dec_cls_token.shape)\n",
    "        print(\"decoder_patches\", decoder_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd0398-0eb8-464e-8bb8-13e7ce1a8480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_split_by_node(urls):\n",
    "    return urls\n",
    "\n",
    "\n",
    "aug_transform = utils.DataPrepper(\n",
    "    masking_strategy=\"conservative\",\n",
    "    patch_depth=patch_size,\n",
    "    patch_height=patch_size,\n",
    "    patch_width=patch_size,\n",
    "    frame_patch_size=frame_patch_size,\n",
    ")\n",
    "\n",
    "if train_urls[:2] == \"s3\":\n",
    "    train_urls = f\"pipe:aws s3 cp {train_urls} -\"\n",
    "print(train_urls)\n",
    "train_data = (\n",
    "    wds.WebDataset(train_urls, resampled=True, nodesplitter=my_split_by_node)\n",
    "    .shuffle(100, initial=100, rng=random.Random(seed))\n",
    "    .rename(\n",
    "        key=\"__key__\",\n",
    "        func=\"func.png\",\n",
    "        header=\"header.npy\",\n",
    "        dataset=\"dataset.txt\",\n",
    "        minmax=\"minmax.npy\",\n",
    "        meansd=\"meansd.png\",\n",
    "    )\n",
    "    .map_dict(\n",
    "        func=utils.grayscale_decoder,\n",
    "        meansd=utils.grayscale_decoder,\n",
    "        minmax=utils.numpy_decoder,\n",
    "    )\n",
    "    .to_tuple(*(\"func\", \"minmax\", \"meansd\"))\n",
    "    .map(aug_transform)\n",
    "    .with_epoch(num_samples_per_epoch)\n",
    ")\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True\n",
    ")\n",
    "\n",
    "if test_urls[:2] == \"s3\":\n",
    "    test_urls = f\"pipe:aws s3 cp {test_urls} -\"\n",
    "print(test_urls)\n",
    "test_data = (\n",
    "    wds.WebDataset(test_urls, resampled=False, nodesplitter=my_split_by_node)\n",
    "    .rename(\n",
    "        key=\"__key__\",\n",
    "        func=\"func.png\",\n",
    "        header=\"header.npy\",\n",
    "        dataset=\"dataset.txt\",\n",
    "        minmax=\"minmax.npy\",\n",
    "        meansd=\"meansd.png\",\n",
    "    )\n",
    "    .map_dict(\n",
    "        func=utils.grayscale_decoder,\n",
    "        meansd=utils.grayscale_decoder,\n",
    "        minmax=utils.numpy_decoder,\n",
    "    )\n",
    "    .to_tuple(*(\"func\", \"minmax\", \"meansd\"))\n",
    "    .map(aug_transform)\n",
    "    .with_epoch(num_samples_per_epoch)\n",
    ")\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c964eb59-ce81-4c4a-a038-77233863cc10",
   "metadata": {},
   "source": [
    "## test that data loaders work and calculate number of iterations per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bcf15b-f378-4b37-a828-534d2daef39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_samp = 0\n",
    "for train_i, out in enumerate(train_dl):\n",
    "    train_samp += len(out[0])\n",
    "num_iterations_per_epoch = train_i\n",
    "print(\"num_iterations_per_epoch\", num_iterations_per_epoch, \"\\n\")\n",
    "\n",
    "func, meansd, brain_pos_pats = out\n",
    "print(func.shape, meansd.shape, brain_pos_pats.shape)\n",
    "\n",
    "# test_samp = 0\n",
    "for test_i, out in enumerate(test_dl):\n",
    "    if test_i > 5:\n",
    "        break\n",
    "    # test_samp += len(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c6fba-5494-4964-b03d-e01c3afe47db",
   "metadata": {},
   "source": [
    "# Playing with the data, visualization of patching + masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd7913-707a-44ba-b031-afc883fb357c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract func volumes and their reference mean and standard deviation volumes\n",
    "func, meansd, brain_pos_pats = out\n",
    "func = func.unsqueeze(1)  # add empty first dimension to serve as 1d channel dimension\n",
    "\n",
    "# patchify func samples\n",
    "print(\"func\", func.shape)\n",
    "patches = model.patchify(func)\n",
    "print(\"patches\", patches.shape)\n",
    "\n",
    "# compress into ViT compatible inputs (bs x seq_len x emb_size)\n",
    "patches_vit = rearrange(patches, \"b ... d -> b (...) d\")\n",
    "print(\"patches_vit\", patches_vit.shape)\n",
    "print(\"num patches in one timepoint\", patches_vit.shape[1] // num_frames)\n",
    "\n",
    "tube_mask = torch.zeros(num_patches // num_frames).to(\n",
    "    torch.bool\n",
    ")  # start by masking everything (aka include nothing)\n",
    "batch_positive_approx = (\n",
    "    brain_pos_pats[:, : num_patches // num_frames].float().mean(dim=0) > 0\n",
    ")  # approximate brain positive patches for the whole batch\n",
    "mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "print(\n",
    "    \"Percentage of brain positive patches\",\n",
    "    len(mask_idx_candidates) / len(batch_positive_approx),\n",
    ")\n",
    "tube_idx = mask_idx_candidates[: int(num_patches / num_frames * (1 - tube_mask_ratio))]\n",
    "print(\"num tube patches =\", len(tube_idx))\n",
    "tube_mask[\n",
    "    tube_idx\n",
    "] = True  # Trues mean to include the patch, False means to remove the patch\n",
    "tube_mask = tube_mask.tile(num_frames)  # repeat masking for the other timepoints\n",
    "print(\"tube mask percent\", tube_mask.sum().item() / len(tube_mask))\n",
    "\n",
    "\n",
    "# create decoder mask similar to tube mask, but ensure no overlap\n",
    "decoder_mask = torch.zeros(num_patches // num_frames).to(\n",
    "    torch.bool\n",
    ")  # start by masking everything (aka include nothing)\n",
    "remaining_mask_idx = mask_idx_candidates[\n",
    "    int(num_patches / num_frames * (1 - tube_mask_ratio)) :\n",
    "]  # brain positive tokens not selected for the encoder tokens\n",
    "decoder_mask_idx = remaining_mask_idx[\n",
    "    : int(num_patches / num_frames * (1 - decoder_mask_ratio))\n",
    "]\n",
    "print(\"num decoder patches =\", len(decoder_mask_idx))\n",
    "decoder_mask[decoder_mask_idx] = True\n",
    "decoder_mask = decoder_mask.tile(num_frames)  # repeat masking for the other timepoints\n",
    "print(\"decoder_mask percent\", decoder_mask.sum() / len(decoder_mask))\n",
    "\n",
    "# apply masks to patches_vit\n",
    "tube_patches_vit = copy.deepcopy(patches_vit)\n",
    "decoder_patches_vit = copy.deepcopy(patches_vit)\n",
    "tube_patches_vit[:, ~tube_mask] = 0.0\n",
    "decoder_patches_vit[:, ~decoder_mask] = 0.0\n",
    "\n",
    "# undo patchification so we can visualize\n",
    "tube_unpatches = rearrange(\n",
    "    tube_patches_vit,\n",
    "    \"b (f d h w) c -> b f d h w c\",\n",
    "    f=num_frames,\n",
    "    d=patch_size,\n",
    "    h=patch_size,\n",
    ")\n",
    "decoder_unpatches = rearrange(\n",
    "    decoder_patches_vit,\n",
    "    \"b (f d h w) c -> b f d h w c\",\n",
    "    f=num_frames,\n",
    "    d=patch_size,\n",
    "    h=patch_size,\n",
    ")\n",
    "print(\"tube_unpatches\", tube_unpatches.shape)\n",
    "print(\"decoder_unpatches\", decoder_unpatches.shape)\n",
    "tube_func = rearrange(\n",
    "    tube_unpatches,\n",
    "    \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "    b=len(func),\n",
    "    f=num_frames,\n",
    "    d=8,\n",
    "    h=8,\n",
    "    w=6,\n",
    "    pd=patch_size,\n",
    "    ph=patch_size,\n",
    "    pw=patch_size,\n",
    "    pf=frame_patch_size,\n",
    ")\n",
    "decoder_func = rearrange(\n",
    "    decoder_unpatches,\n",
    "    \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "    b=len(func),\n",
    "    f=num_frames,\n",
    "    d=8,\n",
    "    h=8,\n",
    "    w=6,\n",
    "    pd=patch_size,\n",
    "    ph=patch_size,\n",
    "    pw=patch_size,\n",
    "    pf=frame_patch_size,\n",
    ")\n",
    "print(\"tube_func\", tube_func.shape)\n",
    "print(\"decoder_func\", decoder_func.shape)\n",
    "\n",
    "idx = 0\n",
    "mean, sd = meansd[idx]\n",
    "brain_pos_pat = brain_pos_pats[idx]\n",
    "\n",
    "print(\"original func without adding mean/sd references\")\n",
    "display(transforms.ToPILImage()(utils.reshape_to_2d(func[idx])))\n",
    "print(\"original func\")\n",
    "display(transforms.ToPILImage()(utils.reshape_to_2d(func[idx] * mean + sd)))\n",
    "print(\n",
    "    f\"Brain positive patches: {brain_pos_pat.count_nonzero()*100/len(brain_pos_pat)}% of the patches are remaining\"\n",
    ")\n",
    "expanded_mask = np.repeat(\n",
    "    np.repeat(\n",
    "        np.repeat(\n",
    "            brain_pos_pat.view(\n",
    "                [\n",
    "                    num_frames // frame_patch_size,\n",
    "                    img_size[0] // patch_size,\n",
    "                    img_size[1] // patch_size,\n",
    "                    img_size[2] // patch_size,\n",
    "                ]\n",
    "            ),\n",
    "            patch_size,\n",
    "            axis=1,\n",
    "        ),\n",
    "        patch_size,\n",
    "        axis=2,\n",
    "    ),\n",
    "    patch_size,\n",
    "    axis=3,\n",
    ")\n",
    "display(transforms.ToPILImage()(utils.reshape_to_2d(expanded_mask).float()))\n",
    "\n",
    "print(\"\\ntube func without adding mean/sd references\")\n",
    "display(transforms.ToPILImage()(utils.reshape_to_2d(tube_func[idx])))\n",
    "print(\"tube func\")\n",
    "display(transforms.ToPILImage()(utils.reshape_to_2d(tube_func[idx] * mean + sd)))\n",
    "\n",
    "print(\"\\ndecoder func without adding mean/sd references\")\n",
    "display(transforms.ToPILImage()(utils.reshape_to_2d(decoder_func[idx])))\n",
    "print(\"decoder func\")\n",
    "display(transforms.ToPILImage()(utils.reshape_to_2d(decoder_func[idx] * mean + sd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604b380-0126-49b2-ab4e-59b1e463d6ad",
   "metadata": {},
   "source": [
    "# Set up optimizer and begin model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da73c08-ca61-48ef-9e63-b70db6f07a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "opt_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 1e-2,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "total_steps = num_epochs * num_iterations_per_epoch\n",
    "print(\"total_steps\", total_steps)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    "    final_div_factor=1000,\n",
    "    last_epoch=-1,\n",
    "    pct_start=2 / num_epochs,\n",
    ")\n",
    "\n",
    "print(\"\\nDone with model preparations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e394dd-2745-41fa-a5aa-54b7b1373a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()\n",
    "model, optimizer, train_dl, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dl, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e258c4-8477-48ed-81af-d415cee246f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "if use_contrastive_loss:\n",
    "    logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))  # learned logit scale\n",
    "lrs, recon_losses, contrastive_losses, test_losses = [], [], [], []\n",
    "recon_image_list = []\n",
    "progress_bar = tqdm(range(epoch, num_epochs), ncols=1200, disable=(local_rank != 0))\n",
    "for epoch in progress_bar:\n",
    "    with torch.cuda.amp.autocast(dtype=data_type):\n",
    "        model.train()\n",
    "        for train_i, batch in enumerate(\n",
    "            train_dl\n",
    "        ):  # total samples in 1 epoch = train_dl.nsamples\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            func, meansd, brain_pos_pats = batch\n",
    "            if use_contrastive_loss:  # create positive pairs by duplicating the batch\n",
    "                func = torch.cat([func, func], dim=0)\n",
    "                meansd = torch.cat([meansd, meansd], dim=0)\n",
    "                brain_pos_pats = torch.cat([brain_pos_pats, brain_pos_pats], dim=0)\n",
    "\n",
    "            func = func.unsqueeze(1).to(device)\n",
    "\n",
    "            # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "            tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            batch_positive_approx = (\n",
    "                brain_pos_pats[:, : num_patches // num_frames].float().mean(dim=0) > 0\n",
    "            )\n",
    "            mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "            mask_idx_candidates = mask_idx_candidates[\n",
    "                torch.randperm(len(mask_idx_candidates))\n",
    "            ]\n",
    "            tube_idx = mask_idx_candidates[\n",
    "                : int(num_patches / num_frames * (1 - tube_mask_ratio))\n",
    "            ]\n",
    "            tube_mask[tube_idx] = True\n",
    "            tube_mask = tube_mask.tile(num_frames)\n",
    "\n",
    "            # create decoder mask\n",
    "            decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            remaining_mask_idx = mask_idx_candidates[\n",
    "                int(num_patches / num_frames * (1 - tube_mask_ratio)) :\n",
    "            ]\n",
    "            decoder_mask_idx = remaining_mask_idx[\n",
    "                : int(num_patches / num_frames * (1 - decoder_mask_ratio))\n",
    "            ]\n",
    "            decoder_mask[decoder_mask_idx] = True\n",
    "            decoder_mask = decoder_mask.tile(num_frames)\n",
    "\n",
    "            # encode the tube patches\n",
    "            encoder_out = model(func, encoder_mask=tube_mask)\n",
    "            if use_cls_token:\n",
    "                enc_cls_token = encoder_out[:, :1, :]\n",
    "\n",
    "            # decode both the encoder_out patches and masked decoder patches\n",
    "            decoder_out = model(\n",
    "                encoder_out, encoder_mask=tube_mask, decoder_mask=decoder_mask\n",
    "            )\n",
    "            # subset only the reconstructed decoder patches\n",
    "            output = decoder_out[:, -num_decoder_patches:]\n",
    "\n",
    "            # compare to ground truth and calculate loss\n",
    "            target_patches = model.patchify(func)\n",
    "            target_patches_vit = rearrange(target_patches, \"b ... d -> b (...) d\")\n",
    "            target = target_patches_vit[:, decoder_mask]\n",
    "            loss = mse(output, target)\n",
    "\n",
    "            # contrastive loss\n",
    "            if use_contrastive_loss:\n",
    "                n_b = len(func) // 2\n",
    "                cls_token1 = enc_cls_token[\n",
    "                    :n_b, 0, :\n",
    "                ]  # first half of batch, cls_token shape B, 1, d_model\n",
    "                cls_token2 = enc_cls_token[n_b:, 0, :]\n",
    "                contrastive_loss = utils.contrastive_loss(\n",
    "                    cls_token1, cls_token2, temperature=logit_scale\n",
    "                )\n",
    "                loss += constrastive_loss_weight * contrastive_loss\n",
    "                contrastive_losses.append(contrastive_loss.item())\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            recon_losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        model.eval()\n",
    "        for test_i, batch in enumerate(test_dl):\n",
    "            func, meansd, brain_pos_pats = batch\n",
    "            func = func.unsqueeze(1).to(device)\n",
    "\n",
    "            # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "            tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            batch_positive_approx = (\n",
    "                brain_pos_pats[:, : num_patches // num_frames].float().mean(dim=0) > 0\n",
    "            )\n",
    "            mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "            mask_idx_candidates = mask_idx_candidates[\n",
    "                torch.randperm(len(mask_idx_candidates))\n",
    "            ]\n",
    "            tube_idx = mask_idx_candidates[\n",
    "                : int(num_patches / num_frames * (1 - tube_mask_ratio))\n",
    "            ]\n",
    "            tube_mask[tube_idx] = True\n",
    "            tube_mask = tube_mask.tile(num_frames)\n",
    "\n",
    "            # create decoder mask\n",
    "            decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            remaining_mask_idx = mask_idx_candidates[\n",
    "                int(num_patches / num_frames * (1 - tube_mask_ratio)) :\n",
    "            ]\n",
    "            decoder_mask_idx = remaining_mask_idx[\n",
    "                : int(num_patches / num_frames * (1 - decoder_mask_ratio))\n",
    "            ]\n",
    "            decoder_mask[decoder_mask_idx] = True\n",
    "            decoder_mask = decoder_mask.tile(num_frames)\n",
    "\n",
    "            # encode the tube patches\n",
    "            encoder_out = model(func, encoder_mask=tube_mask)\n",
    "            # decode both the encoder_out patches and masked decoder patches\n",
    "            decoder_out = model(\n",
    "                encoder_out, encoder_mask=tube_mask, decoder_mask=decoder_mask\n",
    "            )\n",
    "            # subset only the reconstructed decoder patches\n",
    "            output = decoder_out[:, -num_decoder_patches:]\n",
    "\n",
    "            # # compare to ground truth and calculate loss\n",
    "            # target_patches = model.patchify(func)\n",
    "            # target_patches_vit = rearrange(target_patches, 'b ... d -> b (...) d')\n",
    "            # target = target_patches_vit[:,decoder_mask]\n",
    "            # loss = mse(output, target)\n",
    "\n",
    "            # compare to ground truth and calculate loss\n",
    "            target_patches = model.patchify(func)\n",
    "            target_patches_vit = rearrange(target_patches, \"b ... d -> b (...) d\")\n",
    "            target = target_patches_vit[:, decoder_mask]\n",
    "            loss = mse(output, target)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(recon_losses[-(train_i + 1) :]),\n",
    "            \"test/loss\": np.mean(test_losses[-(test_i + 1) :]),\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "        # Plot progress (first sample in batch)\n",
    "        with torch.no_grad():\n",
    "            # prep reference volumes for going back to original data\n",
    "            idx = 0\n",
    "            mean, sd = meansd[idx]\n",
    "            mean, sd = mean.to(device), sd.to(device)\n",
    "            if epoch == 0:\n",
    "                print(\"original volumes without adding mean/sd references\")\n",
    "                display(\n",
    "                    transforms.ToPILImage()(utils.reshape_to_2d(func[idx]) * 5)\n",
    "                )  # scaling by 5 for visualization contrast\n",
    "                print(\"original volumes\")\n",
    "                display(\n",
    "                    transforms.ToPILImage()(utils.reshape_to_2d(func[idx] * mean + sd))\n",
    "                )\n",
    "            if epoch % 5 == 0:\n",
    "                # undo patchification so we can visualize\n",
    "                decode_vis = torch.zeros_like(target_patches_vit)\n",
    "                decode_vis[:, decoder_mask] = output\n",
    "                decoder_unpatches = rearrange(\n",
    "                    decode_vis,\n",
    "                    \"b (f d h w) c -> b f d h w c\",\n",
    "                    f=num_frames,\n",
    "                    d=patch_size,\n",
    "                    h=patch_size,\n",
    "                )\n",
    "                decoder_func = rearrange(\n",
    "                    decoder_unpatches,\n",
    "                    \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "                    b=batch_size,\n",
    "                    f=num_frames,\n",
    "                    d=8,\n",
    "                    h=8,\n",
    "                    w=6,\n",
    "                    pd=patch_size,\n",
    "                    ph=patch_size,\n",
    "                    pw=patch_size,\n",
    "                    pf=frame_patch_size,\n",
    "                )\n",
    "                print(\"recons of decoded patches without adding mean/sd references\")\n",
    "                display(\n",
    "                    transforms.ToPILImage()(utils.reshape_to_2d(decoder_func[idx] * 5))\n",
    "                )  # scaling by 5 for visualization contrast\n",
    "                print(\"recons of decoded patches\")\n",
    "                display(\n",
    "                    transforms.ToPILImage()(\n",
    "                        utils.reshape_to_2d(decoder_func[idx] * mean + sd)\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6e847-2511-4f69-aa70-f1471a5b7d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(recon_losses)\n",
    "plt.title(\"Training re-construction losses\")\n",
    "plt.show()\n",
    "if use_contrastive_loss:\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(contrastive_losses)\n",
    "    plt.title(\"Training contrastive losses\")\n",
    "    plt.show()\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(test_losses)\n",
    "plt.title(\"Test losses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9bdba5-03fe-4c5e-9dde-cdc4152e42b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model ckpt\n",
    "torch.save({\"model_state_dict\": model.state_dict()}, \"last.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
