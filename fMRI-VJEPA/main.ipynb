{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7a60b5f-c5db-46f5-8569-e10ac40fb8a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "NUM GPUS  1\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from models import *\n",
    "from accelerate import Accelerator, load_checkpoint_in_model\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(\"NUM GPUS \", num_devices)\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3de5a1c-ee37-4e61-9ce6-6e2fae1dc7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "\n",
    "# First use \"accelerate config\" in terminal for setup\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "batch_size = global_batch_size // num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81faa0e0-06a1-4b32-a3f0-eefa65ee8f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 1557848\n",
      "device: cuda\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bac630aa-3df9-4ccb-9d93-f638a4fe9c76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'patch8_100eps_4gpu_accelerate_b', 'use_cls_token': False, 'use_contrastive_loss': False, 'constrastive_loss_weight': 1.0, 'global_batch_size': 8, 'num_workers': 4, 'num_epochs': 100, 'seed': 42, 'max_lr': 3e-05, 'num_samples_per_epoch': 1024, 'cache_dir': 'cache/', 'ema': [0.998, 1.0], 'ipe_scale': 1.25, 'ckpt_saving': True, 'ckpt_interval': 50, 'resume_from_ckpt': False, 'wandb_log': True, 'tube_start_masking_ratio': 0.75, 'tube_end_masking_ratio': 0.75, 'decoder_mask_ratio': 0.75, 'depth': 12, 'heads': 12, 'dim': 512, 'mlp_dim': 512, 'patch_size': 8, 'frame_patch_size': 1, 'use_rope_emb': False, 'img_size': [64, 64, 48], 'num_frames': 4, 'train_urls': 's3://proj-fmri/fmri_foundation_datasets/openneuro/{000005..000664}.tar', 'test_urls': 's3://proj-fmri/fmri_foundation_datasets/openneuro/{000000..000004}.tar'}\n",
      "outdir /weka/proj-fmri/ks9249/fMRI-foundation-model/ckpts/patch8_100eps_4gpu_accelerate_b\n",
      "cache_dir cache//7270\n",
      "global_batch_size 8\n",
      "use_cls_token False\n",
      "num_patches 1536\n",
      "num_encoder_patches 384\n",
      "num_decoder_patches 384\n"
     ]
    }
   ],
   "source": [
    "print(config)\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "cache_dir = cache_dir + f'/{np.random.randint(9999)}' # create random subfolder so multiple runs arent using same directory\n",
    "os.makedirs(cache_dir,exist_ok=True)\n",
    "print(\"cache_dir\", cache_dir)\n",
    "\n",
    "if use_contrastive_loss:\n",
    "    global_batch_size = global_batch_size // 2 # contrastive loss doubles the batch size with the same samples and different masks\n",
    "print(\"global_batch_size\", global_batch_size)\n",
    "\n",
    "use_cls_token = True if use_contrastive_loss else use_cls_token\n",
    "print(\"use_cls_token\", use_cls_token)\n",
    "\n",
    "num_patches = int(\n",
    "    (img_size[0] / patch_size)\n",
    "    * (img_size[1] / patch_size)\n",
    "    * (img_size[2] / patch_size)\n",
    "    * num_frames\n",
    ")\n",
    "num_patches_per_timepoint = num_patches // num_frames\n",
    "num_encoder_patches = int(num_patches_per_timepoint * (1 - tube_start_masking_ratio) * num_frames)\n",
    "num_decoder_patches = int(num_patches_per_timepoint * (1 - decoder_mask_ratio) * num_frames)\n",
    "print(\"num_patches\", num_patches)\n",
    "print(\"num_encoder_patches\", num_encoder_patches)\n",
    "print(\"num_decoder_patches\", num_decoder_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b543f5a-65a9-4294-b708-b5ec67dd0fff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "76,138,496 total\n",
      "76,138,496 trainable\n",
      "tensor([ True,  True,  True,  ..., False, False, False], device='cuda:0')\n",
      "tensor([False, False, False,  ...,  True,  True,  True], device='cuda:0')\n",
      "\n",
      "xencoder\n",
      "input shape torch.Size([6, 1, 4, 64, 64, 48])\n",
      "after patching torch.Size([6, 4, 8, 8, 6, 512])\n",
      "convert to embedding torch.Size([6, 4, 8, 8, 6, 512])\n",
      "flattening torch.Size([6, 1536, 512])\n",
      "positional embedding torch.Size([1536, 512])\n",
      "current shape torch.Size([6, 1536, 512])\n",
      "after masking torch.Size([6, 384, 512])\n",
      "final shape torch.Size([6, 384, 512])\n",
      "\n",
      "predictor\n",
      "input shape torch.Size([6, 384, 512])\n",
      "positional embedding torch.Size([1536, 512])\n",
      "masked tokens torch.Size([1152, 512])\n",
      "concatenation torch.Size([6, 1536, 512])\n",
      "after transformer torch.Size([6, 1152, 512])\n",
      "\n",
      "yencoder\n",
      "input shape torch.Size([6, 1, 4, 64, 64, 48])\n",
      "after patching torch.Size([6, 4, 8, 8, 6, 512])\n",
      "convert to embedding torch.Size([6, 4, 8, 8, 6, 512])\n",
      "flattening torch.Size([6, 1536, 512])\n",
      "positional embedding torch.Size([1536, 512])\n",
      "after encoder torch.Size([6, 1536, 512])\n",
      "keeping masked tokens torch.Size([6, 1152, 512])\n"
     ]
    }
   ],
   "source": [
    "model = SimpleViT(\n",
    "    image_size=img_size,  # depth, height, width\n",
    "    image_patch_size=(patch_size,patch_size,patch_size),  # depth, height, width patch size\n",
    "    frames=num_frames,\n",
    "    frame_patch_size=frame_patch_size,\n",
    "    depth=depth,\n",
    "    heads=heads,\n",
    "    dim=dim,\n",
    "    mlp_dim=mlp_dim,  # TODO: right now dim needs to equal mlp_dim, and both need to be 512\n",
    "    channels=1,\n",
    "    use_rope_emb=use_rope_emb,\n",
    "    use_cls_token=use_cls_token,\n",
    ")\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works without error\n",
    "model = model.to(device)\n",
    "inputdata = torch.randn(6, 1, 4, 64, 64, 48).to(device)\n",
    "encoder_mask = torch.zeros(num_patches).to(device).to(torch.bool)\n",
    "encoder_mask[:num_encoder_patches] = True\n",
    "maskedtokens = ~encoder_mask\n",
    "print(encoder_mask)\n",
    "print(maskedtokens)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"\\nxencoder\")\n",
    "    xencoder_out = model(\n",
    "                inputdata,\n",
    "                encoder_mask=encoder_mask,\n",
    "                encoder_type = \"x\",\n",
    "                verbose=True)\n",
    "    print(\"\\npredictor\")\n",
    "    predictor_out = model(\n",
    "                xencoder_out, \n",
    "                encoder_mask=encoder_mask, \n",
    "                encoder_type = \"p\",\n",
    "                verbose=True)\n",
    "    print(\"\\nyencoder\")\n",
    "    yencoderout = model(\n",
    "                inputdata, \n",
    "                encoder_mask=encoder_mask, \n",
    "                encoder_type = \"y\",\n",
    "                verbose=True)\n",
    "    if use_cls_token:\n",
    "        enc_cls_token = xencoder_out[:, :1, :]\n",
    "        encoder_patches = xencoder_out[:, 1:, :]\n",
    "        pred_cls_token = predictor_out[:, :1, :]\n",
    "        predictor_patches = predictor_out[:, 1:, :]\n",
    "        print(\"\\nenc_cls_token\", enc_cls_token.shape)\n",
    "        print(\"encoder_patches\", encoder_patches.shape)\n",
    "        print(\"pred_cls_token\", pred_cls_token.shape)\n",
    "        print(\"predictor_patches\", predictor_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f8193e6-2400-48b4-ba8e-020907572025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipe:aws s3 cp s3://proj-fmri/fmri_foundation_datasets/openneuro/{000005..000664}.tar -\n",
      "pipe:aws s3 cp s3://proj-fmri/fmri_foundation_datasets/openneuro/{000000..000004}.tar -\n"
     ]
    }
   ],
   "source": [
    "aug_transform = utils.DataPrepper(\n",
    "    masking_strategy=\"conservative\",\n",
    "    patch_depth=patch_size,\n",
    "    patch_height=patch_size,\n",
    "    patch_width=patch_size,\n",
    "    frame_patch_size=frame_patch_size,\n",
    ")\n",
    "\n",
    "def log_and_continue(exn):\n",
    "    \"\"\"Call in an exception handler to ignore any exception, issue a warning, and continue.\"\"\"\n",
    "    print(f'Handling webdataset error ({repr(exn)}). Ignoring.')\n",
    "    return True\n",
    "\n",
    "def filter_corrupted_images(sample):\n",
    "    \"\"\"If all the required files are not present don't use them.\"\"\"\n",
    "    correct_data = (\"func.png\" in sample and \"dataset.txt\" in sample and \"header.npy\" in sample and \"meansd.png\" in sample and \"minmax.npy\" in sample)\n",
    "    return correct_data\n",
    "\n",
    "### ================      Train Dataset and DataLoader    ====================\n",
    "if train_urls[:2] == \"s3\":\n",
    "    train_urls = f\"pipe:aws s3 cp {train_urls} -\"\n",
    "print(train_urls)\n",
    "train_data = (\n",
    "    wds.WebDataset(train_urls, resampled=True, nodesplitter=my_split_by_node, cache_dir=cache_dir, handler=log_and_continue)\n",
    "    .shuffle(100, initial=100, rng=random.Random(seed))\n",
    "    .select(filter_corrupted_images)\n",
    "    .rename(key=\"__key__\",\n",
    "        func=\"func.png\",\n",
    "        header=\"header.npy\",\n",
    "        dataset=\"dataset.txt\",\n",
    "        minmax=\"minmax.npy\",\n",
    "        meansd=\"meansd.png\")\n",
    "    .map_dict(func=utils.grayscale_decoder,\n",
    "        meansd=utils.grayscale_decoder,\n",
    "        minmax=utils.numpy_decoder)\n",
    "    .to_tuple(*(\"func\", \"minmax\", \"meansd\"))\n",
    "    .map(aug_transform)\n",
    "    .with_epoch(num_samples_per_epoch)\n",
    ")\n",
    "train_dl = wds.WebLoader(\n",
    "    train_data.batched(batch_size), \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    batch_size=None,\n",
    "    num_workers=num_workers, \n",
    "    persistent_workers=num_workers>0,\n",
    ").with_epoch(num_samples_per_epoch//batch_size)\n",
    "\n",
    "### ================      Test Dataset and DataLoader    ====================\n",
    "if test_urls[:2] == \"s3\":\n",
    "    test_urls = f\"pipe:aws s3 cp {test_urls} -\"\n",
    "print(test_urls)\n",
    "test_data = (\n",
    "    wds.WebDataset(test_urls, resampled=False, nodesplitter=my_split_by_node, cache_dir=cache_dir, handler=log_and_continue)\n",
    "    .select(filter_corrupted_images)\n",
    "    .rename(key=\"__key__\",\n",
    "        func=\"func.png\",\n",
    "        header=\"header.npy\",\n",
    "        dataset=\"dataset.txt\",\n",
    "        minmax=\"minmax.npy\",\n",
    "        meansd=\"meansd.png\")\n",
    "    .map_dict(func=utils.grayscale_decoder,\n",
    "        meansd=utils.grayscale_decoder,\n",
    "        minmax=utils.numpy_decoder)\n",
    "    .to_tuple(*(\"func\", \"minmax\", \"meansd\"))\n",
    "    .map(aug_transform)\n",
    ")\n",
    "test_dl = wds.WebLoader(\n",
    "    test_data.batched(batch_size), \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    batch_size=None,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=num_workers>0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "091cfab6-7e7b-484c-aa8c-3a73bf933785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping deepspeed reconfiguration...\n"
     ]
    }
   ],
   "source": [
    "from accelerate.state import AcceleratorState\n",
    "try:\n",
    "    AcceleratorState().deepspeed_plugin.deepspeed_config['train_micro_batch_size_per_gpu'] = global_batch_size\n",
    "    print(\"deepspeed reconfigured, train_micro_batch_size_per_gpu = \", global_batch_size)\n",
    "except:\n",
    "    print(\"skipping deepspeed reconfiguration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2702a313-4959-49ec-b0c4-6cbdcac8fa03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iterations_per_epoch 128\n",
      "Train batch: torch.Size([8, 4, 64, 64, 48]) torch.Size([8, 2, 64, 64, 48]) torch.Size([8, 1536])\n",
      "test_num_iterations_per_epoch 199\n",
      "Test batch: torch.Size([8, 4, 64, 64, 48]) torch.Size([8, 2, 64, 64, 48]) torch.Size([8, 1536])\n"
     ]
    }
   ],
   "source": [
    "num_iterations_per_epoch = num_samples_per_epoch // batch_size\n",
    "print(f\"num_iterations_per_epoch {num_iterations_per_epoch}\")\n",
    "\n",
    "train_batch = next(iter(train_dl))\n",
    "func, meansd, brain_pos_pats = train_batch\n",
    "print(\"Train batch:\", func.shape, meansd.shape, brain_pos_pats.shape)\n",
    "\n",
    "for test_num_iterations_per_epoch, test_batch in enumerate(test_dl):\n",
    "    pass\n",
    "print(f\"test_num_iterations_per_epoch {test_num_iterations_per_epoch}\")\n",
    "func, meansd, brain_pos_pats = test_batch\n",
    "print(\"Test batch:\", func.shape, meansd.shape, brain_pos_pats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d66a5ce1-6dd1-42c9-82c8-d402e7317ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 12800\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "76,138,496 total\n",
      "76,138,496 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.xencoder_transformer.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.predictor_transformer.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "] #we only update the xencoder and predictor, the yencoder is updated through a moving average of the xencoder\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "total_steps = num_epochs * num_iterations_per_epoch * num_devices\n",
    "print(\"total_steps\", total_steps)\n",
    "\n",
    "momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(num_iterations_per_epoch*num_epochs*ipe_scale)\n",
    "                          for i in range(int(num_iterations_per_epoch*num_epochs*ipe_scale)+1))\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    ")\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "233c03de-3475-4ffa-872b-ca6d9c98103d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_ckpt(tag=\"last\"):\n",
    "    ckpt_path = outdir+f'/{tag}'\n",
    "    os.makedirs(ckpt_path,exist_ok=True)\n",
    "    accelerator.save_model(model, ckpt_path, max_shard_size=\"2GB\", safe_serialization=True)\n",
    "    print(f\"\\n---saved {ckpt_path}!---\\n\")\n",
    "        \n",
    "def save_progress(tag=\"last\"):\n",
    "    if accelerator.is_main_process:\n",
    "        ckpt_path = outdir+f'/{tag}'\n",
    "        torch.save(\n",
    "                {\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"scheduler\": lr_scheduler.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"recon_losses\": recon_losses,\n",
    "                    \"contrastive_losses\": contrastive_losses,\n",
    "                    \"test_losses\": test_losses,\n",
    "                    \"lrs\": lrs,\n",
    "                },\n",
    "                os.path.join(ckpt_path, f\"params.pt\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a4cfd-2f67-417a-9251-eac4baa2aee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if accelerator.is_main_process and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"depth\": depth,\n",
    "      \"heads\": heads,\n",
    "      \"dim\": dim,\n",
    "      \"mlp_dim\": mlp_dim,\n",
    "      \"tube_start_masking_ratio\": tube_start_masking_ratio,\n",
    "      \"tube_end_masking_ratio\": tube_end_masking_ratio,\n",
    "      \"decoder_mask_ratio\": decoder_mask_ratio,\n",
    "      \"num_frames\": num_frames,\n",
    "      \"patch_size\": patch_size,\n",
    "      \"frame_patch_size\": frame_patch_size,\n",
    "      \"use_contrastive_loss\": use_contrastive_loss,\n",
    "      \"use_cls_token\": use_cls_token,\n",
    "      \"constrastive_loss_weight\": constrastive_loss_weight,\n",
    "      \"num_params\": num_params,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_urls\": train_urls,\n",
    "      \"test_urls\": test_urls,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5102a6b4-8587-4121-b422-a8232ea9898c",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7ed4a-2abf-42af-b1ca-798c703d00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "lrs, recon_losses, contrastive_losses, test_losses = [], [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c2ea0-dff7-49cc-8216-a869299db03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume from ckpt (e.g., if you are resuming from a run that got pre-empted)\n",
    "load_progress = False\n",
    "if wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "        load_progress = True\n",
    "elif resume_from_ckpt: # if resuming without using wandb\n",
    "    load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "    load_progress = True\n",
    "    \n",
    "if load_progress:\n",
    "    ckpt_path = outdir+'/last'\n",
    "    prev_params = torch.load(ckpt_path+\"/params.pt\")\n",
    "    optimizer.load_state_dict(prev_params[\"optimizer\"])\n",
    "    lr_scheduler.load_state_dict(prev_params[\"scheduler\"])\n",
    "    epoch = prev_params[\"epoch\"]\n",
    "    recon_losses = prev_params[\"recon_losses\"]\n",
    "    contrastive_losses = prev_params[\"contrastive_losses\"]\n",
    "    test_losses = prev_params[\"test_losses\"]\n",
    "    lrs = prev_params[\"lrs\"]\n",
    "    for _ in range(epoch * num_iterations_per_epoch):\n",
    "            next(momentum_scheduler)\n",
    "    print(\"Loaded model params from\", ckpt_path, \"at epoch\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07810209-197f-4561-abd3-d09c22113c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dl, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dl, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ec911-325d-4d3c-b5e3-6f855661638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = nn.L1Loss() #Following VJEPA architecture, which uses L1 loss not L2 loss\n",
    "\n",
    "progress_bar = tqdm(range(epoch, num_epochs), disable=not accelerator.is_main_process, desc=\"Overall\")\n",
    "for epoch in progress_bar:\n",
    "    # get the masking ratio for the current epoch\n",
    "    tube_mask_ratio = utils.get_masking_ratio(\n",
    "        current_epoch=epoch, \n",
    "        total_epochs=num_epochs, \n",
    "        start_masking_ratio=tube_start_masking_ratio, \n",
    "        end_masking_ratio=tube_end_masking_ratio\n",
    "    )\n",
    "    with torch.cuda.amp.autocast(dtype=data_type):\n",
    "        model.train()\n",
    "        for train_i, batch in enumerate(tqdm(train_dl, disable=not accelerator.is_main_process, \n",
    "                 total=num_iterations_per_epoch, leave=False, desc=\"Training\")):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            func, meansd, brain_pos_pats = batch\n",
    "\n",
    "            func = func.unsqueeze(1).to(device)\n",
    "\n",
    "            # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "            tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            batch_positive_approx = (brain_pos_pats[:, :num_patches // num_frames].float().mean(dim=0) > 0)\n",
    "            mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "            # check if there's not enough brain left for code to continue\n",
    "            if len(mask_idx_candidates) < (int(num_patches/num_frames*(1-tube_mask_ratio))+int(num_patches/num_frames*(1-decoder_mask_ratio))):\n",
    "                print(\"Brain volume skipped due to not enough brain-positive patches remaining...\")\n",
    "                continue\n",
    "            mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "            tube_idx = mask_idx_candidates[:int(num_patches / num_frames * (1 - tube_mask_ratio))]\n",
    "            tube_mask[tube_idx] = True\n",
    "            tube_mask = tube_mask.tile(num_frames)\n",
    "\n",
    "            # feed into x-encoder\n",
    "            xencoder_out = model(func, encoder_mask=tube_mask, encoder_type = \"x\")\n",
    "            \n",
    "            # feed output of x-encoder into predictor\n",
    "            predictor_out = model(xencoder_out, encoder_mask=tube_mask, encoder_type=\"p\")\n",
    "            \n",
    "            # feed entire func into y-encoder\n",
    "            yencoder_out = model(func, encoder_mask=tube_mask, encoder_type = \"y\")\n",
    "            \n",
    "            # compare output of predictor to output of y-encoder and calculate L1 Loss\n",
    "            loss = l1(predictor_out,yencoder_out)\n",
    "            \n",
    "            # backwards + step\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            recon_losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "            \n",
    "            # update y-encoder using exponential-moving average of x-encoder params to prevent collapse\n",
    "            m = next(momentum_scheduler)\n",
    "            with torch.no_grad():\n",
    "                for param_q, param_k in zip(model.xencoder_transformer.parameters(), model.yencoder_transformer.parameters()):\n",
    "                    param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
    "                    \n",
    "        model.eval()\n",
    "        for test_i, batch in enumerate(tqdm(test_dl, disable=not accelerator.is_main_process, total=test_num_iterations_per_epoch, leave=False, desc=\"Testing\")):\n",
    "            func, meansd, brain_pos_pats = batch\n",
    "            func = func.unsqueeze(1).to(device)\n",
    "\n",
    "            # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "            tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            batch_positive_approx = brain_pos_pats[:, :num_patches // num_frames].float().mean(dim=0) > 0\n",
    "            mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "            # check if there's not enough brain left for code to continue\n",
    "            if len(mask_idx_candidates) < (int(num_patches/num_frames*(1-tube_mask_ratio))+int(num_patches/num_frames*(1-decoder_mask_ratio))):\n",
    "                if test_i==0:\n",
    "                    print(\"Brain volume skipped due to not enough brain-positive patches remaining...\")\n",
    "                continue\n",
    "            mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "            tube_idx = mask_idx_candidates[:int(num_patches / num_frames * (1 - tube_mask_ratio))]\n",
    "            tube_mask[tube_idx] = True\n",
    "            tube_mask = tube_mask.tile(num_frames)\n",
    "            \n",
    "            # feed into x-encoder\n",
    "            xencoder_out = model(func, encoder_mask=tube_mask, encoder_type = \"x\")\n",
    "            \n",
    "            # feed output of x-encoder into predictor\n",
    "            predictor_out = model(xencoder_out, encoder_mask=tube_mask, encoder_type=\"p\")\n",
    "            \n",
    "            # feed entire func into y-encoder\n",
    "            yencoder_out = model(func, encoder_mask=tube_mask, encoder_type = \"y\")\n",
    "\n",
    "            # compare output of predictor to output of y-encoder and calculate L1 Loss\n",
    "            loss = l1(predictor_out,yencoder_out)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(recon_losses[-(train_i + 1) :]),\n",
    "            \"test/loss\": np.mean(test_losses[-(test_i + 1) :]),\n",
    "            \"train/num_steps\": len(recon_losses),\n",
    "            \"test/num_steps\": len(test_losses),\n",
    "            \"lr\": np.mean(lrs[-(train_i + 1) :]),\n",
    "            \"epoch\": epoch,\n",
    "            \"tube_mask_ratio\": tube_mask_ratio,\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        if wandb_log: wandb.log(logs)\n",
    "                \n",
    "        # Save model checkpoint\n",
    "        if (ckpt_saving) and ((epoch % ckpt_interval == 0) or (epoch==num_epochs-1)):\n",
    "            save_ckpt()\n",
    "            save_progress()\n",
    "            \n",
    "        # wait for other GPUs to catch up if needed\n",
    "        accelerator.wait_for_everyone()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "# remove cache directories\n",
    "os.system('rm -fr \"%s\"' % f\"{cache_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
