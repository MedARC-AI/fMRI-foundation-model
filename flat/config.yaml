model_name: "flat_nocont_novic_norm"  # "flat_cont_vic_1gamma_norm"
hcp_flat_path: "/weka/proj-medarc/shared/hcp_flat"

use_contrastive_loss: False
use_decoder: True
patch_size: 16

decoder_embed_dim: 512
num_frames: 16
mask_ratio: .75
pred_t_dim: 8
t_patch_size: 2
cls_embed: True
no_qkv_bias: False
sep_pos_embed: True
trunc_init: False
norm_pix_loss: True
contrastive_loss_weight: 1.0  # 2.0

# VICReg params
use_vic_loss: False
vic_loss_weight: 1.0  # 0.05  # 1.0
gamma: 1.0  # 0.5  # 1.0  # 0.5
# mu->v, lamda->i, nu->c
mu: 25
lamda: 25
nu: 1
# fraction of tokens to use for cov loss (all tokens don't fit in mem)
rand_frac: 0.1  # 0.2
# whether to apply vic to cls tok (if cont is also on, 2 losses will apply to cls token)
use_vic_cls: True
pct_masks_to_decode: 1

# Training Configs
batch_size: 32 
num_workers: 10
num_epochs: 100
seed: 42
base_lr: 3.0e-4 # Keep the x.0 else will be converted to string
num_samples_per_epoch: 48000  # 200000
test_num_samples_per_epoch: 320  # 50000
grad_clip: 1.0  # set 0 for no clip
grad_accumulation_steps: 1
test_set: False

# Downstream probe config
probe_num_samples_per_epoch: 800  # 100000
probe_num_epochs: 30
probe_batch_size: 8
probe_base_lr: 3.0e-4

# Saving progress
ckpt_saving: True
ckpt_interval: 5 # in epochs
print_interval: 20 # in steps
resume_from_ckpt: False
wandb_log: True
