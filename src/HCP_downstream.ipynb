{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir /weka/proj-fmri/ckadirt/fMRI-foundation-model/src/checkpoints/NSDflat_large_gsrFalse_40sess_9908\n",
      "Loaded config.yaml from ckpt folder /weka/proj-fmri/ckadirt/fMRI-foundation-model/src/checkpoints/NSDflat_large_gsrFalse_40sess_9908\n",
      "\n",
      "__CONFIG__\n",
      "base_lr = 0.001\n",
      "batch_size = 32\n",
      "ckpt_interval = 25\n",
      "ckpt_saving = True\n",
      "cls_embed = False\n",
      "cls_forward = False\n",
      "contrastive_loss_weight = 1.0\n",
      "datasets_to_include = NSD\n",
      "decoder_cls_embed = False\n",
      "decoder_embed_dim = 512\n",
      "global_pool = False\n",
      "grad_accumulation_steps = 1\n",
      "grad_clip = 1.0\n",
      "gsr = False\n",
      "hcp_flat_path = /weka/proj-medarc/shared/HCP-Flat\n",
      "mask_ratio = 0.75\n",
      "model_name = NSDflat_large_gsrFalse_40sess\n",
      "model_size = large\n",
      "no_qkv_bias = False\n",
      "norm_pix_loss = False\n",
      "nsd_flat_path = /weka/proj-medarc/shared/NSD-Flat\n",
      "num_epochs = 100\n",
      "num_frames = 16\n",
      "num_samples_per_epoch = 200000\n",
      "num_sessions = 40\n",
      "num_workers = 8\n",
      "patch_size = 16\n",
      "pct_masks_to_decode = 1\n",
      "plotting = True\n",
      "pred_t_dim = 8\n",
      "print_interval = 20\n",
      "probe_base_lr = 0.0003\n",
      "probe_batch_size = 8\n",
      "probe_num_epochs = 30\n",
      "probe_num_samples_per_epoch = 100000\n",
      "resume_from_ckpt = True\n",
      "seed = 42\n",
      "sep_pos_embed = True\n",
      "source_embed_mode = add\n",
      "source_embed_train_mode = ce\n",
      "t_patch_size = 2\n",
      "test_num_samples_per_epoch = 50000\n",
      "test_set = False\n",
      "trunc_init = False\n",
      "use_contrastive_loss = False\n",
      "use_decoder_contrastive_loss = False\n",
      "use_source_embeds = False\n",
      "wandb_log = True\n",
      "wandb_rand = 0\n",
      "\n",
      "\n",
      "WORLD_SIZE=1\n",
      "PID of this process = 1793946\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from mae_utils.flat_models import *\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# following fixes a Conv3D CUDNN_NOT_SUPPORTED error\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "## MODEL TO LOAD ##\n",
    "# YOU WILL NEED TO PRECOMPUTE AND SAVE THE FEATURES SEE: prep_HCP_downstream.ipynb\n",
    "model_name = \"NSDflat_large_gsrFalse_5sess_57734\"\n",
    "parquet_folder = \"epoch99\"\n",
    "\n",
    "## DEFINE TARGET AND GLOBAL POOLING ##\n",
    "global_pooling = True\n",
    "target = \"subject_id\" # \"trial_type\" or \"subject_id\" or see table below\n",
    "\n",
    "\n",
    "# outdir = os.path.abspath(f'checkpoints/{model_name}')\n",
    "outdir = os.path.abspath(f'checkpoints/{model_name}')\n",
    "\n",
    "print(\"outdir\", outdir)\n",
    "# Load previous config.yaml if available\n",
    "if os.path.exists(f\"{outdir}/config.yaml\"):\n",
    "    config = yaml.load(open(f\"{outdir}/config.yaml\", 'r'), Loader=yaml.FullLoader)\n",
    "    print(f\"Loaded config.yaml from ckpt folder {outdir}\")\n",
    "    # create global variables from the config\n",
    "    print(\"\\n__CONFIG__\")\n",
    "    for attribute_name in config.keys():\n",
    "        print(f\"{attribute_name} = {config[attribute_name]}\")\n",
    "        globals()[attribute_name] = config[f'{attribute_name}']\n",
    "    print(\"\\n\")\n",
    "\n",
    "world_size = os.getenv('WORLD_SIZE')\n",
    "if world_size is None: \n",
    "    world_size = 1\n",
    "else:\n",
    "    world_size = int(world_size)\n",
    "print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "batch_size = probe_batch_size\n",
    "num_epochs = probe_num_epochs\n",
    "\n",
    "data_type = torch.float32 # change depending on your mixed_precision\n",
    "global_batch_size = batch_size * world_size\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "\n",
    "utils.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c9bffa-f99d-4666-b011-66277bc6a915",
   "metadata": {},
   "source": [
    "# Columns to decode from subjects\n",
    "\n",
    "## 1. Basic Demographics\n",
    "| Variable  | Short Description | Type of Value |\n",
    "|-----------|-------------------|---------------|\n",
    "| Age_in_Yrs       | Age of the subject (in years) | Numerical |\n",
    "| Gender       | Biological sex of the subject (e.g., Male/Female/Other) | Categorical |\n",
    "| Race      | Self-reported racial category (e.g., White, Black, Asian, etc.). | Categorical |\n",
    "| Ethnicity | Self-reported ethnicity (e.g., Hispanic/Latino, Non-Hispanic, etc.). | Categorical |\n",
    "\n",
    "## 2. Cognitive / “IQ-like” Measures\n",
    "| Variable          | Short Description | Type of Value |\n",
    "|--------------------|-------------------|---------------|\n",
    "| PMAT24_A_CR       | Fluid intelligence / abstract reasoning. Assesses pattern recognition, logical reasoning, and problem-solving using 24 matrix-style puzzles. | Numerical (count score) |\n",
    "| CardSort_Unadj    | Executive function (cognitive flexibility). Assesses ability to switch attention between rules or dimensions (raw/unadjusted score). | Numerical |\n",
    "| CardSort_AgeAdj   | Same test as CardSort_Unadj but age-adjusted score. | Numerical (standardized) |\n",
    "| ListSort_Unadj    | Working memory. Involves remembering/sorting lists (e.g., animals, foods) in size order (raw/unadjusted score). | Numerical |\n",
    "| ListSort_AgeAdj   | Same test as ListSort_Unadj but age-adjusted score. | Numerical (standardized) |\n",
    "| PicSeq_Unadj      | Episodic memory. Assesses ability to recall sequences of illustrated objects/activities (raw/unadjusted score). | Numerical |\n",
    "| PicSeq_AgeAdj     | Same test as PicSeq_Unadj but age-adjusted score. | Numerical (standardized) |\n",
    "\n",
    "## 3. Personality Traits (Big Five)\n",
    "| Variable          | Short Description | Type of Value |\n",
    "|--------------------|-------------------|---------------|\n",
    "| NEOFAC_A          | Agreeableness (factor score from NEO inventory) | Numerical |\n",
    "| NEOFAC_O          | Openness to Experience (factor score) | Numerical |\n",
    "| NEOFAC_C          | Conscientiousness (factor score) | Numerical |\n",
    "| NEOFAC_N          | Neuroticism (factor score) | Numerical |\n",
    "| NEOFAC_E          | Extraversion (factor score) | Numerical |\n",
    "| NEORAW_01 to 60   | Raw item-level responses (60 items) from NEO-based inventory, each reflecting specific behavior/preference statements. | Numerical (item-level) |\n",
    "\n",
    "## 4. Psychopathology / Mental Health\n",
    "| Variable          | Short Description | Type of Value |\n",
    "|--------------------|-------------------|---------------|\n",
    "| ASR_Anxd_Raw      | Adult Self-Report anxiety problems (raw score). | Numerical (sum) |\n",
    "| ASR_Attn_Raw      | Adult Self-Report attention problems (raw score). | Numerical (sum) |\n",
    "| ASR_Aggr_Raw      | Adult Self-Report aggressive behavior (raw score). | Numerical (sum) |\n",
    "| DSM_Depr_Raw      | DSM-oriented depression subscale (raw score). | Numerical (sum) |\n",
    "| DSM_Anxi_Raw      | DSM-oriented anxiety subscale (raw score). | Numerical (sum) |\n",
    "| SSAGA_PanicDisorder | Semi-Structured Assessment for the Genetics of Alcoholism: Panic disorder diagnosis (yes/no). | Categorical (binary) |\n",
    "| SSAGA_Depressive_Ep | SSAGA depression episode diagnosis (yes/no). | Categorical (binary) |\n",
    "| SSAGA_Depressive_Sx | SSAGA depression symptom count or severity. | Numerical (count/scale) |\n",
    "\n",
    "## 5. Substance Use Phenotypes\n",
    "| Variable          | Short Description | Type of Value |\n",
    "|--------------------|-------------------|---------------|\n",
    "| SSAGA_Alc_12_Frq  | Frequency of alcohol use in last 12 months (from SSAGA). | Numerical (count/frequency) |\n",
    "| SSAGA_Alc_12_Max_Drinks | Max number of drinks in one sitting, last 12 months. | Numerical (count) |\n",
    "| SSAGA_Times_Used_Illicits | Times used illicit substances (lifetime or specific period). | Numerical (count) |\n",
    "| SSAGA_Times_Used_Cocaine | Times used cocaine. | Numerical (count) |\n",
    "| Total_Drinks_7days | Total number of drinks in the past 7 days (short-term measure). | Numerical (count) |\n",
    "| Total_Any_Tobacco_7days | Total tobacco use in the past 7 days (cigarettes, e-cig, etc.). | Numerical (count) |\n",
    "\n",
    "## 6. Anthropometric / Basic Health\n",
    "| Variable          | Short Description | Type of Value |\n",
    "|--------------------|-------------------|---------------|\n",
    "| BMI               | Body Mass Index (weight/height²). | Numerical |\n",
    "| Height            | Height of the subject (e.g., in cm). | Numerical |\n",
    "| Weight            | Weight of the subject (e.g., in kg). | Numerical |\n",
    "| BPSystolic        | Systolic blood pressure (mmHg). | Numerical |\n",
    "| BPDiastolic       | Diastolic blood pressure (mmHg). | Numerical |\n",
    "| HbA1C             | Glycated hemoglobin level (indicates average blood glucose over ~3 months). | Numerical (continuous) |\n",
    "| ThyroidHormone    | Thyroid hormone levels (e.g., TSH, T4). | Numerical (continuous) |\n",
    "\n",
    "## 7. Sleep / Quality of Life\n",
    "| Variable          | Short Description | Type of Value |\n",
    "|--------------------|-------------------|---------------|\n",
    "| PSQI_Score        | Pittsburgh Sleep Quality Index global score (higher = poorer sleep quality). | Numerical (sum/index) |\n",
    "| PSQI Components   | Sub-scores for sleep latency, duration, disturbances, etc. | Numerical |\n",
    "| PainInterf_Tscore | PROMIS measure of pain interference with daily activities (T-score standardized). | Numerical (T-score) |\n",
    "| LifeSatisf_Unadj  | Self-reported life satisfaction (raw score). | Numerical (score) |\n",
    "| MeanPurp_Unadj    | Self-reported sense of meaning/purpose in life (raw score). | Numerical (score) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15aca0-148e-435f-b8f2-7a708b61a6d9",
   "metadata": {},
   "source": [
    "### Prepare hcp_flat dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1a5b87-fa69-44e1-bdb9-bd9e257485c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mae_utils.flat import load_hcp_flat_mask\n",
    "from mae_utils.flat import create_hcp_flat\n",
    "import mae_utils.visualize as vis\n",
    "\n",
    "if utils.is_interactive(): # Use less samples per epoch for debugging\n",
    "    probe_num_samples_per_epoch = 100000\n",
    "    test_num_samples_per_epoch = 100000\n",
    "    num_epochs = 10\n",
    "\n",
    "\n",
    "# Load ckpt\n",
    "if not os.path.exists(outdir) or not os.path.isdir(outdir):\n",
    "    assert True, (f\"\\nCheckpoint folder {outdir} does not exist.\\n\")\n",
    "else:\n",
    "    checkpoint_files = [f for f in os.listdir(outdir) if f.endswith('.pth')]\n",
    "\n",
    "    # Find the latest ckpt to load\n",
    "    epoch_numbers = []\n",
    "    for file in checkpoint_files:\n",
    "        try:\n",
    "            epoch_number = int(file.split('epoch')[-1].split('.')[0])\n",
    "            epoch_numbers.append(epoch_number)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    latest_epoch = max(epoch_numbers)\n",
    "    checkpoint_name = f\"epoch{latest_epoch}.pth\"\n",
    "    \n",
    "    ### Or provide the specific checkpoint you want to load\n",
    "    # checkpoint_name = \"epoch10.pth\" #\"epoch15.pth\"\n",
    "\n",
    "    # Load the checkpoint\n",
    "#     checkpoint_path = os.path.join(outdir, checkpoint_name)\n",
    "#     state = torch.load(checkpoint_path)\n",
    "\n",
    "# model = mae_vit_large_fmri(\n",
    "#     patch_size=16,\n",
    "#     decoder_embed_dim=decoder_embed_dim,\n",
    "#     t_patch_size=t_patch_size,\n",
    "#     pred_t_dim=pred_t_dim,\n",
    "#     decoder_depth=4,\n",
    "#     cls_embed=cls_embed,\n",
    "#     norm_pix_loss=norm_pix_loss,\n",
    "#     no_qkv_bias=no_qkv_bias,\n",
    "#     sep_pos_embed=sep_pos_embed,\n",
    "#     trunc_init=trunc_init,\n",
    "#     img_mask=state[\"model_state_dict\"]['img_mask']\n",
    "# )\n",
    "\n",
    "# model.load_state_dict(state[\"model_state_dict\"], strict=True) #model_state_dict\n",
    "# print(f\"\\nLoaded checkpoint {checkpoint_name} from {outdir}\\n\")\n",
    "\n",
    "# model.eval()\n",
    "# model.requires_grad_(False)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5bab48-aab8-4f37-b90c-7a78b859e419",
   "metadata": {},
   "source": [
    "### Prepare subjects information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48943eed-e504-4793-84f5-82a095968931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "###### This is for restricted\n",
    "# Categorical columns (e.g., demographic categories, binary diagnoses)\n",
    "categorical_columns = [\n",
    "    \"Gender\",\n",
    "    \"Race\",\n",
    "    \"Ethnicity\",\n",
    "    \"SSAGA_PanicDisorder\",  # Panic disorder diagnosis (yes/no)\n",
    "    \"SSAGA_Depressive_Ep\"   # Depressive episode diagnosis (yes/no)\n",
    "]\n",
    "\n",
    "# Numerical columns (continuous, counts, raw scores, standardized scores, etc.)\n",
    "numerical_columns = [\n",
    "    # Basic demographics\n",
    "    \"Age_in_Yrs\",\n",
    "    \n",
    "    # Cognitive / \"IQ-like\" Measures\n",
    "    \"PMAT24_A_CR\",\n",
    "    \"CardSort_Unadj\",\n",
    "    \"CardSort_AgeAdj\",\n",
    "    \"ListSort_Unadj\",\n",
    "    \"ListSort_AgeAdj\",\n",
    "    \"PicSeq_Unadj\",\n",
    "    \"PicSeq_AgeAdj\",\n",
    "    \n",
    "    # Personality Traits (Big Five)\n",
    "    \"NEOFAC_A\",\n",
    "    \"NEOFAC_O\",\n",
    "    \"NEOFAC_C\",\n",
    "    \"NEOFAC_N\",\n",
    "    \"NEOFAC_E\",\n",
    "    # If you have all 60 NEO item-level responses:\n",
    "    # \"NEORAW_01\", \"NEORAW_02\", ..., \"NEORAW_60\",\n",
    "    \n",
    "    # Psychopathology / Mental Health\n",
    "    \"ASR_Anxd_Raw\",\n",
    "    \"ASR_Attn_Raw\",\n",
    "    \"ASR_Aggr_Raw\",\n",
    "    \"DSM_Depr_Raw\",\n",
    "    \"DSM_Anxi_Raw\",\n",
    "    \"SSAGA_Depressive_Sx\",  # Symptom count or severity\n",
    "    \n",
    "    # Substance Use Phenotypes\n",
    "    \"SSAGA_Alc_12_Frq\",\n",
    "    \"SSAGA_Alc_12_Max_Drinks\",\n",
    "    \"SSAGA_Times_Used_Illicits\",\n",
    "    \"SSAGA_Times_Used_Cocaine\",\n",
    "    \"Total_Drinks_7days\",\n",
    "    \"Total_Any_Tobacco_7days\",\n",
    "    \n",
    "    # Anthropometric / Basic Health\n",
    "    \"BMI\",\n",
    "    \"Height\",\n",
    "    \"Weight\",\n",
    "    \"BPSystolic\",\n",
    "    \"BPDiastolic\",\n",
    "    \"HbA1C\",\n",
    "    \"ThyroidHormone\",\n",
    "    \n",
    "    # Sleep / Quality of Life\n",
    "    \"PSQI_Score\",\n",
    "    # If you have separate PSQI component scores, list them here too, e.g.:\n",
    "    # \"PSQI_Component1\", \"PSQI_Component2\", ...\n",
    "    \"PainInterf_Tscore\",\n",
    "    \"LifeSatisf_Unadj\",\n",
    "    \"MeanPurp_Unadj\"\n",
    "]\n",
    "\n",
    "\n",
    "if target in ['subject_id', 'trial_type']:\n",
    "    target_type = 'special'\n",
    "elif target in categorical_columns:\n",
    "    target_type = 'categorical'\n",
    "elif target in numerical_columns:\n",
    "    target_type = 'numerical'\n",
    "\n",
    "\n",
    "# open the file containing subject information\n",
    "if not (target in ['subject_id', 'trial_type']):\n",
    "    subject_information_HCP_path = os.path.join(hcp_flat_path, \"subjects_data_restricted.csv\")\n",
    "    try:\n",
    "        subject_information_HCP = pd.read_csv(subject_information_HCP_path)\n",
    "    except:\n",
    "        try:\n",
    "            subject_information_HCP = pd.read_csv('./unrestricted_clane9_4_23_2024_13_28_14.csv')   \n",
    "        except:\n",
    "            assert False, \"Subject information file not found\"\n",
    "\n",
    "    # # show the first few rows of the subject information\n",
    "    # subject_information_HCP[age_related_columns + sex_related_columns].head()\n",
    "\n",
    "    # Handle missing values (e.g., impute with mean)\n",
    "    mean_age = subject_information_HCP['Age_in_Yrs'].mean()\n",
    "    \n",
    "\n",
    "    if target in numerical_columns:\n",
    "        # Count NaNs or missing values\n",
    "        n_missing = subject_information_HCP[target].isnull().sum()\n",
    "        print(f\"Number of missing values in {target}: {n_missing}. Replacing with mean.\")\n",
    "        mean_ = subject_information_HCP[target].mean()\n",
    "        # Replace missing values or NaNs with the mean\n",
    "        subject_information_HCP[target].fillna(mean_, inplace=True)\n",
    "        # Initialize the scaler\n",
    "        scaler = StandardScaler()    \n",
    "        # Perform z-score normalization\n",
    "        subject_information_HCP[f'{target}_z'] = scaler.fit_transform(subject_information_HCP[[target]])\n",
    "\n",
    "    if target in categorical_columns:\n",
    "        # Perform label encoding\n",
    "        label_enc = LabelEncoder()\n",
    "        subject_information_HCP[f'{target}_encoded'] = label_enc.fit_transform(subject_information_HCP[target])\n",
    "\n",
    "def train_test_split_by_subject(df, test_ratio=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Split a dataframe into train and test so that\n",
    "    every subject in test also appears in train at least once.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Your dataset, containing at least the columns:\n",
    "        ['sub', ...]\n",
    "    test_ratio : float\n",
    "        Percentage of each subject's rows to allocate to test.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_df : pd.DataFrame\n",
    "    test_df : pd.DataFrame\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "    \n",
    "    # Group by subject\n",
    "    for subject, df_sub in df.groupby('sub'):\n",
    "        n = len(df_sub)\n",
    "        \n",
    "        # If the subject only has 1 row, put it all in train\n",
    "        if n == 1:\n",
    "            train_dfs.append(df_sub)\n",
    "        else:\n",
    "            # Decide how many rows go to test\n",
    "            n_test = int(round(test_ratio * n))\n",
    "            # Ensure at least 1 row ends up in train\n",
    "            # (i.e. if rounding leads to n_test == n, reduce n_test by 1)\n",
    "            if n_test >= n:\n",
    "                n_test = n - 1\n",
    "            \n",
    "            # Randomly sample n_test rows for test\n",
    "            test_rows = df_sub.sample(n_test, random_state=random_state)\n",
    "            # The remaining go to train\n",
    "            train_rows = df_sub.drop(test_rows.index)\n",
    "            \n",
    "            test_dfs.append(test_rows)\n",
    "            train_dfs.append(train_rows)\n",
    "    \n",
    "    # Combine all splits\n",
    "    train_df = pd.concat(train_dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "    \n",
    "def get_label_restricted(subject_id: List[str], target: str, normalized: bool = True) -> List:\n",
    "    \"\"\"\n",
    "    Get the label for the given subject id and target\n",
    "    \"\"\"\n",
    "    subject_id = [int(x) for x in subject_id]\n",
    "\n",
    "    if target in numerical_columns:\n",
    "        target_array = []\n",
    "        for subject in subject_id:\n",
    "            c_target = subject_information_HCP[subject_information_HCP['Subject'] == subject][f'{target}' if not normalized else f'{target}_z'].values\n",
    "            # if the subject is not in the subject information file trigger an error\n",
    "            if len(c_target) == 0:\n",
    "                assert False, f\"Subject {subject} not found in subject information file\"\n",
    "            if len(c_target) > 1:\n",
    "                print(f\"Warning: Multiple entries for subject {subject}\")\n",
    "\n",
    "            target_array.append(np.float32(c_target[0]))\n",
    "\n",
    "        return np.array(target_array)\n",
    "    \n",
    "    elif target in categorical_columns:\n",
    "        target_array = []\n",
    "        for subject in subject_id:\n",
    "            c_target = subject_information_HCP[subject_information_HCP['Subject'] == subject][f'{target}_encoded'].values\n",
    "            # if the subject is not in the subject information file trigger an error\n",
    "            if len(c_target) == 0:\n",
    "                assert False, f\"Subject {subject} not found in subject information file\"\n",
    "            if len(c_target) > 1:\n",
    "                print(f\"Warning: Multiple entries for subject {subject}\")\n",
    "\n",
    "            target_array.append(np.int8(c_target[0]))\n",
    "\n",
    "        return np.array(target_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25fe036-3bea-4968-bdfc-2383c40f54be",
   "metadata": {},
   "source": [
    "### Fit sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3461199-e805-4e9c-8c91-894e83cf8bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: subject_id\n",
      "train: (87748, 9), test: (9498, 9)\n",
      "test: (9498, 9)\n",
      "X_train: (87748, 1024), X_test: (9498, 1024)\n",
      "New shapes bcz of Subjec id prediction.- X_train: (87662, 1024), X_test: (9584, 1024)\n",
      "classes (1093): ['100206' '100307' '100408' ... '994273' '995174' '996782']\n",
      "\n",
      "y_train: (87662,) [1061  370  234  885  817  741  398  107 1057  332   80 1056  185  777\n",
      "  414  657  489  273 1081 1017]\n",
      "y_test: (9584,) [421 803 279 528 678 879 562 338 871 436 221 289 266 307  84 490 178 493\n",
      " 442 701]\n",
      "\n",
      "train_ind: 78895 [70938 24482 28961 85974  2298 42843 60708 83536 24913 60912]\n",
      "val_ind: 8767 [85066 29468 18593 71058 65582 59468 31687  8525 84802 26565]\n",
      "Fitting PCA projection\n",
      "Fitting logistic regression\n",
      "Done:\n",
      "{\"target\": \"subject_id\", \"train_acc\": 0.5002344888776221, \"val_acc\": 0.14086916847268166, \"test_acc\": 0.12635642737896494}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegressionCV, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "print(f\"Target: {target}\")\n",
    "\n",
    "train_features = pd.read_parquet(f\"{outdir}_gp{str(global_pooling)}/{parquet_folder}/HCP/train.parquet\")\n",
    "test_features = pd.read_parquet(f\"{outdir}_gp{str(global_pooling)}/{parquet_folder}/HCP/test.parquet\")\n",
    "\n",
    "print(f\"train: {train_features.shape}, test: {test_features.shape}\")\n",
    "print(f\"test: {test_features.shape}\")\n",
    "\n",
    "X_train = np.stack(train_features[\"feature\"])\n",
    "X_test = np.stack(test_features[\"feature\"])\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "if target == \"subject_id\":\n",
    "    # We have to redo the train test split including every possible subject from test on train.\n",
    "    all_features = pd.concat([train_features, test_features])\n",
    "    all_features[\"sub\"] = all_features[\"sub\"].apply(lambda x: x[0])\n",
    "    train_features, test_features = train_test_split_by_subject(all_features)\n",
    "    labels_train = train_features['sub'].values\n",
    "    labels_test = test_features['sub'].values\n",
    "    X_train = np.stack(train_features[\"feature\"])\n",
    "    X_test = np.stack(test_features[\"feature\"])\n",
    "    print(f\"New shapes bcz of Subjec id prediction.- X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "elif target == \"trial_type\":\n",
    "    labels_train = train_features[\"trial_type\"].values\n",
    "    labels_test = test_features[\"trial_type\"].values\n",
    "elif target_type == \"numerical\":\n",
    "    labels_train = [str(sample[0]) for sample in train_features['sub'].values.tolist()]\n",
    "    labels_test = [str(sample[0]) for sample in test_features['sub'].values.tolist()]\n",
    "    y_train = get_label_restricted(labels_train, target=target)\n",
    "    y_test = get_label_restricted(labels_test, target=target)\n",
    "elif target_type == \"categorical\":\n",
    "    labels_train = [str(sample[0]) for sample in train_features['sub'].values.tolist()]\n",
    "    labels_test = [str(sample[0]) for sample in test_features['sub'].values.tolist()]\n",
    "    y_train = get_label_restricted(labels_train, target=target)\n",
    "    y_test = get_label_restricted(labels_test, target=target)\n",
    "\n",
    "\n",
    "# elif target == \"sex\":\n",
    "#     labels_train = [str(sample[0]) for sample in train_features['sub'].values.tolist()]\n",
    "#     labels_test = [str(sample[0]) for sample in test_features['sub'].values.tolist()]\n",
    "#     labels_train = get_label_restricted(labels_train, target=\"sex\")\n",
    "#     labels_test = get_label_restricted(labels_test, target=\"sex\")\n",
    "# elif target == \"age\":\n",
    "#     labels_train = [str(sample[0]) for sample in train_features['sub'].values.tolist()]\n",
    "#     labels_test = [str(sample[0]) for sample in test_features['sub'].values.tolist()]\n",
    "#     labels_train = get_label_restricted(labels_train, target=\"age\")\n",
    "#     labels_test = get_label_restricted(labels_test, target=\"age\")\n",
    "\n",
    "# labels_train = np.array([label[0] if isinstance(label, np.ndarray) else label for label in labels_train])\n",
    "# labels_test = np.array([label[0] if isinstance(label, np.ndarray) else label for label in labels_test])\n",
    "\n",
    "if target == \"trial_type\":\n",
    "    labels_train = np.array([label[0] if isinstance(label, np.ndarray) else label for label in labels_train])\n",
    "    labels_test = np.array([label[0] if isinstance(label, np.ndarray) else label for label in labels_test])\n",
    "    label_enc = LabelEncoder()\n",
    "    y_train = label_enc.fit_transform(labels_train)\n",
    "    y_test = label_enc.transform(labels_test)\n",
    "    print(f\"classes ({len(label_enc.classes_)}): {label_enc.classes_}\")\n",
    "    \n",
    "elif target == \"subject_id\":\n",
    "    label_enc = LabelEncoder()\n",
    "    y_train = label_enc.fit_transform(labels_train)\n",
    "    y_test = label_enc.transform(labels_test)\n",
    "    print(f\"classes ({len(label_enc.classes_)}): {label_enc.classes_}\")\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"\\ny_train: {y_train.shape} {y_train[:20]}\\n\"\n",
    "    f\"y_test: {y_test.shape} {y_test[:20]}\"\n",
    ")\n",
    "del train_features, test_features\n",
    "\n",
    "train_ind, val_ind = train_test_split(\n",
    "    np.arange(len(X_train)), train_size=0.9, random_state=42\n",
    ")\n",
    "print(\n",
    "    f\"\\ntrain_ind: {len(train_ind)} {train_ind[:10]}\\n\"\n",
    "    f\"val_ind: {len(val_ind)} {val_ind[:10]}\"\n",
    ")\n",
    "X_train, X_val = X_train[train_ind], X_train[val_ind]\n",
    "y_train, y_val = y_train[train_ind], y_train[val_ind]\n",
    "\n",
    "print(\"Fitting PCA projection\")\n",
    "pca = PCA(n_components=384, whiten=True, svd_solver=\"randomized\")\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train = pca.transform(X_train)\n",
    "X_val = pca.transform(X_val)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "if target_type == \"special\" or target_type == \"categorical\":\n",
    "    print(\"Fitting logistic regression\")\n",
    "    clf = LogisticRegressionCV()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = clf.score(X_train, y_train)\n",
    "    val_acc = clf.score(X_val, y_val)\n",
    "    test_acc = clf.score(X_test, y_test)\n",
    "    \n",
    "    result = {\n",
    "        \"target\": target,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"test_acc\": test_acc,\n",
    "    }\n",
    "\n",
    "elif target_type == \"numerical\":\n",
    "    alpha = 10000\n",
    "    print(\"Fitting ridge regression\")\n",
    "    clf = Ridge()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate R² scores\n",
    "    train_r2 = clf.score(X_train, y_train)\n",
    "    val_r2 = clf.score(X_val, y_val)\n",
    "    test_r2 = clf.score(X_test, y_test)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_pred = clf.predict(X_train)\n",
    "    val_pred = clf.predict(X_val)\n",
    "    test_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    train_mse = mean_squared_error(y_train, train_pred)\n",
    "    val_mse = mean_squared_error(y_val, val_pred)\n",
    "    test_mse = mean_squared_error(y_test, test_pred)\n",
    "    \n",
    "    # Compile results\n",
    "    result = {\n",
    "        \"target\": target,\n",
    "        \"train_r2\": train_r2,\n",
    "        \"val_r2\": val_r2,\n",
    "        \"test_r2\": test_r2,\n",
    "        \"train_mse\": float(train_mse),\n",
    "        \"val_mse\": float(val_mse),\n",
    "        \"test_mse\": float(test_mse),\n",
    "    }\n",
    "\n",
    "with open(f\"{outdir}_gp{str(global_pooling)}/{parquet_folder}/HCP/downstream_{target}.json\", 'w') as out_json:\n",
    "    json.dump(result, out_json)\n",
    "\n",
    "print(f\"Done:\\n{json.dumps(result)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation_env",
   "language": "python",
   "name": "foundation_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
