{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-paulscotti/found/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/admin/home-paulscotti/found/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "NUM GPUS  1\n",
      "WORLD_SIZE  1\n",
      "PID of this process = 3206697\n",
      "device = cuda distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from models import get_vit\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(\"NUM GPUS \", num_devices)\n",
    "distributed = True if num_devices>1 else False\n",
    "\n",
    "world_size = os.getenv('COUNT_NODE')\n",
    "if world_size is None: \n",
    "    world_size = 1\n",
    "else:\n",
    "    world_size = int(world_size)\n",
    "print(\"WORLD_SIZE \", world_size)\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "\n",
    "data_type = torch.bfloat16 # change depending on your mixed_precision\n",
    "batch_size = global_batch_size // num_devices\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "print(\"device =\", device, \"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08179db-9c6a-4bc6-a245-79fae6884ca2",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b1c3fe-28ab-40b7-8906-c6a9c8070d00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'patch8_small_test', 'use_cls_token': False, 'use_contrastive_loss': False, 'constrastive_loss_weight': 1.0, 'global_batch_size': 8, 'num_workers': 4, 'num_epochs': 50, 'seed': 42, 'max_lr': 3e-05, 'num_samples_per_epoch': 1024, 'ckpt_saving': False, 'ckpt_interval': 50, 'resume_from_ckpt': False, 'wandb_log': False, 'tube_start_masking_ratio': 0.9, 'tube_end_masking_ratio': 0.9, 'decoder_mask_ratio': 0.95, 'encoder_model': 'vit_small', 'decoder_model': 'vit_small', 'patch_size': 8, 'frame_patch_size': 1, 'use_rope_emb': False, 'masking_strategy': 'MNI', 'img_size': [88, 104, 72], 'num_frames': 4, 'train_urls': '/weka/proj-fmri/paulscotti/old_fMRI-foundation-model/dataset_creation/wds_creation/wds/{000001..000240}.tar', 'test_urls': '/weka/proj-fmri/paulscotti/old_fMRI-foundation-model/dataset_creation/wds_creation/wds/000000.tar'}\n",
      "outdir /weka/proj-fmri/paulscotti/fMRI-foundation-model/ckpts/patch8_small_test\n",
      "global_batch_size 8\n",
      "use_cls_token False\n",
      "num_patches 5148\n",
      "num_encoder_patches 514\n",
      "num_decoder_patches 257\n"
     ]
    }
   ],
   "source": [
    "print(config)\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "if use_contrastive_loss:\n",
    "    global_batch_size = global_batch_size // 2 # contrastive loss doubles the batch size with the same samples and different masks\n",
    "print(\"global_batch_size\", global_batch_size)\n",
    "\n",
    "use_cls_token = True if use_contrastive_loss else use_cls_token\n",
    "print(\"use_cls_token\", use_cls_token)\n",
    "\n",
    "num_patches = int(\n",
    "    (img_size[0] / patch_size)\n",
    "    * (img_size[1] / patch_size)\n",
    "    * (img_size[2] / patch_size)\n",
    "    * num_frames\n",
    ")\n",
    "num_patches_per_timepoint = num_patches // num_frames\n",
    "num_encoder_patches = int(num_patches_per_timepoint * (1 - tube_start_masking_ratio) * num_frames)\n",
    "num_decoder_patches = int(num_patches_per_timepoint * (1 - decoder_mask_ratio) * num_frames)\n",
    "print(\"num_patches\", num_patches)\n",
    "print(\"num_encoder_patches\", num_encoder_patches)\n",
    "print(\"num_decoder_patches\", num_decoder_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8419d-988f-42c6-acb6-b258e0694eee",
   "metadata": {},
   "source": [
    "# Prep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e37b6b9-5b91-4c4a-af85-ac2af69704e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "43,096,320 total\n",
      "43,096,320 trainable\n",
      "\n",
      "encoder\n",
      "torch.Size([6, 1, 4, 88, 104, 72])\n",
      "patched torch.Size([6, 4, 11, 13, 9, 512])\n",
      "patched_emb torch.Size([6, 4, 11, 13, 9, 384])\n",
      "torch.Size([6, 5148, 384])\n",
      "pe torch.Size([5148, 384])\n",
      "x torch.Size([6, 5148, 384])\n",
      "masked torch.Size([6, 514, 384])\n",
      "torch.Size([6, 514, 384])\n",
      "\n",
      "decoder\n",
      "torch.Size([6, 514, 384])\n",
      "pe torch.Size([5148, 384])\n",
      "pos_emd_encoder torch.Size([514, 384])\n",
      "pos_emd_decoder torch.Size([257, 384])\n",
      "x_concat torch.Size([6, 771, 384])\n",
      "torch.Size([6, 771, 384])\n",
      "proj torch.Size([6, 771, 512])\n"
     ]
    }
   ],
   "source": [
    "vit_size = {\n",
    "    \"encoder\": encoder_model,\n",
    "    \"decoder\": decoder_model\n",
    "}\n",
    "    \n",
    "model = get_vit(\n",
    "    size=vit_size,\n",
    "    image_size=img_size,  # depth, height, width\n",
    "    image_patch_size=(patch_size,patch_size,patch_size),  # depth, height, width patch size\n",
    "    frames=num_frames,\n",
    "    frame_patch_size=frame_patch_size,\n",
    "    channels=1,\n",
    "    use_rope_emb=use_rope_emb,\n",
    "    use_cls_token=use_cls_token,\n",
    ")\n",
    "utils.count_params(model)\n",
    "\n",
    "# function to select random num_frames from sample and obtain brain-positive patches\n",
    "aug_transform = utils.DataPrepper(\n",
    "    num_frames=num_frames,\n",
    "    masking_strategy=masking_strategy,\n",
    "    patch_depth=patch_size,\n",
    "    patch_height=patch_size,\n",
    "    patch_width=patch_size,\n",
    "    frame_patch_size=frame_patch_size,\n",
    ")\n",
    "\n",
    "# test that the model works without error\n",
    "model = model.to(device)\n",
    "encoder_mask = torch.zeros(num_patches).to(device).to(torch.bool)\n",
    "encoder_mask[:num_encoder_patches] = True\n",
    "decoder_mask = torch.zeros(num_patches).to(device).to(torch.bool)\n",
    "decoder_mask[-num_decoder_patches:] = True\n",
    "with torch.no_grad():\n",
    "    print(\"\\nencoder\")\n",
    "    encoder_out = model(\n",
    "                torch.randn(6, 1, num_frames, img_size[0], img_size[1], img_size[2]).to(device),\n",
    "                encoder_mask=encoder_mask,\n",
    "                verbose=True)\n",
    "    print(\"\\ndecoder\")\n",
    "    decoder_out = model(\n",
    "                encoder_out, \n",
    "                encoder_mask=encoder_mask, \n",
    "                decoder_mask=decoder_mask, \n",
    "                verbose=True)\n",
    "    if use_cls_token:\n",
    "        enc_cls_token = encoder_out[:, :1, :]\n",
    "        encoder_patches = encoder_out[:, 1:, :]\n",
    "        dec_cls_token = decoder_out[:, :1, :]\n",
    "        decoder_patches = decoder_out[:, 1:, :]\n",
    "        print(\"\\nenc_cls_token\", enc_cls_token.shape)\n",
    "        print(\"encoder_patches\", encoder_patches.shape)\n",
    "        print(\"dec_cls_token\", dec_cls_token.shape)\n",
    "        print(\"decoder_patches\", decoder_patches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd51ddf-fb71-48f4-bdbd-88753b44d2aa",
   "metadata": {},
   "source": [
    "## Create dataset and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd3c153-ad63-4707-a24b-6a3a658fd0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75dd0398-0eb8-464e-8bb8-13e7ce1a8480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/weka/proj-fmri/paulscotti/old_fMRI-foundation-model/dataset_creation/wds_creation/wds/{000001..000240}.tar\n",
      "Adding decoder ImageHandler to decoders.\n",
      "####################################################################################################\n",
      "Building dataloader with the following parameters\n",
      "batch_size: 8, num_workers: 4\n",
      "####################################################################################################\n",
      "Adding decoder ImageHandler to decoders.\n",
      "####################################################################################################\n",
      "Building dataloader with the following parameters\n",
      "batch_size: 8, num_workers: 4\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "print(train_urls)\n",
    "print(test_urls)\n",
    "\n",
    "train_dp = create_dataset(train_urls, \n",
    "                          is_s3=train_urls[:2]==\"s3\", \n",
    "                          sample_shuffle=100, shard_shuffle=100)\n",
    "train_dl = create_loader(train_dp, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "test_dp = create_dataset(test_urls, \n",
    "                         is_s3=test_urls[:2]==\"s3\", \n",
    "                         sample_shuffle=1, shard_shuffle=1)\n",
    "test_dl = create_loader(test_dp, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d05a32-12eb-494a-82df-dce4c7e9924c",
   "metadata": {},
   "source": [
    "### Check data loaders work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "480e7c0b-f58e-4c35-80fe-1284ee0e3966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yielding 2 batches\n",
      "iter 0\n",
      "iter 1\n",
      "Done!\n",
      "input_func torch.Size([8, 32, 88, 104, 72])\n"
     ]
    }
   ],
   "source": [
    "num_it = 2\n",
    "print(f\"Yielding {num_it} batches\")\n",
    "\n",
    "for i, batch in enumerate(train_dl):\n",
    "    print(\"iter\",i)\n",
    "    input_func = batch['func.npy']\n",
    "    if i >= (num_it-1):\n",
    "        break\n",
    "\n",
    "print(\"Done!\")\n",
    "print(\"input_func\", input_func.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c6fba-5494-4964-b03d-e01c3afe47db",
   "metadata": {},
   "source": [
    "# Playing with the data, visualization of patching + masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea69a0b-f0a0-4309-b689-553b911a8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# func, brain_pos_pats = aug_transform(input_func)\n",
    "# print(func.shape)\n",
    "# utils.view_brain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b577d-e326-4014-b123-8400df834fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if masking_strategy==\"MNI\":\n",
    "    MNI_brain = nib.load(\"/weka/proj-fmri/paulscotti/old_fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "    brain_pos_voxels = MNI_brain[6:94,8:112,10:82]\n",
    "    brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "    brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd7913-707a-44ba-b031-afc883fb357c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if utils.is_interactive():\n",
    "    # extract func volumes and their reference mean and standard deviation volumes\n",
    "    if masking_strategy==\"MNI\":\n",
    "        func, _ = aug_transform(input_func)\n",
    "    else:\n",
    "        func, brain_pos_voxels = aug_transform(input_func)\n",
    "        brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "        brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "    func = func.unsqueeze(1)  # add empty first dimension to serve as 1d channel dimension\n",
    "\n",
    "    # patchify func samples\n",
    "    print(\"func\", func.shape)\n",
    "    patches = model.patchify(func)\n",
    "    print(\"patches\", patches.shape)\n",
    "    patches_vit = rearrange(patches, \"b ... d -> b (...) d\")\n",
    "    print(\"patches_vit\", patches_vit.shape)\n",
    "    print(\"num patches in one timepoint\", patches_vit.shape[1] // num_frames)\n",
    "\n",
    "    # start by masking everything (aka include nothing)\n",
    "    tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "    # approximate brain positive patches for the whole batch\n",
    "    batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "    mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "    mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "    print(\"Percentage of brain positive patches\",\n",
    "        len(mask_idx_candidates) / len(batch_positive_approx))\n",
    "    tube_idx = mask_idx_candidates[: int(num_patches / num_frames * (1 - tube_start_masking_ratio))]\n",
    "    print(\"num tube patches =\", len(tube_idx))\n",
    "    tube_mask[tube_idx] = True  # Trues mean to include the patch, False means to remove the patch\n",
    "    tube_mask = tube_mask.tile(num_frames)  # repeat masking for the other timepoints\n",
    "    print(\"tube mask percent\", tube_mask.sum().item() / len(tube_mask))\n",
    "\n",
    "    # create decoder mask similar to tube mask, but ensure no overlap\n",
    "    decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)  # start by masking everything (aka include nothing)\n",
    "    remaining_mask_idx = mask_idx_candidates[int(num_patches / num_frames * (1 - tube_start_masking_ratio)) :]  # brain positive tokens not selected for the encoder tokens\n",
    "    decoder_mask_idx = remaining_mask_idx[:int(num_patches / num_frames * (1 - decoder_mask_ratio))]\n",
    "    print(\"num decoder patches =\", len(decoder_mask_idx))\n",
    "    decoder_mask[decoder_mask_idx] = True\n",
    "    decoder_mask = decoder_mask.tile(num_frames)  # repeat masking for the other timepoints\n",
    "    print(\"decoder_mask percent\", decoder_mask.sum().item() / len(decoder_mask))\n",
    "\n",
    "    # apply masks to patches_vit\n",
    "    tube_patches_vit = copy.deepcopy(patches_vit.detach())\n",
    "    decoder_patches_vit = copy.deepcopy(patches_vit.detach())\n",
    "    # tube_patches_vit[:, tube_mask] = 1\n",
    "    # decoder_patches_vit[:, decoder_mask] = 1\n",
    "    tube_patches_vit[:, ~tube_mask] = 0\n",
    "    decoder_patches_vit[:, ~decoder_mask] = 0\n",
    "\n",
    "    # undo patchification so we can visualize\n",
    "    tube_unpatches = rearrange(\n",
    "        tube_patches_vit,\n",
    "        \"b (f d h w) c -> b f d h w c\",\n",
    "        d=img_size[0]//patch_size,\n",
    "        h=img_size[1]//patch_size,\n",
    "        w=img_size[2]//patch_size,\n",
    "    )\n",
    "    decoder_unpatches = rearrange(\n",
    "        decoder_patches_vit,\n",
    "        \"b (f d h w) c -> b f d h w c\",\n",
    "        d=img_size[0]//patch_size,\n",
    "        h=img_size[1]//patch_size,\n",
    "        w=img_size[2]//patch_size,\n",
    "    )\n",
    "    print(\"tube_unpatches\", tube_unpatches.shape)\n",
    "    print(\"decoder_unpatches\", decoder_unpatches.shape)\n",
    "    \n",
    "    tube_func = rearrange(\n",
    "        tube_unpatches,\n",
    "        \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "        b=len(func),\n",
    "        f=num_frames,\n",
    "        d=img_size[0] // patch_size,\n",
    "        h=img_size[1] // patch_size,\n",
    "        w=img_size[2] // patch_size,\n",
    "        pd=patch_size,\n",
    "        ph=patch_size,\n",
    "        pw=patch_size,\n",
    "        pf=frame_patch_size,\n",
    "    )\n",
    "    decoder_func = rearrange(\n",
    "        decoder_unpatches,\n",
    "        \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "        b=len(func),\n",
    "        f=num_frames,\n",
    "        d=img_size[0] // patch_size,\n",
    "        h=img_size[1] // patch_size,\n",
    "        w=img_size[2] // patch_size,\n",
    "        pd=patch_size,\n",
    "        ph=patch_size,\n",
    "        pw=patch_size,\n",
    "        pf=frame_patch_size,\n",
    "    )\n",
    "    print(\"tube_func\", tube_func.shape)\n",
    "    print(\"decoder_func\", decoder_func.shape)\n",
    "    \n",
    "    brain_pos_vit = copy.deepcopy(patches_vit.detach())\n",
    "    brain_pos_vit[:,batch_positive_approx.repeat(num_frames)] = 1\n",
    "    brain_pos_vit[:,~batch_positive_approx.repeat(num_frames)] = 0\n",
    "    brain_pos_unpatches = rearrange(\n",
    "        brain_pos_vit,\n",
    "        \"b (f d h w) c -> b f d h w c\",\n",
    "        d=img_size[0]//patch_size,\n",
    "        h=img_size[1]//patch_size,\n",
    "        w=img_size[2]//patch_size,\n",
    "    )\n",
    "    brain_pos_func = rearrange(\n",
    "        brain_pos_unpatches,\n",
    "        \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "        b=len(func),\n",
    "        f=num_frames,\n",
    "        d=img_size[0] // patch_size,\n",
    "        h=img_size[1] // patch_size,\n",
    "        w=img_size[2] // patch_size,\n",
    "        pd=patch_size,\n",
    "        ph=patch_size,\n",
    "        pw=patch_size,\n",
    "        pf=frame_patch_size,\n",
    "    )\n",
    "\n",
    "    # Visualize\n",
    "    idx = 0\n",
    "    print(\"original func\")\n",
    "    display(transforms.ToPILImage()(utils.reshape_to_2d(func[idx].clamp(0,1))))\n",
    "    \n",
    "    print(\"\\nbrain-positive patches\")\n",
    "    display(transforms.ToPILImage()(utils.reshape_to_2d(brain_pos_func[idx].clamp(0,1))))\n",
    "\n",
    "    print(\"\\ntube func\")\n",
    "    display(transforms.ToPILImage()(utils.reshape_to_2d(tube_func[idx].clamp(0,1))))\n",
    "\n",
    "    print(\"\\ndecoder func\")\n",
    "    display(transforms.ToPILImage()(utils.reshape_to_2d(decoder_func[idx].clamp(0,1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604b380-0126-49b2-ab4e-59b1e463d6ad",
   "metadata": {},
   "source": [
    "# Set up optimizer and saving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da73c08-ca61-48ef-9e63-b70db6f07a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "num_iterations_per_epoch = num_samples_per_epoch // global_batch_size\n",
    "test_num_iterations_per_epoch = num_samples_per_epoch // global_batch_size\n",
    "\n",
    "total_steps = num_epochs * num_iterations_per_epoch * num_devices\n",
    "print(\"total_steps\", total_steps)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    ")\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f674d-7cd4-49d1-bd8d-c697d9614f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def save_ckpt(tag=\"last\"):\n",
    "#     ckpt_path = outdir+f'/{tag}'\n",
    "#     os.makedirs(ckpt_path,exist_ok=True)\n",
    "#     accelerator.save_model(model, ckpt_path, max_shard_size=\"2GB\", safe_serialization=True)\n",
    "#     print(f\"\\n---saved {ckpt_path}!---\\n\")\n",
    "        \n",
    "# def save_progress(tag=\"last\"):\n",
    "#     if accelerator.is_main_process:\n",
    "#         ckpt_path = outdir+f'/{tag}'\n",
    "#         torch.save(\n",
    "#                 {\n",
    "#                     \"optimizer\": optimizer.state_dict(),\n",
    "#                     \"scheduler\": lr_scheduler.state_dict(),\n",
    "#                     \"epoch\": epoch,\n",
    "#                     \"recon_losses\": recon_losses,\n",
    "#                     \"contrastive_losses\": contrastive_losses,\n",
    "#                     \"test_losses\": test_losses,\n",
    "#                     \"lrs\": lrs,\n",
    "#                 },\n",
    "#                 os.path.join(ckpt_path, f\"params.pt\"),\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b4262-435b-4872-ab4a-424cb9dfd37a",
   "metadata": {},
   "source": [
    "# Start wandb (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56431733-4fb5-4072-840e-22536608f9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"num_iterations_per_epoch\": num_iterations_per_epoch,\n",
    "      \"encoder_model\": encoder_model,\n",
    "      \"decoder_model\": decoder_model,\n",
    "      \"tube_start_masking_ratio\": tube_start_masking_ratio,\n",
    "      \"tube_end_masking_ratio\": tube_end_masking_ratio,\n",
    "      \"decoder_mask_ratio\": decoder_mask_ratio,\n",
    "      \"num_frames\": num_frames,\n",
    "      \"patch_size\": patch_size,\n",
    "      \"frame_patch_size\": frame_patch_size,\n",
    "      \"use_contrastive_loss\": use_contrastive_loss,\n",
    "      \"use_cls_token\": use_cls_token,\n",
    "      \"constrastive_loss_weight\": constrastive_loss_weight,\n",
    "      \"num_params\": num_params,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_urls\": train_urls,\n",
    "      \"test_urls\": test_urls,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a5055-8afd-468a-93bf-32f94bd1d042",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e394dd-2745-41fa-a5aa-54b7b1373a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "lrs, recon_losses, contrastive_losses, test_losses = [], [], [], []\n",
    "best_test_loss = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a5605-fd81-41d4-964e-1752ed6e1289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # resume from ckpt (e.g., if you are resuming from a run that got pre-empted)\n",
    "# load_progress = False\n",
    "# if wandb_log:\n",
    "#     if wandb.run.resumed:\n",
    "#         load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "#         load_progress = True\n",
    "# elif resume_from_ckpt: # if resuming without using wandb\n",
    "#     load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "#     load_progress = True\n",
    "    \n",
    "# if load_progress:\n",
    "#     ckpt_path = outdir+'/last'\n",
    "#     prev_params = torch.load(ckpt_path+\"/params.pt\")\n",
    "#     optimizer.load_state_dict(prev_params[\"optimizer\"])\n",
    "#     lr_scheduler.load_state_dict(prev_params[\"scheduler\"])\n",
    "#     epoch = prev_params[\"epoch\"]\n",
    "#     recon_losses = prev_params[\"recon_losses\"]\n",
    "#     contrastive_losses = prev_params[\"contrastive_losses\"]\n",
    "#     test_losses = prev_params[\"test_losses\"]\n",
    "#     lrs = prev_params[\"lrs\"]\n",
    "#     print(\"Loaded model params from\", ckpt_path, \"at epoch\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e258c4-8477-48ed-81af-d415cee246f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "if use_contrastive_loss:\n",
    "    logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))  # learned logit scale\n",
    "\n",
    "progress_bar = tqdm(range(epoch, num_epochs), disable=local_rank!=0, desc=\"Overall\")\n",
    "for epoch in progress_bar:\n",
    "    # get the masking ratio for the current epoch\n",
    "    tube_mask_ratio = utils.get_masking_ratio(\n",
    "        current_epoch=epoch, \n",
    "        total_epochs=num_epochs, \n",
    "        start_masking_ratio=tube_start_masking_ratio, \n",
    "        end_masking_ratio=tube_end_masking_ratio\n",
    "    )\n",
    "    with torch.cuda.amp.autocast(dtype=data_type):\n",
    "        model.train()\n",
    "        for train_i, batch in enumerate(tqdm(train_dl, disable=local_rank!=0, \n",
    "                 total=num_iterations_per_epoch-1, leave=False, desc=\"Training\")):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_func = batch['func.npy'].to(device)\n",
    "\n",
    "            print(\"device\", local_rank)\n",
    "            print(input_func.shape)\n",
    "\n",
    "            if masking_strategy==\"MNI\":\n",
    "                func, _ = aug_transform(input_func)\n",
    "            else:\n",
    "                func, brain_pos_voxels = aug_transform(input_func)\n",
    "                brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "                brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "                \n",
    "            if use_contrastive_loss:  # create positive pairs by duplicating the batch\n",
    "                func = torch.cat([func, func], dim=0)\n",
    "                meansd = torch.cat([meansd, meansd], dim=0)\n",
    "                brain_pos_pats = torch.cat([brain_pos_pats, brain_pos_pats], dim=0)\n",
    "\n",
    "            func = func.unsqueeze(1).to(device)\n",
    "\n",
    "            # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "            tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "            mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "            # check if there's not enough brain left for code to continue\n",
    "            if len(mask_idx_candidates) < (int(num_patches/num_frames*(1-tube_mask_ratio))+int(num_patches/num_frames*(1-decoder_mask_ratio))):\n",
    "                print(\"Brain volume skipped due to not enough brain-positive patches remaining...\")\n",
    "                continue\n",
    "            mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "            tube_idx = mask_idx_candidates[:int(num_patches / num_frames * (1 - tube_mask_ratio))]\n",
    "            tube_mask[tube_idx] = True\n",
    "            tube_mask = tube_mask.tile(num_frames)\n",
    "\n",
    "            # create decoder mask\n",
    "            decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            remaining_mask_idx = mask_idx_candidates[int(num_patches / num_frames * (1 - tube_mask_ratio)):]\n",
    "            decoder_mask_idx = remaining_mask_idx[:int(num_patches / num_frames * (1 - decoder_mask_ratio))]\n",
    "            decoder_mask[decoder_mask_idx] = True\n",
    "            decoder_mask = decoder_mask.tile(num_frames)\n",
    "\n",
    "            # encode the tube patches\n",
    "            encoder_out = model(func, encoder_mask=tube_mask)\n",
    "            if use_cls_token:\n",
    "                enc_cls_token = encoder_out[:,:1,:]\n",
    "\n",
    "            # decode both the encoder_out patches and masked decoder patches\n",
    "            decoder_out = model(encoder_out, encoder_mask=tube_mask, decoder_mask=decoder_mask)\n",
    "            # subset only the reconstructed decoder patches\n",
    "            output = decoder_out[:, -decoder_mask.sum():]\n",
    "\n",
    "            # compare to ground truth and calculate loss\n",
    "            target_patches = model.patchify(func)\n",
    "            target_patches_vit = rearrange(target_patches, \"b ... d -> b (...) d\")\n",
    "            target = target_patches_vit[:, decoder_mask]\n",
    "            loss = mse(output, target)\n",
    "\n",
    "            # contrastive loss\n",
    "            if use_contrastive_loss:\n",
    "                n_b = len(func) // 2\n",
    "                cls_token1 = enc_cls_token[:n_b, 0, :]  # first half of batch, cls_token shape B, 1, d_model\n",
    "                cls_token2 = enc_cls_token[n_b:, 0, :]\n",
    "                contrastive_loss = utils.contrastive_loss(cls_token1, cls_token2, temperature=logit_scale)\n",
    "                loss += constrastive_loss_weight * contrastive_loss\n",
    "                contrastive_losses.append(contrastive_loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            recon_losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "            if train_i >= (num_iterations_per_epoch-1):\n",
    "                break\n",
    "\n",
    "        if epoch%5==0:\n",
    "            model.eval()\n",
    "            for test_i, batch in enumerate(tqdm(test_dl, disable=local_rank!=0, total=test_num_iterations_per_epoch-1, leave=False, desc=\"Testing\")):\n",
    "                input_func = batch['func.npy'].to(device)\n",
    "                \n",
    "                if masking_strategy==\"MNI\":\n",
    "                    func, _ = aug_transform(input_func)\n",
    "                else:\n",
    "                    func, brain_pos_voxels = aug_transform(input_func)\n",
    "                    brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "                    brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "                func = func.unsqueeze(1).to(device)\n",
    "    \n",
    "                # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "                tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "                batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "                mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "                # check if there's not enough brain left for code to continue\n",
    "                if len(mask_idx_candidates) < (int(num_patches/num_frames*(1-tube_mask_ratio))+int(num_patches/num_frames*(1-decoder_mask_ratio))):\n",
    "                    if test_i==0:\n",
    "                        print(\"Brain volume skipped due to not enough brain-positive patches remaining...\")\n",
    "                    continue\n",
    "                mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "                tube_idx = mask_idx_candidates[:int(num_patches / num_frames * (1 - tube_mask_ratio))]\n",
    "                tube_mask[tube_idx] = True\n",
    "                tube_mask = tube_mask.tile(num_frames)\n",
    "    \n",
    "                # create decoder mask\n",
    "                decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "                remaining_mask_idx = mask_idx_candidates[int(num_patches / num_frames * (1 - tube_mask_ratio)):]\n",
    "                decoder_mask_idx = remaining_mask_idx[:int(num_patches / num_frames * (1 - decoder_mask_ratio))]\n",
    "                decoder_mask[decoder_mask_idx] = True\n",
    "                decoder_mask = decoder_mask.tile(num_frames)\n",
    "    \n",
    "                # encode the tube patches\n",
    "                encoder_out = model(func, encoder_mask=tube_mask)\n",
    "                # decode both the encoder_out patches and masked decoder patches\n",
    "                decoder_out = model(encoder_out, encoder_mask=tube_mask, decoder_mask=decoder_mask)\n",
    "                # subset only the reconstructed decoder patches\n",
    "                output = decoder_out[:, -decoder_mask.sum():]\n",
    "    \n",
    "                # compare to ground truth and calculate loss\n",
    "                target_patches = model.patchify(func)\n",
    "                target_patches_vit = rearrange(target_patches, \"b ... d -> b (...) d\")\n",
    "                target = target_patches_vit[:, decoder_mask]\n",
    "                loss = mse(output, target)\n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "                if test_i >= (test_num_iterations_per_epoch-1):\n",
    "                    break\n",
    "\n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(recon_losses[-(train_i + 1) :]),\n",
    "            \"test/loss\": np.mean(test_losses[-(test_i + 1) :]),\n",
    "            \"train/num_steps\": len(recon_losses),\n",
    "            \"test/num_steps\": len(test_losses),\n",
    "            \"lr\": np.mean(lrs[-(train_i + 1) :]),\n",
    "            \"epoch\": epoch,\n",
    "            \"tube_mask_ratio\": tube_mask_ratio,\n",
    "            \"decoder_mask_ratio\": decoder_mask_ratio,\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        if wandb_log: wandb.log(logs)\n",
    "\n",
    "        # Plot progress (first sample in batch)\n",
    "        with torch.no_grad():\n",
    "            if utils.is_interactive():\n",
    "                # prep reference volumes for going back to original data\n",
    "                idx = 0\n",
    "                if epoch == 0:\n",
    "                    print(\"original volumes\")\n",
    "                    display(transforms.ToPILImage()(utils.reshape_to_2d(func[idx].clamp(0,1))))\n",
    "                if epoch % 5 == 0:\n",
    "                    # undo patchification so we can visualize\n",
    "                    decode_vis = torch.zeros_like(target_patches_vit)\n",
    "                    decode_vis[:, decoder_mask] = output.to(decode_vis.dtype)\n",
    "                    decoder_unpatches = rearrange(\n",
    "                        decode_vis,\n",
    "                        \"b (f d h w) c -> b f d h w c\",\n",
    "                        d=img_size[0]//patch_size,\n",
    "                        h=img_size[1]//patch_size,\n",
    "                        w=img_size[2]//patch_size,\n",
    "                    )\n",
    "                    decoder_func = rearrange(\n",
    "                        decoder_unpatches,\n",
    "                        \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "                        b=batch_size,\n",
    "                        f=num_frames,\n",
    "                        d=img_size[0]//patch_size,\n",
    "                        h=img_size[1]//patch_size,\n",
    "                        w=img_size[2]//patch_size,\n",
    "                        pd=patch_size,\n",
    "                        ph=patch_size,\n",
    "                        pw=patch_size,\n",
    "                        pf=frame_patch_size,\n",
    "                    )\n",
    "                    print(\"recons of decoded patches\")\n",
    "                    display(transforms.ToPILImage()(utils.reshape_to_2d(decoder_func[idx].clamp(0,1))))\n",
    "                \n",
    "        # # Save model checkpoint\n",
    "        # if (ckpt_saving) and ((epoch % ckpt_interval == 0) or (epoch==num_epochs-1)):\n",
    "        #     save_ckpt()\n",
    "        #     save_progress()\n",
    "            \n",
    "        # wait for other GPUs to catch up if needed\n",
    "        if distributed: torch.distributed.barrier()\n",
    "        get_accelerator().empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6e847-2511-4f69-aa70-f1471a5b7d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(recon_losses)\n",
    "plt.title(\"Training re-construction losses\")\n",
    "plt.show()\n",
    "if use_contrastive_loss:\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(contrastive_losses)\n",
    "    plt.title(\"Training contrastive losses\")\n",
    "    plt.show()\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(test_losses)\n",
    "plt.title(\"Test losses\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "found",
   "language": "python",
   "name": "found"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
