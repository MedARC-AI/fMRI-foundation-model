{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6af0bb-19b2-472a-bd32-cd10bd997132",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a60b5f-c5db-46f5-8569-e10ac40fb8a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached5\n",
      "Number of available CUDA devices: 0\n",
      "LOCAL RANK=0\n",
      "NUM GPUS=1\n",
      "NODE=0\n",
      "GLOBAL RANK=0\n",
      "WORLD_SIZE=1\n",
      "PID of this process = 3015285\n",
      "device = cuda distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ks9249/.conda/envs/found/lib/python3.12/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "import gc\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from models import *\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "import schedulers\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "print(\"reached5\")\n",
    "### Multi-GPU config ###\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"Number of available CUDA devices: {device_count}\")\n",
    "\n",
    "local_rank = os.getenv('LOCAL_RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(f\"LOCAL RANK={local_rank}\")\n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(f\"NUM GPUS={num_devices}\")\n",
    "distributed = True if num_devices>1 else False\n",
    "if distributed: assert device_count==num_devices\n",
    "\n",
    "node = os.getenv('SLURM_NODEID')\n",
    "if node is None:\n",
    "    node = 0\n",
    "else:\n",
    "    node = int(node)\n",
    "print(f\"NODE={node}\")\n",
    "\n",
    "global_rank = os.getenv('RANK')\n",
    "if global_rank is None:\n",
    "    global_rank = 0\n",
    "else:\n",
    "    global_rank = int(global_rank)\n",
    "print(f\"GLOBAL RANK={global_rank}\")\n",
    "\n",
    "world_size = os.getenv('WORLD_SIZE')\n",
    "if world_size is None: \n",
    "    world_size = 1\n",
    "else:\n",
    "    world_size = int(world_size)\n",
    "print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "# batch_size = global_batch_size // num_devices\n",
    "global_batch_size = batch_size * num_devices\n",
    "\n",
    "# FSDP Setup\n",
    "if distributed:\n",
    "    import torch.distributed as dist\n",
    "    import torch.multiprocessing as mp\n",
    "    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "    from torch.distributed.fsdp.api import BackwardPrefetch, CPUOffload, ShardingStrategy\n",
    "    import functools\n",
    "    from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy, transformer_auto_wrap_policy\n",
    "    print(\"starting init_process_group...\")\n",
    "    dist.init_process_group(\"nccl\", rank=global_rank, world_size=world_size)\n",
    "    print(f\"setting device to cuda:{local_rank}\")\n",
    "    try:\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.device('cuda',local_rank)\n",
    "        print(f\"\\nSuccessfully set cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "    except Exception as error:        \n",
    "        print(f\"\\nFAILED TO SET DEVICE cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "        print(\"An exception occurred:\", error)\n",
    "        \n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "print(\"device =\", device, \"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3de5a1c-ee37-4e61-9ce6-6e2fae1dc7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'split_lr_narrowpredictor_gradientmonitor_v5', 'use_cls_token': False, 'use_contrastive_loss': False, 'constrastive_loss_weight': 1.0, 'batch_size': 4, 'num_workers': 1, 'num_epochs': 300, 'seed': 42, 'warmup': 40, 'start_lr': 0.0002, 'lr': 0.000625, 'final_lr': 1e-06, 'num_samples_per_epoch': 1024, 'ema': [0.998, 1.0], 'ipe_scale': 1.25, 'reg_coeff': 0, 'ckpt_saving': True, 'ckpt_interval': 5, 'resume_from_ckpt': False, 'wandb_log': True, 'x_encoder_start_masking_ratio': 0.85, 'x_encoder_end_masking_ratio': 0.85, 'y_encoder_mask_ratio': 0.85, 'use_rope_emb': False, 'masking_strategy': 'MNI', 'depth': 12, 'dim': 512, 'patch_size': 8, 'frame_patch_size': 4, 'embed_dim': 384, 'mlp_dim': 1536, 'num_heads': 12, 'dim_head': 32, 'image_size': [88, 104, 72], 'num_frames': 4, 'train_urls': ['/scratch/gpfs/ks9249/nsdfoundation/wds/{000005..000099}.tar'], 'test_urls': '/scratch/gpfs/ks9249/nsdfoundation/wds/{000000..000004}.tar', 'is_s3': False}\n",
      "outdir /scratch/gpfs/ks9249/fMRI-foundation-model/ckpts/split_lr_narrowpredictor_gradientmonitor_v5\n",
      "global_batch_size 4\n",
      "use_cls_token False\n",
      "num_patches 5148\n",
      "num_encoder_patches 772\n",
      "num_decoder_patches 772\n"
     ]
    }
   ],
   "source": [
    "print(config)\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "if use_contrastive_loss:\n",
    "    global_batch_size = global_batch_size // 2 # contrastive loss doubles the batch size with the same samples and different masks\n",
    "print(\"global_batch_size\", global_batch_size)\n",
    "\n",
    "use_cls_token = True if use_contrastive_loss else use_cls_token\n",
    "print(\"use_cls_token\", use_cls_token)\n",
    "\n",
    "num_patches = int(\n",
    "    (image_size[0] / patch_size)\n",
    "    * (image_size[1] / patch_size)\n",
    "    * (image_size[2] / patch_size)\n",
    "    * num_frames\n",
    ")\n",
    "num_patches_per_timepoint = num_patches // num_frames\n",
    "num_encoder_patches = int(num_patches_per_timepoint * (1 - x_encoder_start_masking_ratio) * num_frames)\n",
    "num_decoder_patches = int(num_patches_per_timepoint * (1 - y_encoder_mask_ratio) * num_frames)\n",
    "print(\"num_patches\", num_patches)\n",
    "print(\"num_encoder_patches\", num_encoder_patches)\n",
    "print(\"num_decoder_patches\", num_decoder_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee5be68-d850-42e9-a693-b78f22a1d19b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test to Check Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0915c828-19d2-49a5-b7bc-b170fcfaecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_encoder\n",
      "param counts:\n",
      "22,067,584 total\n",
      "22,067,584 trainable\n",
      "22067584\n",
      "y_encoder\n",
      "param counts:\n",
      "22,067,584 total\n",
      "22,067,584 trainable\n",
      "22067584\n",
      "predictor\n",
      "param counts:\n",
      "11,430,016 total\n",
      "11,430,016 trainable\n",
      "11430016\n"
     ]
    }
   ],
   "source": [
    "image_depth, image_height, image_width = image_size\n",
    "image_patch_size=(patch_size,patch_size,patch_size)\n",
    "patch_depth, patch_height, patch_width = image_patch_size\n",
    "patch_dim = patch_depth * patch_height * patch_width * frame_patch_size\n",
    "\n",
    "x_encoder = Transformer(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    patch_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"x_encoder\")\n",
    "print(utils.count_params(x_encoder))\n",
    "y_encoder = Transformer(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    patch_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"y_encoder\")\n",
    "print(utils.count_params(y_encoder))\n",
    "predictor = Transformer(\n",
    "    embed_dim,\n",
    "    depth // 2,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    patch_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"predictor\")\n",
    "print(utils.count_params(predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b543f5a-65a9-4294-b708-b5ec67dd0fff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleViT(\n\u001b[1;32m      2\u001b[0m     x_encoder\u001b[38;5;241m=\u001b[39mx_encoder,\n\u001b[1;32m      3\u001b[0m     y_encoder\u001b[38;5;241m=\u001b[39my_encoder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     use_cls_token\u001b[38;5;241m=\u001b[39muse_cls_token,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m utils\u001b[38;5;241m.\u001b[39mcount_params(model)\n",
      "File \u001b[0;32m~/.conda/envs/found/lib/python3.12/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/found/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/found/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/found/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/found/lib/python3.12/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.conda/envs/found/lib/python3.12/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/found/lib/python3.12/site-packages/torch/cuda/__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    301\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    306\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "model = SimpleViT(\n",
    "    x_encoder=x_encoder,\n",
    "    y_encoder=y_encoder,\n",
    "    predictor=predictor,\n",
    "    image_size=image_size, \n",
    "    image_patch_size=image_patch_size, \n",
    "    num_frames=num_frames,\n",
    "    frame_patch_size=frame_patch_size,\n",
    "    channels=1,\n",
    "    use_rope_emb=use_rope_emb,\n",
    "    use_cls_token=use_cls_token,\n",
    ")\n",
    "model = model.to(device)\n",
    "utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "478ba6bb-150a-498f-98fb-29deef53389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_transform = utils.DataPrepper(\n",
    "    num_frames=num_frames,\n",
    "    masking_strategy=masking_strategy,\n",
    "    patch_depth=patch_size,\n",
    "    patch_height=patch_size,\n",
    "    patch_width=patch_size,\n",
    "    frame_patch_size=frame_patch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f0441c-1e4d-40d2-b2fc-f0dc560e0a56",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce27aac-53e0-4960-a19a-edb634b51d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import create_dataset, create_loader\n",
    "\n",
    "if not is_s3:\n",
    "    print(train_urls)\n",
    "    if distributed: dist.barrier()\n",
    "    train_dp = create_dataset(train_urls, \n",
    "                              is_s3=is_s3, \n",
    "                              sample_shuffle=1, shard_shuffle=100)\n",
    "    train_dl = create_loader(train_dp, batch_size=batch_size, num_workers=num_workers)\n",
    "else:\n",
    "    print(\"Dataloading from s3!\")\n",
    "    train_urls = s3_train_urls\n",
    "    print(train_urls)\n",
    "    train_dp = create_dataset(train_urls, \n",
    "                              is_s3=is_s3, \n",
    "                              sample_shuffle=1, shard_shuffle=100)\n",
    "    train_dl = create_loader(train_dp, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e3be1-6075-42d4-9171-0b2350883f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not distributed:\n",
    "    num_it = 2\n",
    "    print(f\"Yielding {num_it} batches\")\n",
    "    \n",
    "    for i, batch in enumerate(train_dl):\n",
    "        print(\"iter\",i)\n",
    "        input_func = batch['func.npy']\n",
    "        if i >= (num_it-1):\n",
    "            break\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    print(\"input_func\", input_func.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d66a5ce1-6dd1-42c9-82c8-d402e7317ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iterations_per_epoch 256\n",
      "total_steps 76800\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "55,565,568 total\n",
      "33,497,984 trainable\n",
      "\n",
      "momentum_scheduler set\n"
     ]
    }
   ],
   "source": [
    "parameters = [\n",
    "    {'params': (p for n, p in model.x_encoder.named_parameters() if ('bias' not in n) and (len(p.shape) != 1)), 'lr': 0.00003},\n",
    "    {'params': (p for n, p in model.x_encoder.named_parameters() if ('bias' in n) or (len(p.shape) == 1)), \n",
    "     'WD_exclude': True,'weight_decay': 0,'lr': 0.00003},\n",
    "        {'params': (p for n, p in model.predictor.named_parameters() if ('bias' not in n) and (len(p.shape) != 1)),'lr': 0.0003},\n",
    "    {'params': (p for n, p in model.predictor.named_parameters() if ('bias' in n) or (len(p.shape) == 1)), \n",
    "     'WD_exclude': True,'weight_decay': 0,'lr': 0.0003},\n",
    "]\n",
    "\n",
    "    \n",
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        utils.trunc_normal_(m.weight, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, torch.nn.LayerNorm):\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        torch.nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "for m in model.x_encoder.modules():\n",
    "    init_weights(m)\n",
    "for m in  model.predictor.modules():\n",
    "    init_weights(m)\n",
    "\n",
    "model.y_encoder = copy.deepcopy(model.x_encoder)\n",
    "\n",
    "for p in model.y_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "    \n",
    "#optimizer = torch.optim.AdamW(opt_grouped_parameters)\n",
    "#predictor_optimizer = torch.optim.AdamW(predictor_parameters,lr=0.0003)\n",
    "optimizer = torch.optim.AdamW(parameters)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // global_batch_size\n",
    "print(\"num_iterations_per_epoch\", num_iterations_per_epoch)\n",
    "total_steps = num_epochs * num_iterations_per_epoch * num_devices\n",
    "print(\"total_steps\", total_steps)\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)\n",
    "\n",
    "momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(total_steps*ipe_scale)\n",
    "                          for i in range(int(total_steps*ipe_scale)+1))\n",
    "count=0\n",
    "print(\"\\nmomentum_scheduler set\")\n",
    "\n",
    "#lr_scheduler = schedulers.WarmupCosineSchedule(\n",
    "        #optimizer,\n",
    "        #warmup_steps=int(warmup * num_iterations_per_epoch),\n",
    "        #start_lr=start_lr,\n",
    "        #ref_lr=lr,\n",
    "        #final_lr=final_lr,\n",
    "        #T_max=int(ipe_scale * num_epochs * num_iterations_per_epoch),\n",
    "# )\n",
    "#print(\"\\nlr_scheduler set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0f425-076c-46e4-9184-4475a44dad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if masking_strategy==\"MNI\":\n",
    "    from einops.layers.torch import Rearrange\n",
    "    \n",
    "    MNI_brain = nib.load(\"/scratch/gpfs/ks9249/fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "    brain_pos_voxels = MNI_brain[6:94,8:112,10:82]\n",
    "    #brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "    #brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "    brain_pos_pats = Rearrange(\n",
    "            \"b c (f pf) (d pd) (h ph) (w pw) -> b f d h w (pd ph pw pf c)\",\n",
    "            pd=patch_size,\n",
    "            ph=patch_size,\n",
    "            pw=patch_size,\n",
    "            pf=1,\n",
    "        )(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "    \n",
    "    brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233c03de-3475-4ffa-872b-ca6d9c98103d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_ckpt_path = outdir+f'/last.pth'\n",
    "\n",
    "def save_ckpt(model,tag=\"last\"):\n",
    "    if distributed: dist.barrier()\n",
    "    model_states = model.state_dict()\n",
    "    if global_rank == 0:\n",
    "        ckpt_path = outdir+f'/{tag}.pth'\n",
    "        os.makedirs(outdir,exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_states,\n",
    "            #'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, ckpt_path)\n",
    "        print(f\"\\n---saved {ckpt_path}!---\\n\")\n",
    "        # save config.yaml copy\n",
    "        with open(f'{outdir}/config.yaml', 'w') as file:\n",
    "            yaml.dump(config, file)\n",
    "\n",
    "def resume_ckpt(model, optimizer, device, ckpt_path=default_ckpt_path):\n",
    "    if global_rank == 0:\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        #lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        epoch = 0\n",
    "    if distributed: dist.barrier()\n",
    "    torch.cuda.empty_cache()\n",
    "    return model, optimizer, lr_scheduler, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e45f79-e050-4173-b42d-005fc66a2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"]=\"offline\"\n",
    "\n",
    "if utils.is_interactive():\n",
    "    wandb_log = False\n",
    "if global_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"depth\": depth,\n",
    "      \"mlp_dim\": mlp_dim,\n",
    "      \"x_encoder_start_masking_ratio\": x_encoder_start_masking_ratio,\n",
    "      \"x_encoder_end_masking_ratio\": x_encoder_end_masking_ratio,\n",
    "      \"y_encoder_mask_ratio\": y_encoder_mask_ratio,\n",
    "      \"num_frames\": num_frames,\n",
    "      \"patch_size\": patch_size,\n",
    "      \"frame_patch_size\": frame_patch_size,\n",
    "      \"use_contrastive_loss\": use_contrastive_loss,\n",
    "      \"use_cls_token\": use_cls_token,\n",
    "      \"constrastive_loss_weight\": constrastive_loss_weight,\n",
    "      \"num_params\": num_params,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_urls\": train_urls,\n",
    "      \"is_s3\": is_s3,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5102a6b4-8587-4121-b422-a8232ea9898c",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7ed4a-2abf-42af-b1ca-798c703d00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "lrs, recon_losses, contrastive_losses, train_reg, grad_norms_encoder, grad_norms_predictor, train_cos = [], [], [], [], [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a97f07-9781-45e3-bb49-76a667a3dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume_from_ckpt is True:\n",
    "    if os.path.exists(default_ckpt_path):\n",
    "        print(f\"Resuming from {default_ckpt_path}...\")\n",
    "        model, optimizer, lr_scheduler, epoch = resume_ckpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e4373-0b1b-444d-8b81-749e387096a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(z, h):\n",
    "    loss = 0.\n",
    "    # Compute loss and accumulate for each mask-enc/mask-pred pair\n",
    "    for zi, hi in zip(z, h):\n",
    "        loss += torch.mean(torch.abs(zi - hi)**loss_exp) / loss_exp\n",
    "    loss /= len(z)\n",
    "    return loss\n",
    "\n",
    "def reg_fn(z):\n",
    "    return sum([torch.sqrt(zi.var(dim=1) + 0.0001) for zi in z]) / len(z)\n",
    "\n",
    "def cos_fn(z):\n",
    "    # Flatten z from [batch, channels, height, width] to [batch, -1]\n",
    "    z_flat = z.flatten(1)\n",
    "    # Compute normalized vectors to prevent division by zero\n",
    "    z_norm = torch.nn.functional.normalize(z_flat, p=2, dim=1)\n",
    "    # Compute cosine similarity matrix\n",
    "    cossim_matrix = torch.mm(z_norm, z_norm.transpose(0, 1))\n",
    "    # Compute the sum of cosine similarities of distinct pairs\n",
    "    cossim_sum = (cossim_matrix.sum() - len(z))/2\n",
    "    # Compute the total number of distinct pairs\n",
    "    cossim_num = len(z) * (len(z) - 1) / 2\n",
    "    # Return the average cosine similarity for distinct pairs\n",
    "    return cossim_sum / cossim_num\n",
    "\n",
    "l1 = nn.L1Loss() #Following VJEPA architecture, which uses L1 loss not L2 loss\n",
    "\n",
    "if use_contrastive_loss:\n",
    "    logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))  # learned logit scale\n",
    "    \n",
    "if distributed: dist.barrier()\n",
    "progress_bar = tqdm(range(epoch, num_epochs), disable=global_rank!=0)\n",
    "for epoch in progress_bar:\n",
    "    # get the masking ratio for the current epoch\n",
    "    tube_mask_ratio = utils.get_masking_ratio(\n",
    "        current_epoch=epoch, \n",
    "        total_epochs=num_epochs, \n",
    "        start_masking_ratio=x_encoder_start_masking_ratio, \n",
    "        end_masking_ratio=x_encoder_end_masking_ratio\n",
    "    )\n",
    "    with torch.cuda.amp.autocast(dtype=data_type):\n",
    "        model.train()\n",
    "        for train_i, batch in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_func = batch['func.npy']\n",
    "            \n",
    "            if masking_strategy==\"MNI\":\n",
    "                func, _ = aug_transform(input_func)\n",
    "            else:\n",
    "                func, brain_pos_voxels = aug_transform(input_func)\n",
    "                brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "                brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "                \n",
    "            if use_contrastive_loss:  # create positive pairs by duplicating the batch\n",
    "                func = torch.cat([func, func], dim=0)\n",
    "                meansd = torch.cat([meansd, meansd], dim=0)\n",
    "                brain_pos_pats = torch.cat([brain_pos_pats, brain_pos_pats], dim=0)\n",
    "                \n",
    "            func = func.unsqueeze(1).to(device)\n",
    "\n",
    "            # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "            \n",
    "            tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "            mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "            mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "            tube_idx = mask_idx_candidates[:int(num_patches / num_frames * (1 - tube_mask_ratio))]\n",
    "            tube_mask[tube_idx] = True\n",
    "            tube_mask = tube_mask.tile(num_frames//frame_patch_size)\n",
    "            # print(\"before encoder\");utils.print_cuda_memory_usage()\n",
    "            \n",
    "            # feed into x-encoder\n",
    "            xencoder_out = model(func, encoder_mask=tube_mask, encoder_type = \"x\", device = device)\n",
    "            # print(\"x_encoder\");utils.print_cuda_memory_usage()\n",
    "            \n",
    "            # feed entire func into y-encoder\n",
    "            with torch.no_grad():\n",
    "                yencoder_out = model(func, encoder_mask=tube_mask, encoder_type = \"y\", device = device)\n",
    "            # print(\"y_encoder\");utils.print_cuda_memory_usage()\n",
    "            \n",
    "            # feed output of x-encoder into predictor\n",
    "            predictor_out = model(xencoder_out, encoder_mask=tube_mask, encoder_type=\"p\", device = device)\n",
    "            # print(\"predictor\");utils.print_cuda_memory_usage()\n",
    "\n",
    "\n",
    "            # compare output of predictor to output of y-encoder and calculate L1 Loss\n",
    "            loss = l1(predictor_out,yencoder_out)  # jepa prediction loss\n",
    "            loss_cos = cos_fn(yencoder_out)   # cosine similarity accross batch\n",
    "\n",
    "            \n",
    "            # contrastive loss\n",
    "            if use_contrastive_loss:\n",
    "                n_b = len(func) // 2\n",
    "                cls_token1 = enc_cls_token[:n_b, 0, :]  # first half of batch, cls_token shape B, 1, d_model\n",
    "                cls_token2 = enc_cls_token[n_b:, 0, :]\n",
    "                contrastive_loss = utils.contrastive_loss(cls_token1, cls_token2, temperature=logit_scale)\n",
    "                loss += constrastive_loss_weight * contrastive_loss\n",
    "                contrastive_losses.append(contrastive_loss.item())\n",
    "\n",
    "            \n",
    "\n",
    "            if train_i==0 and epoch==0:\n",
    "                print(\"calculated first loss\")\n",
    "            if train_i==1 and epoch==0:\n",
    "                print(\"reached train_i=1\")\n",
    "            if train_i==0 and epoch==1:\n",
    "                print(\"reached epoch1\")\n",
    "            \n",
    "            # backwards + step\n",
    "            loss.backward()\n",
    "            # clip gradient\n",
    "            #torch.nn.utils.clip_grad_norm_(model.x_encoder.parameters(), 1)\n",
    "            #torch.nn.utils.clip_grad_norm_(model.predictor.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            \n",
    "            #lr_scheduler.step()\n",
    "\n",
    "            \n",
    "            recon_losses.append(loss.item())\n",
    "            #lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "            train_cos.append(loss_cos.item())\n",
    "\n",
    "            #calculate gradient norms to monitor for instability\n",
    "            grads_encoder = [\n",
    "                param.grad.detach().flatten()\n",
    "                for param in model.x_encoder.parameters()\n",
    "                if param.grad is not None\n",
    "            ]\n",
    "            grads_predictor = [\n",
    "                param.grad.detach().flatten()\n",
    "                for param in model.predictor.parameters()\n",
    "                if param.grad is not None\n",
    "            ]\n",
    "            norm_encoder = torch.cat(grads_encoder).norm()\n",
    "            norm_predictor = torch.cat(grads_predictor).norm()\n",
    "            grad_norms_encoder.append(norm_encoder.item())\n",
    "            grad_norms_predictor.append(norm_predictor.item())\n",
    "                \n",
    "            # update y-encoder using exponential-moving average of x-encoder params to prevent collapse\n",
    "            m = next(momentum_scheduler)\n",
    "            with torch.no_grad():\n",
    "                for param_q, param_k in zip(model.x_encoder.parameters(), model.y_encoder.parameters()):\n",
    "                    param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
    "\n",
    "            if train_i==0 and epoch==0:\n",
    "                print(\"finished first epoch!\")\n",
    "\n",
    "            if train_i >= (num_iterations_per_epoch-1):\n",
    "                print(\"train_i\", train_i, \"local_rank\", local_rank, \"global_rank\", global_rank)\n",
    "                break\n",
    "\n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(recon_losses[-(train_i + 1) :]),\n",
    "            \"train/num_steps\": len(recon_losses),\n",
    "            #\"lr\": np.mean(lrs[-(train_i + 1) :]),\n",
    "            \"epoch\": epoch,\n",
    "            \"tube_mask_ratio\": tube_mask_ratio,\n",
    "            #\"train/loss_reg\": np.mean(train_reg[-(train_i + 1) :]),\n",
    "            \"train/loss_cos\": np.mean(train_cos[-(train_i + 1) :]),\n",
    "            \"grad_norm_encoder\": np.mean(grad_norms_encoder[-(train_i + 1) :]),\n",
    "            \"grad_norm_predictor\": np.mean(grad_norms_predictor[-(train_i + 1) :]),\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        if distributed: print(logs)\n",
    "            \n",
    "        if global_rank==0:\n",
    "            if wandb_log: wandb.log(logs)\n",
    "                    \n",
    "        # Save model checkpoint\n",
    "        if (ckpt_saving) and ((epoch % ckpt_interval == 0) or (epoch==num_epochs-1)):\n",
    "            save_ckpt(model,\"last\")\n",
    "            \n",
    "        # wait for other GPUs to catch up if needed\n",
    "        if distributed: dist.barrier()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "if distributed:\n",
    "    dist.barrier()\n",
    "    dist.destroy_process_group()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
