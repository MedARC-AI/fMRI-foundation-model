{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0ba2f3-4e5b-4d03-89b4-9b6868536f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CUDA devices: 1\n",
      "LOCAL RANK=0\n",
      "NUM GPUS=1\n",
      "NODE=0\n",
      "GLOBAL RANK=0\n",
      "WORLD_SIZE=1\n",
      "NOT distributed\n",
      "PID of this process = 2466608\n",
      "device = cuda distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "import gc\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from models import *\n",
    "from mindeye_models import *\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "# from accelerate import Accelerator\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "### Multi-GPU config ###\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"Number of available CUDA devices: {device_count}\")\n",
    "\n",
    "local_rank = os.getenv('LOCAL_RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(f\"LOCAL RANK={local_rank}\")\n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(f\"NUM GPUS={num_devices}\")\n",
    "distributed = True if num_devices>1 else False\n",
    "if distributed: assert device_count==num_devices\n",
    "\n",
    "node = os.getenv('SLURM_NODEID')\n",
    "if node is None:\n",
    "    node = 0\n",
    "else:\n",
    "    node = int(node)\n",
    "print(f\"NODE={node}\")\n",
    "\n",
    "global_rank = os.getenv('RANK')\n",
    "if global_rank is None:\n",
    "    global_rank = 0\n",
    "else:\n",
    "    global_rank = int(global_rank)\n",
    "print(f\"GLOBAL RANK={global_rank}\")\n",
    "\n",
    "world_size = os.getenv('WORLD_SIZE')\n",
    "if world_size is None: \n",
    "    world_size = 1\n",
    "else:\n",
    "    world_size = int(world_size)\n",
    "print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# FSDP Setup\n",
    "if distributed:\n",
    "    import torch.distributed as dist\n",
    "    import torch.multiprocessing as mp\n",
    "    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "    from torch.distributed.fsdp.api import BackwardPrefetch, CPUOffload, ShardingStrategy\n",
    "    import functools\n",
    "    from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\n",
    "    print(\"starting init_process_group...\")\n",
    "    dist.init_process_group(\"nccl\", rank=global_rank, world_size=world_size)\n",
    "    print(f\"setting device to cuda:{local_rank}\")\n",
    "    try:\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.cuda.current_device() #torch.device('cuda',local_rank)\n",
    "        print(f\"\\nSuccessfully set cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "    except Exception as error:        \n",
    "        print(f\"\\nFAILED TO SET DEVICE cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "        print(\"An exception occurred:\", error)\n",
    "    dist.barrier()\n",
    "    print(\"passed barrier\\n\")\n",
    "else:\n",
    "    print(\"NOT distributed\")\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "print(\"device =\", device, \"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed487017-385c-4e79-b005-71eda9a4c960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load VJEPA parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "\n",
    "# Load MindEye parameters from yaml config (will override any params with same name)\n",
    "mindeye_config = yaml.load(open('mindeye_config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in mindeye_config.keys():\n",
    "    globals()[attribute_name] = mindeye_config[f'{attribute_name}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0abbeb-6d4e-4359-b955-ba23a0072767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  8\n",
      "global_batch_size:  8\n"
     ]
    }
   ],
   "source": [
    "# First use \"accelerate config\" in terminal for setup\n",
    "global_batch_size = batch_size * num_devices\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "# accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "global_batch_size = batch_size * num_devices\n",
    "print(\"batch_size: \", batch_size)\n",
    "print(\"global_batch_size: \", global_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3eb3e-e718-4741-a88b-4e858efcf1ab",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b7b1894-b35e-4acc-9281-8025dddb09c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vjepa config\n",
      "\n",
      " {'model_name': 's3_jepa_small_4node_FSDP_1worker', 'use_cls_token': False, 'use_contrastive_loss': False, 'constrastive_loss_weight': 1.0, 'batch_size': 1, 'num_workers': 1, 'num_epochs': 200, 'seed': 42, 'max_lr': 3e-05, 'num_samples_per_epoch': 1024, 'ema': [0.998, 1.0], 'ipe_scale': 1.25, 'ckpt_saving': True, 'ckpt_interval': 50, 'resume_from_ckpt': False, 'wandb_log': True, 'x_encoder_start_masking_ratio': 0.95, 'x_encoder_end_masking_ratio': 0.95, 'y_encoder_mask_ratio': 0.95, 'patch_size': 8, 'frame_patch_size': 1, 'use_rope_emb': False, 'masking_strategy': 'MNI', 'embed_dim': 384, 'mlp_dim': 1536, 'depth': 12, 'num_heads': 6, 'dim_head': 64, 'image_size': [88, 104, 72], 'num_frames': 4, 'is_s3': True, 'train_urls': ['/scratch/fmri_foundation_datasets/NSD_MNI_wds/{000000..000494}.tar', '/scratch/fmri_foundation_datasets/NSD_MNI_wds/{000496..000740}.tar'], 's3_train_urls': ['s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_wds/{000000..000494}.tar', 's3://proj-fmri/fmri_foundation_datasets/NSD_MNI_wds/{000496..000740}.tar']}\n",
      "mindeye_config\n",
      " {'model_name': 'jepa_NOpretrain_bs8_xc', 'mae_model_name': 's3_jepa_mini_4gpu_b2', 'batch_size': 8, 'num_workers': 10, 'mixed_precision': 'fp16', 'num_epochs': 50, 'seed': 42, 'max_lr': 0.0003, 'ckpt_saving': False, 'ckpt_interval': 50, 'resume_from_ckpt': False, 'wandb_log': True, 'in_dim': 1236480, 'hidden_dim': 512, 'drop': 0.15, 'mixup_pct': 0.33, 'nsd_wds_path': '/weka/proj-fmri/shared/mindeyev2_dataset/wds', 'nsd_raw_path': '/weka/proj-fmri/shared/mindeyev2_dataset', 'nsd_image_path': '/weka/proj-fmri/shared/mindeyev2_dataset', 'num_sessions': 5}\n",
      "outdir /weka/proj-fmri/paulscotti/fMRI-foundation-model/ckpts/s3_jepa_small_4node_FSDP_1worker\n",
      "num_patches 5148\n"
     ]
    }
   ],
   "source": [
    "print(\"vjepa config\\n\\n\",config)\n",
    "print(\"mindeye_config\\n\",mindeye_config)\n",
    "\n",
    "if utils.is_interactive():\n",
    "    ckpt_saving = False\n",
    "    # wandb_log = False\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{config[\"model_name\"]}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "num_patches = int(\n",
    "    (image_size[0] / patch_size)\n",
    "    * (image_size[1] / patch_size)\n",
    "    * (image_size[2] / patch_size)\n",
    "    * num_frames\n",
    ")\n",
    "print(\"num_patches\", num_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441c6fcd-1d17-4283-8c97-e8a10b932136",
   "metadata": {},
   "source": [
    "# Load VJEPA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03af316-fea0-4b50-b5ac-2ccdd3d08178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_encoder\n",
      "param counts:\n",
      "21,275,904 total\n",
      "21,275,904 trainable\n",
      "21275904\n",
      "y_encoder\n",
      "param counts:\n",
      "21,275,904 total\n",
      "21,275,904 trainable\n",
      "21275904\n",
      "predictor\n",
      "param counts:\n",
      "21,275,904 total\n",
      "21,275,904 trainable\n",
      "21275904\n"
     ]
    }
   ],
   "source": [
    "image_depth, image_height, image_width = image_size\n",
    "image_patch_size=(patch_size,patch_size,patch_size)\n",
    "patch_depth, patch_height, patch_width = image_patch_size\n",
    "x_encoder = Transformer(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"x_encoder\")\n",
    "print(utils.count_params(x_encoder))\n",
    "y_encoder = Transformer(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"y_encoder\")\n",
    "print(utils.count_params(y_encoder))\n",
    "predictor = Transformer(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"predictor\")\n",
    "print(utils.count_params(predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4278478f-378f-42cb-91bf-eae684ac997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "64,026,880 total\n",
      "64,026,880 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64026880"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleViT(\n",
    "    x_encoder=x_encoder,\n",
    "    y_encoder=y_encoder,\n",
    "    predictor=predictor,\n",
    "    image_size=image_size, \n",
    "    image_patch_size=image_patch_size, \n",
    "    num_frames=num_frames,\n",
    "    frame_patch_size=frame_patch_size,\n",
    "    channels=1,\n",
    "    use_rope_emb=use_rope_emb,\n",
    "    use_cls_token=use_cls_token,\n",
    ")\n",
    "utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212b974d-90b0-4d4b-888d-1137f6e26861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load from ckpt\n",
    "# mae_ckpt_pth = os.path.abspath(f'../ckpts/{mae_model_name}/last.pth')\n",
    "# print(\"mae_ckpt_pth\", mae_ckpt_pth)\n",
    "\n",
    "# checkpoint = torch.load(mae_ckpt_pth)\n",
    "# state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b845aa6d-a8b6-4f71-a029-ebb1429d3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set foundation model to evaluation\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd412c-ac9e-482a-8c13-5e0a7967ca8a",
   "metadata": {},
   "source": [
    "# Setup MindEye Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f2266c-8278-470c-90fa-6aa77eb284f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nsddata_raw_stimuli = pd.read_csv(f\"{nsd_raw_path}/nsddata_rawdata.csv\")\n",
    "TR_delay = 3 # to account for bold hrf\n",
    "train_TRs = np.round(nsddata_raw_stimuli[nsddata_raw_stimuli['shared1000'] == False]['global_TR_onsets'].values + TR_delay).astype(np.int32)\n",
    "test_TRs = np.round(nsddata_raw_stimuli[nsddata_raw_stimuli['shared1000'] == True]['global_TR_onsets'].values + TR_delay).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "369253a2-ba58-4362-a599-d5b2b21a2c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images! torch.Size([73000, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{nsd_image_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:] \n",
    "images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images!\", images.shape)\n",
    "\n",
    "# Load MindEye hdf5\n",
    "f = h5py.File(f'{nsd_raw_path}/subj01_mnidata.h5', 'r') #subj01_rawdata_old.h5\n",
    "mindeye_global_trs = f['global_trs'][:]\n",
    "mindeye_funcs = f['funcs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6ccbc02-1111-4e02-8809-68c319a1d037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=\"ViT-bigG-14\",\n",
    "    version=\"laion2b_s39b_b160k\",\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "clip_img_embedder.to(device)\n",
    "clip_seq_dim, clip_emb_dim = 256, 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce30ef3-e898-412e-a6d7-2ab002dcf9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iterations_per_epoch:  468\n",
      "Training with 5 sessions\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/train/{0..4}.tar\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/new_test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subj = s = 1\n",
    "subj_list = [subj]\n",
    "\n",
    "num_samples_per_epoch = (750*num_sessions) // num_devices\n",
    "num_iterations_per_epoch = num_samples_per_epoch // batch_size\n",
    "print(\"num_iterations_per_epoch: \", num_iterations_per_epoch)\n",
    "train_data = {}\n",
    "train_dl = {}\n",
    "\n",
    "print(f\"Training with {num_sessions} sessions\")\n",
    "train_url = f\"{nsd_wds_path}/subj0{s}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "print(train_url)\n",
    "    \n",
    "train_data[f'subj0{s}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=utils.my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "# train_dl[f'subj0{s}'] = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "train_dl[f'subj0{s}'] = wds.WebLoader(\n",
    "    train_data[f'subj0{s}'].batched(batch_size), \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    batch_size=None,\n",
    "    num_workers=num_workers, \n",
    "    persistent_workers=num_workers>0,\n",
    ").with_epoch(num_iterations_per_epoch)\n",
    "\n",
    "if global_rank==0:\n",
    "    print(\"Loaded all subj train dls and betas!\\n\")\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{nsd_wds_path}/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "    print(test_url)\n",
    "    test_data = wds.WebDataset(test_url,resampled=True,nodesplitter=utils.my_split_by_node)\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    # test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "    test_dl = wds.WebLoader(\n",
    "        test_data.batched(num_test),\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        batch_size=None,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=num_workers>0,\n",
    "    ).with_epoch(10)\n",
    "    print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60f49b37-0aa8-47f1-987f-e2f53000a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate.state import AcceleratorState\n",
    "# try:\n",
    "#     AcceleratorState().deepspeed_plugin.deepspeed_config['train_micro_batch_size_per_gpu'] = global_batch_size\n",
    "#     print(\"deepspeed reconfigured, train_micro_batch_size_per_gpu = \", global_batch_size)\n",
    "# except:\n",
    "#     print(\"skipping deepspeed reconfiguration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0444190e-33ff-4903-bdf2-0845f450d04a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "633,078,272 total\n",
      "633,078,272 trainable\n",
      "param counts:\n",
      "228,956,824 total\n",
      "228,956,824 trainable\n",
      "param counts:\n",
      "862,035,096 total\n",
      "862,035,096 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "862035096"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mindeye = MindEyeModule()\n",
    "mindeye.ridge = RidgeRegression(np.array([in_dim]), out_features=hidden_dim)\n",
    "mindeye.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, n_blocks=4, drop=drop,\n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim, clip_scale=1)\n",
    "utils.count_params(mindeye.ridge)\n",
    "utils.count_params(mindeye.backbone)\n",
    "utils.count_params(mindeye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84127f7a-2108-4e5d-8ac6-6149b13c1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed: \n",
    "    my_auto_wrap_policy = functools.partial(\n",
    "        size_based_auto_wrap_policy, min_num_params=200000\n",
    "    )\n",
    "    print(f\"\\nPrepping FSDP on {global_rank} {node}...\\n\")\n",
    "    mindeye = FSDP(\n",
    "        mindeye,\n",
    "        sharding_strategy=ShardingStrategy.HYBRID_SHARD,\n",
    "        auto_wrap_policy=my_auto_wrap_policy,\n",
    "        use_orig_params=False,\n",
    "        cpu_offload=None, #CPUOffload(offload_params=True)\n",
    "        sync_module_states=True,\n",
    "        limit_all_gathers=True, # See https://github.com/pytorch/pytorch/issues/91165\n",
    "        device_id=device,\n",
    "    )\n",
    "    print(f\"\\nSuccessfully loaded FSDP model to device on global_rank {global_rank}\\n\")\n",
    "    dist.barrier()\n",
    "    print(f\"\\nSuccessfully passed barrier! {global_rank}\\n\")\n",
    "else:\n",
    "    mindeye=mindeye.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ad1b981-00f4-4072-8402-c0775881edd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 23400\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "64,026,880 total\n",
      "0 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in mindeye.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in mindeye.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in mindeye.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "total_steps = num_epochs * num_iterations_per_epoch\n",
    "print(\"total_steps\", total_steps)\n",
    "pct_start = 2/num_epochs if num_epochs>1 else 1.\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    ")\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55228b07-e8ae-448c-972e-800aa6dc7ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_ckpt(tag=\"last\"):\n",
    "    ckpt_path = outdir+f'/{tag}/downstream'\n",
    "    os.makedirs(ckpt_path,exist_ok=True)\n",
    "    accelerator.save_model(model, ckpt_path, max_shard_size=\"2GB\", safe_serialization=True)\n",
    "    print(f\"\\n---saved {ckpt_path}!---\\n\")\n",
    "        \n",
    "def save_progress(tag=\"last\"):\n",
    "    if accelerator.is_main_process:\n",
    "        ckpt_path = outdir+f'/{tag}/downstream'\n",
    "        torch.save(\n",
    "                {\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"scheduler\": lr_scheduler.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"losses\": losses,\n",
    "                    \"test_losses\": test_losses,\n",
    "                    \"lrs\": lrs,\n",
    "                },\n",
    "                os.path.join(ckpt_path, f\"params.pt\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d290b05d-9985-4a68-919e-2baeccc4af21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb found_downstream run jepa_NOpretrain_bs8_xc\n",
      "wandb_config:\n",
      " {'model_name': 'jepa_NOpretrain_bs8_xc', 'mae_model_name': 's3_jepa_mini_4gpu_b2', 'global_batch_size': 8, 'batch_size': 8, 'num_epochs': 50, 'num_sessions': 5, 'num_samples_per_epoch': 3750, 'in_dim': 1236480, 'hidden_dim': 512, 'mixup_pct': 0.33, 'num_params': 0, 'max_lr': 0.0003, 'ckpt_interval': 50, 'ckpt_saving': False, 'seed': 42, 'distributed': False, 'num_devices': 1, 'world_size': 1, 'train_url': '/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/train/{0..4}.tar', 'test_url': '/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/new_test/0.tar'}\n",
      "wandb_id: jepa_NOpretrain_bs8_xc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaul-scotti\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/weka/proj-fmri/paulscotti/MindEyeV2/wandb/run-20240320_005146-jepa_NOpretrain_bs8_xc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/paul-scotti/found_downstream/runs/jepa_NOpretrain_bs8_xc' target=\"_blank\">jepa_NOpretrain_bs8_xc</a></strong> to <a href='https://wandb.ai/paul-scotti/found_downstream' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/paul-scotti/found_downstream' target=\"_blank\">https://wandb.ai/paul-scotti/found_downstream</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/paul-scotti/found_downstream/runs/jepa_NOpretrain_bs8_xc' target=\"_blank\">https://wandb.ai/paul-scotti/found_downstream/runs/jepa_NOpretrain_bs8_xc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if global_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found_downstream'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"mae_model_name\": mae_model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"in_dim\": in_dim,\n",
    "      \"hidden_dim\": hidden_dim,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_params\": num_params,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2a116-fc5d-4be9-8f9c-01b00618c8f9",
   "metadata": {},
   "source": [
    "# Train MindEye with Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49ee95c4-2011-4187-96e7-4fa840458507",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6c9624a-5f39-46b9-83d3-5c8288511490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1928) tensor(0.3745)\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "MNI_brain = nib.load(\"/weka/proj-fmri/paulscotti/fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "brain_pos_voxels = MNI_brain[6:94,8:112,10:82]\n",
    "brain_pos_voxels[6:94,:(112-60),10:62] = 0\n",
    "brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "tube_mask = (brain_pos_pats_vit > 0).tile(num_frames)\n",
    "print(tube_mask.sum(), tube_mask.sum() / len(tube_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0eafa9d-8e17-436e-80b0-831996536452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume from ckpt (e.g., if you are resuming from a run that got pre-empted)\n",
    "load_progress = False\n",
    "if wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "        load_progress = True\n",
    "elif resume_from_ckpt: # if resuming without using wandb\n",
    "    load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "    load_progress = True\n",
    "    \n",
    "if load_progress:\n",
    "    ckpt_path = outdir+'/last'\n",
    "    prev_params = torch.load(ckpt_path+\"/params.pt\")\n",
    "    optimizer.load_state_dict(prev_params[\"optimizer\"])\n",
    "    lr_scheduler.load_state_dict(prev_params[\"scheduler\"])\n",
    "    epoch = prev_params[\"epoch\"]\n",
    "    losses = prev_params[\"losses\"]\n",
    "    test_losses = prev_params[\"test_losses\"]\n",
    "    lrs = prev_params[\"lrs\"]\n",
    "    print(\"Loaded model params from\", ckpt_path, \"at epoch\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb27dfcf-804f-4145-8c21-73740b280920",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "# mindeye, optimizer, *train_dls, lr_scheduler = accelerator.prepare(\n",
    "#     mindeye, optimizer, *train_dls, lr_scheduler\n",
    "# )\n",
    "# # skipping test_dl because we just use local_rank=0 for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17c8d10d-63da-4ae9-8888-d7ee0db9995c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jepa_NOpretrain_bs8_xc starting with epoch 0 / 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b9563fb1f1471cba57ef14ceb28613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "\n",
      "reached start of train!\n",
      "\n",
      "\n",
      "reached start of 0 train loop!\n",
      "\n",
      "\n",
      "reached start of 0 test loop!\n",
      "\n",
      "{'train/loss': 1.9824971268829117, 'test/loss': 2.2237605094909667, 'train/lr': 1.5134250794724758e-05, 'train/num_steps': 468, 'test/num_steps': 10, 'train/fwd_pct_correct': 0.21447649572649571, 'train/bwd_pct_correct': 0.13408119658119658, 'test/test_fwd_pct_correct': 0.1, 'test/test_bwd_pct_correct': 0.125, 'train/loss_clip_total': 1.9824971268829117, 'test/loss_clip_total': 2.2237605094909667}\n",
      "epoch 1\n",
      "{'train/loss': 1.9573936194945605, 'test/loss': 2.266576814651489, 'train/lr': 2.4426744301344326e-05, 'train/num_steps': 936, 'test/num_steps': 20, 'train/fwd_pct_correct': 0.32371794871794873, 'train/bwd_pct_correct': 0.14716880341880342, 'test/test_fwd_pct_correct': 0.1125, 'test/test_bwd_pct_correct': 0.1125, 'train/loss_clip_total': 1.9573936194945605, 'test/loss_clip_total': 2.266576814651489}\n",
      "epoch 2\n",
      "{'train/loss': 1.913430614858611, 'test/loss': 2.2169209003448485, 'train/lr': 3.9471253080328574e-05, 'train/num_steps': 1404, 'test/num_steps': 30, 'train/fwd_pct_correct': 0.3346688034188034, 'train/bwd_pct_correct': 0.16052350427350429, 'test/test_fwd_pct_correct': 0.075, 'test/test_bwd_pct_correct': 0.1125, 'train/loss_clip_total': 1.913430614858611, 'test/loss_clip_total': 2.2169209003448485}\n",
      "epoch 3\n",
      "{'train/loss': 1.8430747040826032, 'test/loss': 2.2777748823165895, 'train/lr': 5.9610073225192654e-05, 'train/num_steps': 1872, 'test/num_steps': 40, 'train/fwd_pct_correct': 0.38808760683760685, 'train/bwd_pct_correct': 0.18830128205128205, 'test/test_fwd_pct_correct': 0.1375, 'test/test_bwd_pct_correct': 0.125, 'train/loss_clip_total': 1.8430747040826032, 'test/loss_clip_total': 2.2777748823165895}\n",
      "epoch 4\n",
      "{'train/loss': 1.7256352614897947, 'test/loss': 2.2821281433105467, 'train/lr': 8.396279176427633e-05, 'train/num_steps': 2340, 'test/num_steps': 50, 'train/fwd_pct_correct': 0.4425747863247863, 'train/bwd_pct_correct': 0.23504273504273504, 'test/test_fwd_pct_correct': 0.1125, 'test/test_bwd_pct_correct': 0.1125, 'train/loss_clip_total': 1.7256352614897947, 'test/loss_clip_total': 2.2821281433105467}\n",
      "epoch 5\n",
      "{'train/loss': 1.5894287755856147, 'test/loss': 2.366747760772705, 'train/lr': 0.00011146477585698752, 'train/num_steps': 2808, 'test/num_steps': 60, 'train/fwd_pct_correct': 0.4439102564102564, 'train/bwd_pct_correct': 0.28338675213675213, 'test/test_fwd_pct_correct': 0.075, 'test/test_bwd_pct_correct': 0.125, 'train/loss_clip_total': 1.5894287755856147, 'test/loss_clip_total': 2.366747760772705}\n",
      "epoch 6\n",
      "{'train/loss': 1.6464047607703087, 'test/loss': 2.3166735410690307, 'train/lr': 0.0001409137155682136, 'train/num_steps': 3276, 'test/num_steps': 70, 'train/fwd_pct_correct': 0.4198717948717949, 'train/bwd_pct_correct': 0.265491452991453, 'test/test_fwd_pct_correct': 0.0875, 'test/test_bwd_pct_correct': 0.125, 'train/loss_clip_total': 1.6464047607703087, 'test/loss_clip_total': 2.3166735410690307}\n",
      "epoch 7\n",
      "{'train/loss': 1.5497053111465569, 'test/loss': 2.3828922510147095, 'train/lr': 0.00017102218550079114, 'train/num_steps': 3744, 'test/num_steps': 80, 'train/fwd_pct_correct': 0.44177350427350426, 'train/bwd_pct_correct': 0.29967948717948717, 'test/test_fwd_pct_correct': 0.125, 'test/test_bwd_pct_correct': 0.1375, 'train/loss_clip_total': 1.5497053111465569, 'test/loss_clip_total': 2.3828922510147095}\n",
      "epoch 8\n",
      "{'train/loss': 1.5836251029092023, 'test/loss': 2.337239718437195, 'train/lr': 0.00020047392743826044, 'train/num_steps': 4212, 'test/num_steps': 90, 'train/fwd_pct_correct': 0.4206730769230769, 'train/bwd_pct_correct': 0.2844551282051282, 'test/test_fwd_pct_correct': 0.1, 'test/test_bwd_pct_correct': 0.1125, 'train/loss_clip_total': 1.5836251029092023, 'test/loss_clip_total': 2.337239718437195}\n",
      "epoch 9\n",
      "{'train/loss': 1.5231684480722134, 'test/loss': 2.340053153038025, 'train/lr': 0.000227981393477953, 'train/num_steps': 4680, 'test/num_steps': 100, 'train/fwd_pct_correct': 0.4481837606837607, 'train/bwd_pct_correct': 0.3074252136752137, 'test/test_fwd_pct_correct': 0.125, 'test/test_bwd_pct_correct': 0.1375, 'train/loss_clip_total': 1.5231684480722134, 'test/loss_clip_total': 2.340053153038025}\n",
      "epoch 10\n",
      "{'train/loss': 1.3794079494272542, 'test/loss': 2.300849723815918, 'train/lr': 0.00025234203402934446, 'train/num_steps': 5148, 'test/num_steps': 110, 'train/fwd_pct_correct': 0.5029380341880342, 'train/bwd_pct_correct': 0.3579059829059829, 'test/test_fwd_pct_correct': 0.1375, 'test/test_bwd_pct_correct': 0.1, 'train/loss_clip_total': 1.3794079494272542, 'test/loss_clip_total': 2.300849723815918}\n",
      "epoch 11\n",
      "{'train/loss': 1.34383060853196, 'test/loss': 2.3994571447372435, 'train/lr': 0.00027249086992358994, 'train/num_steps': 5616, 'test/num_steps': 120, 'train/fwd_pct_correct': 0.48050213675213677, 'train/bwd_pct_correct': 0.3659188034188034, 'test/test_fwd_pct_correct': 0.1125, 'test/test_bwd_pct_correct': 0.1125, 'train/loss_clip_total': 1.34383060853196, 'test/loss_clip_total': 2.3994571447372435}\n",
      "epoch 12\n",
      "{'train/loss': 1.3810844329050465, 'test/loss': 2.383548045158386, 'train/lr': 0.00028754705032843877, 'train/num_steps': 6084, 'test/num_steps': 130, 'train/fwd_pct_correct': 0.5069444444444444, 'train/bwd_pct_correct': 0.34428418803418803, 'test/test_fwd_pct_correct': 0.0875, 'test/test_bwd_pct_correct': 0.125, 'train/loss_clip_total': 1.3810844329050465, 'test/loss_clip_total': 2.383548045158386}\n",
      "epoch 13\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "NaN loss",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m fwd_percent_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mtopk(utils\u001b[38;5;241m.\u001b[39mprenormed_batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    116\u001b[0m bwd_percent_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mtopk(utils\u001b[38;5;241m.\u001b[39mprenormed_batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 118\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# accelerator.backward(loss)\u001b[39;00m\n",
      "File \u001b[0;32m/weka/proj-fmri/paulscotti/fMRI-foundation-model/fMRI-VJEPA/utils.py:136\u001b[0m, in \u001b[0;36mcheck_loss\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_loss\u001b[39m(loss):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss\u001b[38;5;241m.\u001b[39misnan()\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 136\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNaN loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: NaN loss"
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch, num_epochs), disable=global_rank!=0)\n",
    "mse = nn.MSELoss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "test_image=None\n",
    "num_test_eval=batch_size # should instead be 300 to mimic MindEye2 retrieval evaluation, but this leads to OOM\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    print(f\"epoch {epoch}\")\n",
    "    mindeye.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    loss_clip_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    annot_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    for s, train_dl in enumerate(train_dls):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            for iter, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(train_dl):\n",
    "                image0 = images[behav0[:,0,0].cpu().long()].float()\n",
    "                image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "\n",
    "                if epoch==0 and iter==0: print(\"\\nreached start of train!\\n\")\n",
    "\n",
    "                # if images are not fully preloaded, then can do this inefficient but more memory friendly approach\n",
    "                # for ib,b in enumerate(behav0[:,0,0].cpu().long()):\n",
    "                #     if ib==0:\n",
    "                #         image0 = torch.Tensor(images[[b]])\n",
    "                #     else:\n",
    "                #         image0 = torch.vstack((image0, torch.Tensor(images[[b]])))\n",
    "                # image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "                \n",
    "                # get the corresponding raw voxel time series\n",
    "                for ib,b in enumerate(behav0[:,0,5].cpu().long().numpy()):\n",
    "                    tr = (nsddata_raw_stimuli[nsddata_raw_stimuli['global_trial'].isin([b.item()])]['global_TR_onsets'].values + TR_delay).astype(np.int32).item()\n",
    "                    if ib==0:\n",
    "                        voxels_raw = mindeye_funcs[tr-2:tr+2][None][None]\n",
    "                    else:\n",
    "                        voxels_raw = np.vstack((voxels_raw, mindeye_funcs[tr-2:tr+2][None][None]))\n",
    "                voxels_raw = torch.Tensor(voxels_raw).to(device)\n",
    "                \n",
    "                ## Process it through pretrained VJEPA Y-Encoder (encodes the entire voxels_raw) ##\n",
    "                encoder_out = model(voxels_raw, encoder_mask=tube_mask, encoder_type = \"y\")\n",
    "                voxel0 = encoder_out.flatten(1).unsqueeze(1).cpu()\n",
    "                \n",
    "                assert len(voxel0) == batch_size\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                    perm_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = perm\n",
    "                    betas_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = betas\n",
    "                    select_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = select\n",
    "\n",
    "                voxel_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = voxel0\n",
    "\n",
    "                if iter >= num_iterations_per_epoch:\n",
    "                    break\n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i in range(num_iterations_per_epoch):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss=0.\n",
    "            if train_i==0 and epoch==0: print(f\"\\nreached start of {train_i} train loop!\\n\")\n",
    "\n",
    "            voxel_list = [voxel_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "            image = image_iters[train_i].detach()\n",
    "            image = image.to(device)\n",
    "\n",
    "            clip_target = clip_img_embedder(image)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                perm_list = [perm_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                perm = torch.cat(perm_list, dim=0)\n",
    "                betas_list = [betas_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                betas = torch.cat(betas_list, dim=0)\n",
    "                select_list = [select_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                select = torch.cat(select_list, dim=0)\n",
    "\n",
    "            voxel_ridge_list = [mindeye.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "            voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "            backbone, clip_voxels = mindeye.backbone(voxel_ridge)\n",
    "\n",
    "            clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "            clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):                \n",
    "                loss_clip = utils.mixco_nce(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006,\n",
    "                    perm=perm, betas=betas, select=select)\n",
    "            else:\n",
    "                epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=epoch_temp)\n",
    "\n",
    "            loss_clip_total += loss_clip.item()\n",
    "            loss += loss_clip\n",
    "\n",
    "            # forward and backward top 1 accuracy        \n",
    "            labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "            fwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "            bwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            utils.check_loss(loss)\n",
    "            loss.backward()\n",
    "            # accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    mindeye.eval()\n",
    "    if global_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):  \n",
    "                loss=0.     \n",
    "                if test_i==0 and epoch==0: print(f\"\\nreached start of {test_i} test loop!\\n\")\n",
    "\n",
    "                coco_idx = behav[:,0,0].cpu().long()\n",
    "                _,test_indices = np.unique(coco_idx, return_index=True)\n",
    "                test_indices = np.random.permutation(test_indices)[:num_test_eval]\n",
    "                image = images[coco_idx[test_indices]].float().to(device)\n",
    "                \n",
    "                # get the corresponding raw voxel time series\n",
    "                for ib,b in enumerate(behav[test_indices,0,5].cpu().long().numpy()):\n",
    "                    tr = (nsddata_raw_stimuli[nsddata_raw_stimuli['global_trial'].isin([b.item()])]['global_TR_onsets'].values + TR_delay).astype(np.int32).item()\n",
    "                    if ib==0:\n",
    "                        voxels_raw = mindeye_funcs[tr-2:tr+2][None][None]\n",
    "                    else:\n",
    "                        voxels_raw = np.vstack((voxels_raw, mindeye_funcs[tr-2:tr+2][None][None]))\n",
    "                voxels_raw = torch.Tensor(voxels_raw).to(device)\n",
    "                \n",
    "                ## Process it through pretrained MAE ##\n",
    "                encoder_out = model(voxels_raw, encoder_mask=tube_mask, encoder_type = \"y\")\n",
    "                voxel = encoder_out.flatten(1).unsqueeze(1)\n",
    "\n",
    "                assert len(image) == num_test_eval\n",
    "\n",
    "                clip_target = clip_img_embedder(image.float())\n",
    "\n",
    "                voxel_ridge = mindeye.ridge(voxel,0) # 0th index of subj_list\n",
    "                backbone, clip_voxels = mindeye.backbone(voxel_ridge)\n",
    "\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006)\n",
    "\n",
    "                test_loss_clip_total += loss_clip.item()\n",
    "                loss += loss_clip\n",
    "\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                test_fwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                test_bwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "            \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "            \"train/lr\": lrs[-1],\n",
    "            \"train/num_steps\": len(losses),\n",
    "            \"test/num_steps\": len(test_losses),\n",
    "            \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "            \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "            \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "            \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "            \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "            \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        if not distributed: print(logs)\n",
    "        if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt()\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    # accelerator.wait_for_everyone()\n",
    "    if distributed: dist.barrier()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46b8a2-ac00-489d-8ab1-18b5b590d6ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training losses\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(test_losses)\n",
    "plt.title(\"Test losses\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
