{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CUDA devices: 1\n",
      "LOCAL RANK=0\n",
      "NUM GPUS=1\n",
      "NODE=0\n",
      "GLOBAL RANK=0\n",
      "WORLD_SIZE=1\n",
      "PID of this process = 1339581\n",
      "device = cuda distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from models import *\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "\n",
    "import schedulefree\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "### Multi-GPU config ###\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"Number of available CUDA devices: {device_count}\")\n",
    "\n",
    "local_rank = os.getenv('LOCAL_RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(f\"LOCAL RANK={local_rank}\")\n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(f\"NUM GPUS={num_devices}\")\n",
    "distributed = True if num_devices>1 else False\n",
    "if distributed: assert device_count==num_devices\n",
    "\n",
    "node = os.getenv('SLURM_NODEID')\n",
    "if node is None:\n",
    "    node = 0\n",
    "else:\n",
    "    node = int(node)\n",
    "print(f\"NODE={node}\")\n",
    "\n",
    "global_rank = os.getenv('RANK')\n",
    "if global_rank is None:\n",
    "    global_rank = 0\n",
    "else:\n",
    "    global_rank = int(global_rank)\n",
    "print(f\"GLOBAL RANK={global_rank}\")\n",
    "\n",
    "world_size = os.getenv('WORLD_SIZE')\n",
    "if world_size is None: \n",
    "    world_size = 1\n",
    "else:\n",
    "    world_size = int(world_size)\n",
    "print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "# batch_size = global_batch_size // num_devices\n",
    "global_batch_size = batch_size * world_size\n",
    "\n",
    "# FSDP Setup\n",
    "if distributed:\n",
    "    import torch.distributed as dist\n",
    "    import torch.multiprocessing as mp\n",
    "    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "    from torch.distributed.fsdp.api import BackwardPrefetch, CPUOffload, ShardingStrategy\n",
    "    import functools\n",
    "    from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy, transformer_auto_wrap_policy\n",
    "    print(\"starting init_process_group...\")\n",
    "    dist.init_process_group(\"nccl\", rank=global_rank, world_size=world_size)\n",
    "    print(f\"setting device to cuda:{local_rank}\")\n",
    "    try:\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.device('cuda',local_rank)\n",
    "        print(f\"\\nSuccessfully set cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "    except Exception as error:        \n",
    "        print(f\"\\nFAILED TO SET DEVICE cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "        print(\"An exception occurred:\", error)\n",
    "        \n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "print(\"device =\", device, \"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08179db-9c6a-4bc6-a245-79fae6884ca2",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b1c3fe-28ab-40b7-8906-c6a9c8070d00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'mini_nomask_logitsCLS_downstream_40ep_l', 'use_cls_token': True, 'use_contrastive_loss': True, 'contrastive_loss_weight': 0.1, 'batch_size': 256, 'num_workers': 10, 'num_epochs': 20, 'seed': 42, 'max_lr': 3e-06, 'num_samples_per_epoch': 1024, 'test_num_samples_per_epoch': 384, 'ckpt_saving': True, 'ckpt_interval': 50, 'resume_from_ckpt': True, 'wandb_log': True, 'tube_start_masking_ratio': 0.75, 'tube_end_masking_ratio': 0.75, 'decoder_mask_ratio': 0.75, 'patch_size': [8, 8, 8], 'frame_patch_size': 4, 'use_rope_emb': False, 'masking_strategy': 'None', 'encoder_model': 'vit_mini', 'decoder_model': 'vit_mini', 'img_size': [88, 104, 72], 'num_frames': 4, 'is_s3': False, 'train_urls': ['/weka/proj-fmri/shared/NSD_MNI_wds/{000000..000699}.tar'], 'test_urls': ['/weka/proj-fmri/shared/NSD_MNI_wds/{000700..000738}.tar']}\n",
      "outdir /weka/proj-fmri/paulscotti/fMRI-foundation-model/ckpts/mini_nomask_logitsCLS_downstream_40ep_l\n",
      "global_batch_size 256\n",
      "use_cls_token True\n",
      "num_patches 5148\n",
      "num_patches_per_timepoint 1287\n",
      "num_encoder_patches 321\n",
      "num_decoder_patches 321\n"
     ]
    }
   ],
   "source": [
    "print(config)\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "os.makedirs(outdir,exist_ok=True)\n",
    "print(\"outdir\", outdir)\n",
    "print(\"global_batch_size\", global_batch_size)\n",
    "print(\"use_cls_token\", use_cls_token)\n",
    "\n",
    "if type(patch_size) == int:\n",
    "    patch_size = [patch_size,patch_size,patch_size]\n",
    "patch_depth = patch_size[0]\n",
    "patch_height = patch_size[1]\n",
    "patch_width = patch_size[2]\n",
    "\n",
    "num_patches = int(\n",
    "    (img_size[0] / patch_depth)\n",
    "    * (img_size[1] / patch_height)\n",
    "    * (img_size[2] / patch_width)\n",
    "    * num_frames\n",
    ")\n",
    "num_patches_per_timepoint = num_patches // frame_patch_size\n",
    "num_encoder_patches = int(np.floor((num_patches_per_timepoint * num_frames // frame_patch_size) * (1 - tube_start_masking_ratio)))\n",
    "num_decoder_patches = int(np.floor((num_patches_per_timepoint * num_frames  // frame_patch_size) * (1 - decoder_mask_ratio)))\n",
    "print(\"num_patches\", num_patches)\n",
    "print(\"num_patches_per_timepoint\", num_patches_per_timepoint)\n",
    "print(\"num_encoder_patches\", num_encoder_patches)\n",
    "print(\"num_decoder_patches\", num_decoder_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8419d-988f-42c6-acb6-b258e0694eee",
   "metadata": {},
   "source": [
    "# Prep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e37b6b9-5b91-4c4a-af85-ac2af69704e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "1,693,008 total\n",
      "1,693,008 trainable\n",
      "\n",
      "encoder\n",
      "torch.Size([256, 1, 4, 88, 104, 72])\n",
      "patched torch.Size([256, 1, 11, 13, 9, 2048])\n",
      "reshaped torch.Size([256, 1287, 2048])\n",
      "masked torch.Size([256, 321, 2048])\n",
      "patched_emb torch.Size([256, 321, 48])\n",
      "pe torch.Size([1287, 48])\n",
      "masked torch.Size([256, 322, 48])\n",
      "torch.Size([256, 322, 48])\n",
      "\n",
      "decoder\n",
      "torch.Size([256, 322, 48])\n",
      "pe torch.Size([1287, 48])\n",
      "pos_emd_encoder torch.Size([321, 48])\n",
      "pos_emd_decoder torch.Size([321, 48])\n",
      "x_concat torch.Size([256, 643, 48])\n",
      "torch.Size([256, 643, 48])\n",
      "proj torch.Size([256, 643, 2048])\n",
      "\n",
      "enc_cls_token torch.Size([256, 1, 48])\n",
      "encoder_patches torch.Size([256, 321, 48])\n",
      "dec_cls_token torch.Size([256, 1, 2048])\n",
      "decoder_patches torch.Size([256, 642, 2048])\n"
     ]
    }
   ],
   "source": [
    "vit_size = {\n",
    "    \"encoder\": encoder_model,\n",
    "    \"decoder\": decoder_model\n",
    "}\n",
    "    \n",
    "model = get_vit(\n",
    "    size=vit_size,\n",
    "    image_size=img_size,  # depth, height, width\n",
    "    image_patch_size=(patch_depth,patch_height,patch_width),  # depth, height, width patch size\n",
    "    frames=num_frames,\n",
    "    frame_patch_size=frame_patch_size,\n",
    "    channels=1,\n",
    "    use_rope_emb=use_rope_emb,\n",
    "    use_cls_token=use_cls_token,\n",
    ")\n",
    "utils.count_params(model)\n",
    "\n",
    "# function to select random num_frames from sample and obtain brain-positive patches\n",
    "aug_transform = utils.DataPrepper(\n",
    "    num_frames=num_frames*2,\n",
    "    masking_strategy=masking_strategy,\n",
    "    patch_depth=patch_depth,\n",
    "    patch_height=patch_height,\n",
    "    patch_width=patch_width,\n",
    "    frame_patch_size=frame_patch_size,\n",
    ")\n",
    "\n",
    "# test that the model works without error\n",
    "model = model.to(device)\n",
    "encoder_mask = torch.zeros(num_patches_per_timepoint).to(torch.bool)\n",
    "encoder_mask[:num_encoder_patches] = True\n",
    "decoder_mask = torch.zeros(num_patches_per_timepoint).to(torch.bool)\n",
    "decoder_mask[-num_decoder_patches:] = True\n",
    "decoder_mask[encoder_mask] = False\n",
    "with torch.no_grad():\n",
    "    print(\"\\nencoder\")\n",
    "    encoder_out = model(\n",
    "                torch.randn(batch_size, 1, num_frames, img_size[0], img_size[1], img_size[2]).to(device),\n",
    "                encoder_mask=encoder_mask,\n",
    "                verbose=True)\n",
    "    print(\"\\ndecoder\")\n",
    "    decoder_out = model(\n",
    "                encoder_out, \n",
    "                encoder_mask=encoder_mask, \n",
    "                decoder_mask=decoder_mask, \n",
    "                verbose=True)\n",
    "    if use_cls_token:\n",
    "        enc_cls_token = encoder_out[:, :1, :]\n",
    "        encoder_patches = encoder_out[:, 1:, :]\n",
    "        dec_cls_token = decoder_out[:, :1, :]\n",
    "        decoder_patches = decoder_out[:, 1:, :]\n",
    "        print(\"\\nenc_cls_token\", enc_cls_token.shape)\n",
    "        print(\"encoder_patches\", encoder_patches.shape)\n",
    "        print(\"dec_cls_token\", dec_cls_token.shape)\n",
    "        print(\"decoder_patches\", decoder_patches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414cb007-5552-4c5a-8fbd-9b158565770c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Add \"linear\" probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb3df855-81c4-4d2c-aff2-42a19dd0e67e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input_dim, h=256, num_classes=8):\n",
    "        super(LinearProbe, self).__init__()\n",
    "        # self.classifier = nn.Linear(input_dim, num_classes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(input_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(input_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e6a865b-8348-486e-ab88-040a87fa2ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if use_cls_token:\n",
    "#     model.cont = LinearProbe((num_encoder_patches+1)*model.encoder_embed_dim,h=768,num_classes=768)\n",
    "# else:\n",
    "# model.cont = LinearProbe(model.encoder_embed_dim,h=256,num_classes=256)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd51ddf-fb71-48f4-bdbd-88753b44d2aa",
   "metadata": {},
   "source": [
    "## Create dataset and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75dd0398-0eb8-464e-8bb8-13e7ce1a8480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from dataloader import create_dataset, create_loader\n",
    "# train_urls = train_urls[0]\n",
    "# print(train_urls)\n",
    "\n",
    "# train_dp = create_dataset(train_urls, \n",
    "#                           is_s3=train_urls[:2]==\"s3\", \n",
    "#                           sample_shuffle=100, shard_shuffle=100)\n",
    "# train_dl = create_loader(train_dp, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cadb1eb6-dc41-416b-ace5-70bd7c7d887c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/weka/proj-fmri/shared/NSD_MNI_wds/{000000..000699}.tar']\n",
      "['/weka/proj-fmri/shared/NSD_MNI_wds/{000700..000738}.tar']\n"
     ]
    }
   ],
   "source": [
    "def log_and_continue(exn):\n",
    "    \"\"\"Call in an exception handler to ignore any exception, issue a warning, and continue.\"\"\"\n",
    "    print(f'Handling webdataset error ({repr(exn)}). Ignoring.')\n",
    "    return True\n",
    "\n",
    "def filter_corrupted_images(sample):\n",
    "    \"\"\"If all the required files are not present don't use them.\"\"\"\n",
    "    correct_data = (\"func.npy\" in sample)\n",
    "    return correct_data\n",
    "\n",
    "### ================      Train Dataset and DataLoader    ====================\n",
    "from braceexpand import braceexpand\n",
    "print(train_urls)\n",
    "if is_s3:\n",
    "    expanded_urls = [f\"pipe:aws s3 cp {url} -\" for pattern in train_urls for url in braceexpand(pattern)]\n",
    "else:\n",
    "    expanded_urls = [str(url) for pattern in train_urls for url in braceexpand(pattern)]\n",
    "\n",
    "train_data = (\n",
    "    wds.WebDataset(expanded_urls, resampled=True, nodesplitter=wds.split_by_node, handler=log_and_continue)\n",
    "    .shuffle(100, initial=100, rng=random.Random(seed))\n",
    "    .select(filter_corrupted_images)\n",
    "    .decode(\"torch\")\n",
    ")\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "### ================      Test Dataset and DataLoader    ====================\n",
    "print(test_urls)\n",
    "if is_s3:\n",
    "    expanded_urls = [f\"pipe:aws s3 cp {url} -\" for pattern in test_urls for url in braceexpand(pattern)]\n",
    "else:\n",
    "    expanded_urls = [str(url) for pattern in train_urls for url in braceexpand(pattern)]\n",
    "\n",
    "test_data = (\n",
    "    wds.WebDataset(expanded_urls, resampled=True, nodesplitter=wds.split_by_node, handler=log_and_continue)\n",
    "    .shuffle(100, initial=100, rng=random.Random(seed))\n",
    "    .select(filter_corrupted_images)\n",
    "    .decode(\"torch\")\n",
    ")\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d05a32-12eb-494a-82df-dce4c7e9924c",
   "metadata": {},
   "source": [
    "### Check data loaders work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "480e7c0b-f58e-4c35-80fe-1284ee0e3966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if utils.is_interactive():\n",
    "#     start_time = time.time() \n",
    "#     num_it = 2\n",
    "#     print(f\"Yielding {num_it} batches\")\n",
    "    \n",
    "#     for i, batch in enumerate(test_dl):\n",
    "#         print(\"iter\",i)\n",
    "#         input_func = batch['func.npy']\n",
    "#         subject_id = batch['subject_id.txt']\n",
    "#         subject_id = [int(subject[-2:]) for subject in subject_id]\n",
    "#         # session_id = batch['session_id.txt']\n",
    "#         # session_id = [int(session[-2:]) for session in session_id]\n",
    "#         func, brain_pos_pats = aug_transform(input_func)\n",
    "#         if i >= (num_it-1):\n",
    "#             break\n",
    "    \n",
    "#     print(\"Done!\")\n",
    "#     print(\"input_func\", input_func.shape)\n",
    "#     print(\"func\", func.shape)\n",
    "#     print(\"subject_id\", subject_id)\n",
    "\n",
    "#     end_time = time.time()  \n",
    "#     execution_time = end_time - start_time  \n",
    "#     print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c6fba-5494-4964-b03d-e01c3afe47db",
   "metadata": {},
   "source": [
    "### Playing with the data, visualization of patching + masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea69a0b-f0a0-4309-b689-553b911a8da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if utils.is_interactive():\n",
    "#     func, brain_pos_pats = aug_transform(input_func)\n",
    "#     print(func.shape)\n",
    "#     display(utils.view_brain(func,cut_coords=(44,44,44)))\n",
    "# # plt.hist(func[0,0].flatten().clamp(.25,3),bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e6baa-4b1c-4f38-b078-70b2b092d14d",
   "metadata": {},
   "source": [
    "# Set up optimizer and saving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9553ad47-e76d-4ee1-a238-aad13a4be043",
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:    \n",
    "    # my_auto_wrap_policy = functools.partial(\n",
    "    #     size_based_auto_wrap_policy, min_num_params=200000\n",
    "    # )\n",
    "    my_auto_wrap_policy = functools.partial(\n",
    "        transformer_auto_wrap_policy, \n",
    "        transformer_layer_cls={\n",
    "            Attention, # <--- Your Transformer layer class\n",
    "        },\n",
    "    )\n",
    "    print(f\"\\nPrepping FSDP on {global_rank} {node}...\\n\")\n",
    "    model = model.to(device)\n",
    "    model = FSDP(\n",
    "        model,\n",
    "        sharding_strategy=ShardingStrategy.HYBRID_SHARD,\n",
    "        auto_wrap_policy=my_auto_wrap_policy,\n",
    "        use_orig_params=False,\n",
    "        cpu_offload=None, #CPUOffload(offload_params=True)\n",
    "        sync_module_states=True,\n",
    "        limit_all_gathers=True, # See https://github.com/pytorch/pytorch/issues/91165\n",
    "        device_id=device,\n",
    "    )\n",
    "    print(f\"\\nSuccessfully loaded FSDP model to device on global_rank {global_rank}\\n\")\n",
    "    dist.barrier()\n",
    "    print(f\"\\nSuccessfully loaded FSDP model to device on global_rank {global_rank}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4da73c08-ca61-48ef-9e63-b70db6f07a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iterations_per_epoch 4\n",
      "probe_num_iterations_per_epoch 1\n",
      "total_steps 80\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "1,838,256 total\n",
      "1,838,256 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "if distributed:\n",
    "    max_lr = max_lr * global_batch_size\n",
    "    print(f\"multiply lr {max_lr} by global batch size: max_lr={max_lr}\")\n",
    "\n",
    "# optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "optimizer = schedulefree.AdamWScheduleFree(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // global_batch_size\n",
    "print(\"num_iterations_per_epoch\", num_iterations_per_epoch)\n",
    "\n",
    "probe_num_iterations_per_epoch = test_num_samples_per_epoch // global_batch_size\n",
    "print(\"probe_num_iterations_per_epoch\", probe_num_iterations_per_epoch)\n",
    "\n",
    "total_steps = num_epochs * num_iterations_per_epoch * num_devices\n",
    "print(\"total_steps\", total_steps)\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#     optimizer,\n",
    "#     max_lr=max_lr,\n",
    "#     total_steps=total_steps,\n",
    "# )\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f51f674d-7cd4-49d1-bd8d-c697d9614f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_ckpt(model,tag=\"last\"):\n",
    "    if distributed: dist.barrier()\n",
    "    model_states = model.state_dict()\n",
    "    if global_rank == 0:\n",
    "        os.makedirs(outdir,exist_ok=True)\n",
    "        ckpt_path = outdir+f'/{tag}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_states,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, ckpt_path)\n",
    "        print(f\"\\n---saved {ckpt_path}!---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b4262-435b-4872-ab4a-424cb9dfd37a",
   "metadata": {},
   "source": [
    "# Start wandb (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56431733-4fb5-4072-840e-22536608f9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb found run mini_nomask_logitsCLS_downstream_40ep_l\n",
      "wandb_config:\n",
      " {'model_name': 'mini_nomask_logitsCLS_downstream_40ep_l', 'global_batch_size': 256, 'batch_size': 256, 'num_epochs': 20, 'num_samples_per_epoch': 1024, 'test_num_samples_per_epoch': 384, 'num_iterations_per_epoch': 4, 'encoder_model': 'vit_mini', 'decoder_model': 'vit_mini', 'tube_start_masking_ratio': 0.75, 'tube_end_masking_ratio': 0.75, 'decoder_mask_ratio': 0.75, 'num_frames': 4, 'patch_size': [8, 8, 8], 'frame_patch_size': 4, 'use_contrastive_loss': True, 'use_cls_token': True, 'contrastive_loss_weight': 0.1, 'num_params': 1838256, 'max_lr': 3e-06, 'ckpt_interval': 50, 'ckpt_saving': False, 'seed': 42, 'distributed': False, 'num_devices': 1, 'world_size': 1, 'train_urls': ['/weka/proj-fmri/shared/NSD_MNI_wds/{000000..000699}.tar']}\n",
      "wandb_id: mini_nomask_logitsCLS_downstream_40ep_l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaul-scotti\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/weka/proj-fmri/paulscotti/MindEyeV2/wandb/run-20240421_203216-mini_nomask_logitsCLS_downstream_40ep_l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/paul-scotti/found/runs/mini_nomask_logitsCLS_downstream_40ep_l' target=\"_blank\">mini_nomask_logitsCLS_downstream_40ep_l</a></strong> to <a href='https://wandb.ai/paul-scotti/found' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/paul-scotti/found' target=\"_blank\">https://wandb.ai/paul-scotti/found</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/paul-scotti/found/runs/mini_nomask_logitsCLS_downstream_40ep_l' target=\"_blank\">https://wandb.ai/paul-scotti/found/runs/mini_nomask_logitsCLS_downstream_40ep_l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if utils.is_interactive():\n",
    "#     wandb_log = False\n",
    "    ckpt_saving = False\n",
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"test_num_samples_per_epoch\": test_num_samples_per_epoch,\n",
    "      \"num_iterations_per_epoch\": num_iterations_per_epoch,\n",
    "      \"encoder_model\": encoder_model,\n",
    "      \"decoder_model\": decoder_model,\n",
    "      \"tube_start_masking_ratio\": tube_start_masking_ratio,\n",
    "      \"tube_end_masking_ratio\": tube_end_masking_ratio,\n",
    "      \"decoder_mask_ratio\": decoder_mask_ratio,\n",
    "      \"num_frames\": num_frames,\n",
    "      \"patch_size\": patch_size,\n",
    "      \"frame_patch_size\": frame_patch_size,\n",
    "      \"use_contrastive_loss\": use_contrastive_loss,\n",
    "      \"use_cls_token\": use_cls_token,\n",
    "      \"contrastive_loss_weight\": contrastive_loss_weight,\n",
    "      \"num_params\": num_params,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_urls\": train_urls,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a5055-8afd-468a-93bf-32f94bd1d042",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5e394dd-2745-41fa-a5aa-54b7b1373a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "lrs, train_losses, recon_losses, contrastive_losses = [], [], [], []\n",
    "cos_sim_encoder_output, cos_sim_decoder_output, cos_sim_encoder_output_patchwise = [], [], []\n",
    "probe_losses, probe_accs, test_losses, test_accs = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b9a5605-fd81-41d4-964e-1752ed6e1289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # resume from ckpt (e.g., if you are resuming from a run that got pre-empted)\n",
    "# load_progress = False\n",
    "# if wandb_log:\n",
    "#     if wandb.run.resumed:\n",
    "#         load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "#         load_progress = True\n",
    "# elif resume_from_ckpt: # if resuming without using wandb\n",
    "#     load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "#     load_progress = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "154e8f51-6918-45df-b6b8-ca46c89ba34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if masking_strategy==\"MNI\":\n",
    "    from einops.layers.torch import Rearrange\n",
    "    MNI_brain = nib.load(\"/weka/proj-fmri/paulscotti/fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "    brain_pos_voxels = MNI_brain[6:94,8:112,10:82]\n",
    "    brain_pos_pats = Rearrange(\n",
    "            \"b c (f pf) (d pd) (h ph) (w pw) -> b f d h w (pd ph pw pf c)\",\n",
    "            pd=patch_depth,\n",
    "            ph=patch_height,\n",
    "            pw=patch_width,\n",
    "            pf=1,\n",
    "        )(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "    brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3461199-e805-4e9c-8c91-894e83cf8bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2778883b6d5b442dba4d66aa5a0be678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.078125 2.080991744995117\n",
      "test 0 0.146484375 2.087881088256836\n",
      "test 1 0.033203125 2.1139793395996094\n",
      "{'train/loss': 0.6546683609485626, 'train/recon_losses': nan, 'train/contrastive_losses': 6.546683311462402, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.6011962890625, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.2939453125, 'train/probe_losses': 2.080991744995117, 'train/probe_accs': 0.078125, 'test/probe_losses': 2.1009302139282227, 'test/probe_accs': 0.08984375, 'lr': 3e-06, 'epoch': 0, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-paulscotti/found/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/admin/home-paulscotti/found/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2265625 2.132457733154297\n",
      "test 0 0.10546875 2.0733165740966797\n",
      "test 1 0.1875 1.9566822052001953\n",
      "{'train/loss': 0.646026149392128, 'train/recon_losses': nan, 'train/contrastive_losses': 6.460261344909668, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.6077880859375, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.294189453125, 'train/probe_losses': 2.132457733154297, 'train/probe_accs': 0.2265625, 'test/probe_losses': 2.0149993896484375, 'test/probe_accs': 0.146484375, 'lr': 3e-06, 'epoch': 1, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.123046875 2.136058807373047\n",
      "test 0 0.119140625 2.041532516479492\n",
      "test 1 0.275390625 1.9539203643798828\n",
      "{'train/loss': 0.6089650392532349, 'train/recon_losses': nan, 'train/contrastive_losses': 6.0896501541137695, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.609375, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.3001708984375, 'train/probe_losses': 2.136058807373047, 'train/probe_accs': 0.123046875, 'test/probe_losses': 1.9977264404296875, 'test/probe_accs': 0.197265625, 'lr': 3e-06, 'epoch': 2, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.294921875 1.9231395721435547\n",
      "test 0 0.123046875 1.9924964904785156\n",
      "test 1 0.232421875 2.002117156982422\n",
      "{'train/loss': 0.6081273704767227, 'train/recon_losses': nan, 'train/contrastive_losses': 6.081273555755615, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.6173095703125, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.29730224609375, 'train/probe_losses': 1.9231395721435547, 'train/probe_accs': 0.294921875, 'test/probe_losses': 1.9973068237304688, 'test/probe_accs': 0.177734375, 'lr': 3e-06, 'epoch': 3, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.15625 1.9835872650146484\n",
      "test 0 0.162109375 2.0044002532958984\n",
      "test 1 0.181640625 1.993692398071289\n",
      "{'train/loss': 0.6120030879974365, 'train/recon_losses': nan, 'train/contrastive_losses': 6.120030879974365, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.61279296875, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.30224609375, 'train/probe_losses': 1.9835872650146484, 'train/probe_accs': 0.15625, 'test/probe_losses': 1.9990463256835938, 'test/probe_accs': 0.171875, 'lr': 3e-06, 'epoch': 4, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.08203125 2.095348358154297\n",
      "test 0 0.052734375 2.236387252807617\n",
      "test 1 0.0703125 2.2432994842529297\n",
      "{'train/loss': 0.6117484122514725, 'train/recon_losses': nan, 'train/contrastive_losses': 6.117484092712402, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.6046142578125, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.3111572265625, 'train/probe_losses': 2.095348358154297, 'train/probe_accs': 0.08203125, 'test/probe_losses': 2.2398433685302734, 'test/probe_accs': 0.0615234375, 'lr': 3e-06, 'epoch': 5, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.15625 2.082639694213867\n",
      "test 0 0.01171875 2.237241744995117\n",
      "test 1 0.05859375 2.2262916564941406\n",
      "{'train/loss': 0.610412523150444, 'train/recon_losses': nan, 'train/contrastive_losses': 6.104125022888184, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.6123046875, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.30657958984375, 'train/probe_losses': 2.082639694213867, 'train/probe_accs': 0.15625, 'test/probe_losses': 2.231766700744629, 'test/probe_accs': 0.03515625, 'lr': 3e-06, 'epoch': 6, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.00390625 2.2390995025634766\n",
      "test 0 0.1015625 2.056934356689453\n",
      "test 1 0.064453125 1.9691143035888672\n",
      "{'train/loss': 0.6085265427827835, 'train/recon_losses': nan, 'train/contrastive_losses': 6.085265159606934, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.63623046875, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.31298828125, 'train/probe_losses': 2.2390995025634766, 'train/probe_accs': 0.00390625, 'test/probe_losses': 2.01302433013916, 'test/probe_accs': 0.0830078125, 'lr': 3e-06, 'epoch': 7, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.494140625 1.8124122619628906\n",
      "test 0 0.392578125 1.8616065979003906\n",
      "test 1 0.3828125 1.8176498413085938\n",
      "{'train/loss': 0.6146479845046997, 'train/recon_losses': nan, 'train/contrastive_losses': 6.146479606628418, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.613525390625, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.31793212890625, 'train/probe_losses': 1.8124122619628906, 'train/probe_accs': 0.494140625, 'test/probe_losses': 1.8396282196044922, 'test/probe_accs': 0.3876953125, 'lr': 3e-06, 'epoch': 8, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.021484375 2.2320098876953125\n",
      "test 0 0.025390625 2.181182861328125\n",
      "test 1 0.033203125 2.175107955932617\n",
      "{'train/loss': 0.6042157411575317, 'train/recon_losses': nan, 'train/contrastive_losses': 6.042157173156738, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.6165771484375, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.31536865234375, 'train/probe_losses': 2.2320098876953125, 'train/probe_accs': 0.021484375, 'test/probe_losses': 2.178145408630371, 'test/probe_accs': 0.029296875, 'lr': 3e-06, 'epoch': 9, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.04296875 2.068845748901367\n",
      "test 0 0.095703125 2.147918701171875\n",
      "test 1 0.275390625 2.0189361572265625\n",
      "{'train/loss': 0.6057839542627335, 'train/recon_losses': nan, 'train/contrastive_losses': 6.057839393615723, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.6259765625, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.30718994140625, 'train/probe_losses': 2.068845748901367, 'train/probe_accs': 0.04296875, 'test/probe_losses': 2.0834274291992188, 'test/probe_accs': 0.185546875, 'lr': 3e-06, 'epoch': 10, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.248046875 2.012929916381836\n",
      "test 0 0.1875 2.1199779510498047\n",
      "test 1 0.177734375 2.0317916870117188\n",
      "{'train/loss': 0.6008661240339279, 'train/recon_losses': nan, 'train/contrastive_losses': 6.008661270141602, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.624267578125, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.3172607421875, 'train/probe_losses': 2.012929916381836, 'train/probe_accs': 0.248046875, 'test/probe_losses': 2.0758848190307617, 'test/probe_accs': 0.1826171875, 'lr': 3e-06, 'epoch': 11, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.15625 2.024129867553711\n",
      "test 0 0.361328125 1.8665599822998047\n",
      "test 1 0.22265625 1.9156570434570312\n",
      "{'train/loss': 0.6274944394826889, 'train/recon_losses': nan, 'train/contrastive_losses': 6.274944305419922, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.62109375, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.3148193359375, 'train/probe_losses': 2.024129867553711, 'train/probe_accs': 0.15625, 'test/probe_losses': 1.891108512878418, 'test/probe_accs': 0.2919921875, 'lr': 3e-06, 'epoch': 12, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.046875 2.0176239013671875\n",
      "test 0 0.140625 1.9483413696289062\n",
      "test 1 0.287109375 1.853952407836914\n",
      "{'train/loss': 0.5929619073867798, 'train/recon_losses': nan, 'train/contrastive_losses': 5.929618835449219, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.6199951171875, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.31878662109375, 'train/probe_losses': 2.0176239013671875, 'train/probe_accs': 0.046875, 'test/probe_losses': 1.9011468887329102, 'test/probe_accs': 0.2138671875, 'lr': 3e-06, 'epoch': 13, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.025390625 2.3530330657958984\n",
      "test 0 0.22265625 1.9583473205566406\n",
      "test 1 0.26171875 2.0058231353759766\n",
      "{'train/loss': 0.6034502238035202, 'train/recon_losses': nan, 'train/contrastive_losses': 6.034502029418945, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.636962890625, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.31683349609375, 'train/probe_losses': 2.3530330657958984, 'train/probe_accs': 0.025390625, 'test/probe_losses': 1.9820852279663086, 'test/probe_accs': 0.2421875, 'lr': 3e-06, 'epoch': 14, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.037109375 2.5336132049560547\n",
      "test 0 0.06640625 2.2813949584960938\n",
      "test 1 0.345703125 1.9364681243896484\n",
      "{'train/loss': 0.5937714576721191, 'train/recon_losses': nan, 'train/contrastive_losses': 5.937714576721191, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.6195068359375, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.31719970703125, 'train/probe_losses': 2.5336132049560547, 'train/probe_accs': 0.037109375, 'test/probe_losses': 2.108931541442871, 'test/probe_accs': 0.2060546875, 'lr': 3e-06, 'epoch': 15, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.080078125 2.155559539794922\n",
      "test 0 0.056640625 2.157787322998047\n",
      "test 1 0.146484375 2.017892837524414\n",
      "{'train/loss': 0.6005350202322006, 'train/recon_losses': nan, 'train/contrastive_losses': 6.005350112915039, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.6202392578125, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.3182373046875, 'train/probe_losses': 2.155559539794922, 'train/probe_accs': 0.080078125, 'test/probe_losses': 2.0878400802612305, 'test/probe_accs': 0.1015625, 'lr': 3e-06, 'epoch': 16, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.111328125 2.099821090698242\n",
      "test 0 0.4296875 1.8980712890625\n",
      "test 1 0.294921875 1.9661064147949219\n",
      "{'train/loss': 0.6011319160461426, 'train/recon_losses': nan, 'train/contrastive_losses': 6.011319160461426, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.6202392578125, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.31549072265625, 'train/probe_losses': 2.099821090698242, 'train/probe_accs': 0.111328125, 'test/probe_losses': 1.932088851928711, 'test/probe_accs': 0.3623046875, 'lr': 3e-06, 'epoch': 17, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.119140625 2.0135326385498047\n",
      "test 0 0.265625 1.861032485961914\n",
      "test 1 0.158203125 1.972625732421875\n",
      "{'train/loss': 0.5891992747783661, 'train/recon_losses': nan, 'train/contrastive_losses': 5.891992568969727, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.6190185546875, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.316650390625, 'train/probe_losses': 2.0135326385498047, 'train/probe_accs': 0.119140625, 'test/probe_losses': 1.9168291091918945, 'test/probe_accs': 0.2119140625, 'lr': 3e-06, 'epoch': 18, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n",
      "0 0.181640625 2.0815296173095703\n",
      "test 0 0.154296875 2.060464859008789\n",
      "test 1 0.46875 1.8413829803466797\n",
      "{'train/loss': 0.5968149304389954, 'train/recon_losses': nan, 'train/contrastive_losses': 5.968149185180664, 'train/num_steps': 0, 'train/cos_sim_encoder_output': 0.61767578125, 'train/cos_sim_decoder_output': nan, 'train/cos_sim_encoder_output_patchwise': 0.31536865234375, 'train/probe_losses': 2.0815296173095703, 'train/probe_accs': 0.181640625, 'test/probe_losses': 1.9509239196777344, 'test/probe_accs': 0.3115234375, 'lr': 3e-06, 'epoch': 19, 'tube_mask_ratio': 0.75, 'decoder_mask_ratio': 0.75}\n"
     ]
    }
   ],
   "source": [
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "crossentropy = nn.CrossEntropyLoss()\n",
    "if use_contrastive_loss:\n",
    "    contrastive_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs)\n",
    "progress_bar = tqdm(range(epoch, num_epochs), disable=local_rank!=0, desc=\"Overall\")\n",
    "for epoch in progress_bar:\n",
    "    # get the masking ratio for the current epoch\n",
    "    tube_mask_ratio = utils.get_masking_ratio(\n",
    "        current_epoch=epoch, \n",
    "        total_epochs=num_epochs, \n",
    "        start_masking_ratio=tube_start_masking_ratio, \n",
    "        end_masking_ratio=tube_end_masking_ratio\n",
    "    )\n",
    "    with torch.cuda.amp.autocast(dtype=data_type):\n",
    "        model.train()\n",
    "        optimizer.train()\n",
    "        for train_i, batch in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_func = batch['func.npy']\n",
    "\n",
    "            subject_id = batch['subject_id.txt']\n",
    "            subject_id = torch.Tensor([int(subject[-2:]) for subject in subject_id]).long()\n",
    "            subject_id = torch.repeat_interleave(subject_id.long(), 2).to(device)\n",
    "            # session_id = batch['session_id.txt']\n",
    "            # session_id = torch.Tensor([int(session[-2:]) for session in session_id]).long().repeat(2).to(device)\n",
    "            # session_id = torch.repeat_interleave(session_id.long(), 2)\n",
    "\n",
    "            if masking_strategy==\"None\":\n",
    "                func, _ = aug_transform(input_func)\n",
    "                brain_pos_pats_vit = torch.ones(num_patches_per_timepoint)\n",
    "            elif masking_strategy==\"MNI\":\n",
    "                func, _ = aug_transform(input_func)\n",
    "            else:\n",
    "                func, brain_pos_voxels = aug_transform(input_func)\n",
    "                brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "                brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "\n",
    "            func = func.reshape(-1, num_frames, \n",
    "                                func.shape[-3], func.shape[-2], func.shape[-1])\n",
    "            func = func.unsqueeze(1).clamp(0,1)\n",
    "            \n",
    "            # create encoder and decoder masks\n",
    "            rand_patches = torch.randperm(num_patches_per_timepoint)\n",
    "            \n",
    "            encoder_mask = torch.zeros(num_patches_per_timepoint).to(torch.bool)\n",
    "            encoder_mask[rand_patches[:num_encoder_patches]] = True\n",
    "            encoder_mask = encoder_mask.tile(num_frames//frame_patch_size)\n",
    "            \n",
    "            decoder_mask = torch.zeros(num_patches_per_timepoint).to(torch.bool)\n",
    "            decoder_mask[rand_patches[num_encoder_patches:num_encoder_patches+num_decoder_patches]] = True\n",
    "            decoder_mask = decoder_mask.tile(num_frames//frame_patch_size)\n",
    "\n",
    "            # encode the tube patches\n",
    "            encoder_out = model(func, encoder_mask=encoder_mask, device=device)\n",
    "            if use_cls_token:\n",
    "                enc_cls_token = encoder_out[:,:1,:]\n",
    "\n",
    "            # decode both the encoder_out patches and masked decoder patches\n",
    "            decoder_out = model(encoder_out, encoder_mask=encoder_mask, decoder_mask=decoder_mask, device=device)\n",
    "            # subset only the reconstructed decoder patches\n",
    "            output = decoder_out[:, -decoder_mask.sum():]\n",
    "\n",
    "            # compare to ground truth and calculate loss\n",
    "            target_patches = model.patchify(func)\n",
    "            target_patches_vit = rearrange(target_patches, \"b ... d -> b (...) d\")\n",
    "            target = target_patches_vit.to(device)[:, decoder_mask]\n",
    "\n",
    "            target_mean = target.mean(0)\n",
    "            target_std = target.std(0)\n",
    "            target_normed = (target - target_mean) / (target_std + 1e-6)\n",
    "\n",
    "            recon_loss = mse(output, target_normed)\n",
    "            recon_losses.append(recon_loss.item())\n",
    "            loss = recon_loss\n",
    "\n",
    "            # contrastive loss\n",
    "            if use_contrastive_loss:\n",
    "                # encode the decoder patches\n",
    "                encoder_out2 = model(func, encoder_mask=decoder_mask, device=device)\n",
    "                enc_cls_token2 = encoder_out2[:,:1,:]\n",
    "                \n",
    "                temp = contrastive_temps[epoch]\n",
    "                \n",
    "                logits = (nn.functional.normalize(enc_cls_token.flatten(1),dim=-1) @\n",
    "                            nn.functional.normalize(enc_cls_token2.flatten(1),dim=-1).T) / temp\n",
    "                \n",
    "                # logits = (nn.functional.normalize(model.cont(encoder_out.flatten(1)),dim=-1) @\n",
    "                #             nn.functional.normalize(model.cont(encoder_out2.flatten(1)),dim=-1).T) / temp\n",
    "                \n",
    "                labels = torch.arange(len(logits)).long().to(device)\n",
    "                loss1 = crossentropy(logits, labels)\n",
    "                # loss1 = -(logits.log_softmax(-1) * labels.softmax(-1)).sum(-1).mean()\n",
    "                loss2 = crossentropy(logits.T, labels)\n",
    "                contr_loss = (loss1 + loss2)/2\n",
    "                \n",
    "                contrastive_losses.append(contr_loss.item())\n",
    "                loss += (contr_loss * contrastive_loss_weight)\n",
    "\n",
    "            cos_sim_encoder_output_patchwise.append(utils.patchwise_cosine_similarity(encoder_out).mean().item())\n",
    "            cos_sim_encoder_output.append(utils.batchwise_cosine_similarity(encoder_out.flatten(1)/1e3,encoder_out.flatten(1)/1e3)[~torch.eye(len(encoder_out),dtype=torch.bool)].mean().item())\n",
    "            cos_sim_decoder_output.append(utils.batchwise_cosine_similarity(output,output)[~torch.eye(len(output),dtype=torch.bool)].mean().item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            if train_i >= (num_iterations_per_epoch-1):\n",
    "                break\n",
    "\n",
    "        # reset linear_probe\n",
    "        # if use_cls_token:\n",
    "        #     linear_probe = LinearProbe((num_patches_per_timepoint+1)*model.encoder_embed_dim)\n",
    "        # else:\n",
    "        #     linear_probe = LinearProbe(num_patches_per_timepoint*model.encoder_embed_dim)\n",
    "        linear_probe = LinearProbe(model.encoder_embed_dim)\n",
    "        linear_probe = linear_probe.to(device)\n",
    "        probe_opt_grouped_parameters = [\n",
    "            {'params': [p for n, p in linear_probe.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "            {'params': [p for n, p in linear_probe.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        ]\n",
    "        probe_optimizer = torch.optim.AdamW(probe_opt_grouped_parameters, lr=3e-3)\n",
    "\n",
    "        if True:#(epoch % 5 == 0) or (epoch == num_epochs-1):\n",
    "            model.eval()\n",
    "            optimizer.eval()\n",
    "            linear_probe.train()\n",
    "            for probe_i, batch in enumerate(train_dl):\n",
    "                probe_optimizer.zero_grad()\n",
    "\n",
    "                input_func = batch['func.npy']\n",
    "\n",
    "                subject_id = batch['subject_id.txt']\n",
    "                subject_id = torch.Tensor([int(subject[-2:]) for subject in subject_id]).long()\n",
    "                subject_id = torch.repeat_interleave(subject_id.long(), 2).to(device)\n",
    "\n",
    "                func, _ = aug_transform(input_func)\n",
    "                func = func.reshape(-1, num_frames, \n",
    "                                    func.shape[-3], func.shape[-2], func.shape[-1])\n",
    "                func = func.unsqueeze(1).clamp(0,1)\n",
    "\n",
    "                encoder_mask = torch.ones(num_patches_per_timepoint).to(torch.bool)\n",
    "                encoder_mask = encoder_mask.tile(num_frames//frame_patch_size)\n",
    "\n",
    "                # encode the tube patches\n",
    "                with torch.no_grad():\n",
    "                    encoder_out = model(func, encoder_mask=encoder_mask, device=device)\n",
    "                    encoder_out = encoder_out[:,:1,:]\n",
    "                    encoder_out = nn.functional.normalize(encoder_out,dim=-1)\n",
    "\n",
    "                # linear probe\n",
    "                subject_pred = linear_probe(encoder_out.flatten(1).to(device))\n",
    "                probe_loss = crossentropy(subject_pred, subject_id-1) # minus 1 because subject_id is 1-indexed\n",
    "\n",
    "                probe_accuracy = (torch.max(subject_pred,1).indices == (subject_id-1)).sum() / len(subject_id)\n",
    "                probe_accs.append(probe_accuracy.item())\n",
    "                probe_losses.append(probe_loss.item())\n",
    "\n",
    "                print(probe_i, probe_accuracy.item(), probe_loss.item())\n",
    "\n",
    "                probe_loss.backward()\n",
    "                probe_optimizer.step()\n",
    "\n",
    "                if probe_i >= (probe_num_iterations_per_epoch-1):\n",
    "                    break\n",
    "\n",
    "            for test_i, batch in enumerate(test_dl):\n",
    "                input_func = batch['func.npy']\n",
    "\n",
    "                subject_id = batch['subject_id.txt']\n",
    "                subject_id = torch.Tensor([int(subject[-2:]) for subject in subject_id]).long()\n",
    "                subject_id = torch.repeat_interleave(subject_id.long(), 2).to(device)\n",
    "\n",
    "                func, _ = aug_transform(input_func)\n",
    "                func = func.reshape(-1, num_frames, \n",
    "                                    func.shape[-3], func.shape[-2], func.shape[-1])\n",
    "                func = func.unsqueeze(1).clamp(0,1)\n",
    "\n",
    "                encoder_mask = torch.ones(num_patches_per_timepoint).to(torch.bool)\n",
    "                encoder_mask = encoder_mask.tile(num_frames//frame_patch_size)\n",
    "\n",
    "                # encode the tube patches\n",
    "                with torch.no_grad():\n",
    "                    encoder_out = model(func, encoder_mask=encoder_mask, device=device)\n",
    "                    encoder_out = encoder_out[:,:1,:]\n",
    "                    encoder_out = nn.functional.normalize(encoder_out,dim=-1)\n",
    "\n",
    "                # linear probe\n",
    "                subject_pred = linear_probe(encoder_out.flatten(1).to(device))\n",
    "                test_loss = crossentropy(subject_pred, subject_id-1) # minus 1 because subject_id is 1-indexed\n",
    "\n",
    "                test_accuracy = (torch.max(subject_pred,1).indices == (subject_id-1)).sum() / len(subject_id)\n",
    "                test_accs.append(test_accuracy.item())\n",
    "                test_losses.append(test_loss.item())\n",
    "\n",
    "                print(\"test\", test_i, test_accuracy.item(), test_loss.item())\n",
    "\n",
    "                if test_i >= 1:\n",
    "                    break\n",
    "\n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(train_losses[-(train_i + 1) :]),\n",
    "            \"train/recon_losses\": np.mean(recon_losses[-(train_i + 1) :]),\n",
    "            \"train/contrastive_losses\": np.mean(contrastive_losses[-(train_i + 1) :]),\n",
    "            \"train/num_steps\": len(recon_losses),\n",
    "            \"train/cos_sim_encoder_output\": np.mean(cos_sim_encoder_output[-(train_i + 1) :]),\n",
    "            \"train/cos_sim_decoder_output\": np.mean(cos_sim_decoder_output[-(train_i + 1) :]),\n",
    "            \"train/cos_sim_encoder_output_patchwise\": np.mean(cos_sim_encoder_output_patchwise[-(train_i + 1) :]),\n",
    "            \"train/probe_losses\": np.mean(probe_losses[-(probe_i + 1) :]),\n",
    "            \"train/probe_accs\": np.mean(probe_accs[-(probe_i + 1) :]),\n",
    "            \"test/probe_losses\": np.mean(test_losses[-(test_i + 1) :]),\n",
    "            \"test/probe_accs\": np.mean(test_accs[-(test_i + 1) :]),\n",
    "            \"lr\": np.mean(lrs[-(train_i + 1) :]),\n",
    "            \"epoch\": epoch,\n",
    "            \"tube_mask_ratio\": tube_mask_ratio,\n",
    "            \"decoder_mask_ratio\": decoder_mask_ratio,\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        if utils.is_interactive(): print(logs)\n",
    "\n",
    "        # Plot progress (first sample in batch)\n",
    "        with torch.no_grad():\n",
    "            if utils.is_interactive() or wandb_log:\n",
    "                if epoch % 50 == 0:\n",
    "                    output = (output * target_std) + target_mean\n",
    "                    idx = 0\n",
    "                    \n",
    "                    decode_vis = torch.zeros_like(target_patches_vit)\n",
    "                    decode_vis[:, decoder_mask] = output.to(decode_vis.device).to(decode_vis.dtype)\n",
    "                    decoder_unpatches = rearrange(\n",
    "                        decode_vis,\n",
    "                        \"b (f d h w) c -> b f d h w c\",\n",
    "                        d=img_size[0]//patch_depth,\n",
    "                        h=img_size[1]//patch_height,\n",
    "                        w=img_size[2]//patch_width,\n",
    "                    )\n",
    "                    decoder_func = rearrange(\n",
    "                        decoder_unpatches,\n",
    "                        \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "                        b=batch_size*2,\n",
    "                        f=num_frames//frame_patch_size,\n",
    "                        d=img_size[0]//patch_depth,\n",
    "                        h=img_size[1]//patch_height,\n",
    "                        w=img_size[2]//patch_width,\n",
    "                        pd=patch_depth,\n",
    "                        ph=patch_height,\n",
    "                        pw=patch_width,\n",
    "                        pf=frame_patch_size,\n",
    "                    )\n",
    "                    orig_image = utils.reshape_to_2d(func[idx])\n",
    "                    recon_image = utils.reshape_to_2d(decoder_func[idx])\n",
    "\n",
    "                    combined_image = orig_image.clone()\n",
    "                    combined_image[recon_image!=0] = recon_image[recon_image!=0]\n",
    "\n",
    "                    random_start = np.arange(3100,3450)\n",
    "                    orig_image = transforms.ToPILImage()(orig_image[:,random_start])\n",
    "                    recon_image = transforms.ToPILImage()(recon_image[:,random_start])\n",
    "                    combined_image = transforms.ToPILImage()(combined_image[:,random_start])\n",
    "\n",
    "                    if wandb_log:\n",
    "                        logs[f\"train/orig\"] = wandb.Image(orig_image, caption=f\"epoch{epoch:03d}\")\n",
    "                        logs[f\"train/recon\"] = wandb.Image(recon_image, caption=f\"epoch{epoch:03d}\")\n",
    "                        logs[f\"train/combined\"] = wandb.Image(combined_image, caption=f\"epoch{epoch:03d}\")\n",
    "                    else:\n",
    "                        if epoch==0:\n",
    "                            print(\"orig_image\")\n",
    "                            display(orig_image)\n",
    "                            print(\"recon_image\")\n",
    "                            display(recon_image)\n",
    "                            print(\"combined_image\")\n",
    "                        display(combined_image)\n",
    "\n",
    "    if wandb_log: wandb.log(logs)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if (ckpt_saving) and ((epoch % ckpt_interval == 0) or (epoch==num_epochs-1)):\n",
    "        save_ckpt(model,\"last\")\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    if distributed: dist.barrier()\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "if distributed:\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6779720-0d08-4aec-87e2-439e0c1e9dca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_mask.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840c426-d7f5-4d0c-acd1-5ea47d453b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6e847-2511-4f69-aa70-f1471a5b7d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(probe_losses)\n",
    "# plt.title(\"Training re-construction losses\")\n",
    "plt.show()\n",
    "if use_contrastive_loss:\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(contrastive_losses)\n",
    "    plt.title(\"Training contrastive losses\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri_new",
   "language": "python",
   "name": "fmri_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
