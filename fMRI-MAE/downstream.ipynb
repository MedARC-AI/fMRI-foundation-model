{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0859a181-0681-489e-9939-8d47614b9fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK=0\n",
      "NUM GPUS=1\n",
      "GLOBAL RANK=0\n",
      "batch_size 32\n",
      "skipping deepspeed reconfiguration...\n",
      "PID of this process = 617152\n",
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-paulscotti/found/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from models import *\n",
    "from mindeye_models import *\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('LOCAL_RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(f\"LOCAL RANK={local_rank}\")\n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(f\"NUM GPUS={num_devices}\")\n",
    "distributed = True if num_devices>1 else False\n",
    "\n",
    "global_rank = os.getenv('RANK')\n",
    "if global_rank is None:\n",
    "    global_rank = 0\n",
    "else:\n",
    "    global_rank = int(global_rank)\n",
    "print(f\"GLOBAL RANK={global_rank}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "    \n",
    "# Load MindEye parameters from yaml config (will override any params with same name)\n",
    "mindeye_config = yaml.load(open('mindeye_config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in mindeye_config.keys():\n",
    "    globals()[attribute_name] = mindeye_config[f'{attribute_name}']\n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "\n",
    "batch_size = global_batch_size // num_devices\n",
    "print(\"batch_size\", batch_size)\n",
    "    \n",
    "# First use \"accelerate config\" in terminal and setup using deepspeed stage 2 with CPU offloading\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "\n",
    "from accelerate.state import AcceleratorState\n",
    "try:\n",
    "    AcceleratorState().deepspeed_plugin.deepspeed_config['train_micro_batch_size_per_gpu'] = batch_size\n",
    "    print(\"deepspeed reconfigured, train_micro_batch_size_per_gpu = \", batch_size)\n",
    "except:\n",
    "    print(\"skipping deepspeed reconfiguration...\")\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b35bc5-8674-4061-a606-f44249d167df",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9fa256a-cc44-4346-8d6c-1bed3779d5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae config\n",
      "\n",
      " {'model_name': 'framepatchsize1_4gpu_bs8_3e7_norm_rope_atomcos_001contrastiveweight', 'use_cls_token': False, 'use_contrastive_loss': True, 'contrastive_loss_weight': 0.001, 'batch_size': 8, 'num_workers': 10, 'num_epochs': 100, 'seed': 42, 'max_lr': 3e-07, 'num_samples_per_epoch': 1024, 'ckpt_saving': True, 'ckpt_interval': 50, 'resume_from_ckpt': True, 'wandb_log': True, 'tube_start_masking_ratio': 0.9, 'tube_end_masking_ratio': 0.9, 'decoder_mask_ratio': 0.85, 'patch_size': [8, 8, 8], 'frame_patch_size': 1, 'use_rope_emb': True, 'masking_strategy': 'MNI', 'encoder_model': 'vit_base', 'decoder_model': 'vit_base', 'img_size': [88, 104, 72], 'num_frames': 4, 'is_s3': False, 'train_urls': ['/weka/proj-fmri/shared/NSD_MNI_wds/{000000..000738}.tar']}\n",
      "mindeye_config\n",
      " {'model_name': 'downstream_framepatchsize4_bs8_300ep_contr_nolinear', 'mae_model_name': 'framepatchsize1_4gpu_bs8_300ep_NEWcont_3e8_norm_rope_LONG', 'global_batch_size': 32, 'mixed_precision': 'fp16', 'num_epochs': 12, 'seed': 42, 'max_lr': 0.0003, 'multi_subject': False, 'multisubject_ckpt': 'None', 'ckpt_saving': False, 'ckpt_interval': 99, 'resume_from_ckpt': False, 'wandb_log': False, 'in_dim': 393216, 'hidden_dim': 2056, 'drop': 0.15, 'mixup_pct': 0.0, 'nsd_wds_path': '/weka/proj-fmri/shared/mindeyev2_dataset/wds', 'nsd_raw_path': '/weka/proj-fmri/shared/mindeyev2_dataset', 'nsd_image_path': '/weka/proj-fmri/shared/mindeyev2_dataset', 'num_sessions': 40}\n",
      "mae_ckpt_pth /weka/proj-fmri/paulscotti/fMRI-foundation-model/ckpts/framepatchsize1_4gpu_bs8_300ep_NEWcont_3e8_norm_rope_LONG/last.pth\n",
      "outdir /weka/proj-fmri/paulscotti/fMRI-foundation-model/ckpts/downstream_framepatchsize4_bs8_300ep_contr_nolinear\n",
      "use_cls_token False\n",
      "num_patches 5148\n"
     ]
    }
   ],
   "source": [
    "print(\"mae config\\n\\n\",config)\n",
    "print(\"mindeye_config\\n\",mindeye_config)\n",
    "\n",
    "# if utils.is_interactive():\n",
    "#     ckpt_saving = False\n",
    "#     wandb_log = False\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "mae_ckpt_pth = os.path.abspath(f'../ckpts/{mae_model_name}/last.pth')\n",
    "print(\"mae_ckpt_pth\", mae_ckpt_pth)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "use_cls_token = False #True if use_contrastive_loss else use_cls_token\n",
    "print(\"use_cls_token\", use_cls_token)\n",
    "\n",
    "if type(patch_size) == int:\n",
    "    patch_size = [patch_size,patch_size,patch_size]\n",
    "patch_depth = patch_size[0]\n",
    "patch_height = patch_size[1]\n",
    "patch_width = patch_size[2]\n",
    "\n",
    "num_patches = int(\n",
    "    (img_size[0] / patch_depth)\n",
    "    * (img_size[1] / patch_height)\n",
    "    * (img_size[2] / patch_width)\n",
    "    * num_frames\n",
    ")\n",
    "print(\"num_patches\", num_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8115109a-30a1-448e-a1ea-1b115ed8cb6d",
   "metadata": {},
   "source": [
    "# Load pretrained foundation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b40287e-bab2-4791-9407-e63ad5b0a3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "170,830,848 total\n",
      "170,830,848 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "170830848"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_size = {\n",
    "    \"encoder\": encoder_model,\n",
    "    \"decoder\": decoder_model\n",
    "}\n",
    "    \n",
    "model = get_vit(\n",
    "    size=vit_size,\n",
    "    image_size=img_size,  # depth, height, width\n",
    "    image_patch_size=(patch_depth,patch_height,patch_width),  # depth, height, width patch size\n",
    "    frames=num_frames,\n",
    "    frame_patch_size=frame_patch_size,\n",
    "    channels=1,\n",
    "    use_rope_emb=use_rope_emb,\n",
    "    use_cls_token=use_cls_token,\n",
    ")\n",
    "utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b45ad3-e8d5-40cd-b041-1defdbf15155",
   "metadata": {},
   "source": [
    "## Load pretrained ckpt for MAE foundation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62bb3f5f-cf7f-42c5-ab8c-d4bf143ba727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c426853-d268-4daf-ac19-faa40c2cd362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(mae_ckpt_pth, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])#, strict=False)\n",
    "\n",
    "# set foundation model to evaluation\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd89a4-955f-439d-b3c9-8f261c06eed2",
   "metadata": {},
   "source": [
    "# Setup MindEye model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dae53d1-abff-4ddb-98cd-2cab04b9f0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nsddata_raw_stimuli = pd.read_csv(f\"{nsd_raw_path}/nsddata_rawdata.csv\")\n",
    "TR_delay = 3 # to account for bold hrf\n",
    "train_TRs = np.round(nsddata_raw_stimuli[nsddata_raw_stimuli['shared1000'] == False]['global_TR_onsets'].values + TR_delay).astype(np.int32)\n",
    "test_TRs = np.round(nsddata_raw_stimuli[nsddata_raw_stimuli['shared1000'] == True]['global_TR_onsets'].values + TR_delay).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d5015e9-120e-4240-886c-6959796e8faf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images! torch.Size([73000, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{nsd_image_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:] \n",
    "images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images!\", images.shape)\n",
    "\n",
    "# Load MindEye hdf5\n",
    "f = h5py.File(f'{nsd_raw_path}/subj01_mnidata.h5', 'r') #subj01_rawdata_old.h5\n",
    "mindeye_global_trs = f['global_trs'][:]\n",
    "mindeye_funcs = f['funcs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d79bcf35-45a6-4dc5-90da-c96c3a596401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenOpenCLIPImageEmbedder(\n",
       "  (model): CLIP(\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (patch_dropout): Identity()\n",
       "      (ln_pre): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): ModuleList(\n",
       "          (0-47): 48 x ResidualAttentionBlock(\n",
       "            (ln_1): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (ls_1): Identity()\n",
       "            (ln_2): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1664, out_features=8192, bias=True)\n",
       "              (gelu): GELU(approximate='none')\n",
       "              (c_proj): Linear(in_features=8192, out_features=1664, bias=True)\n",
       "            )\n",
       "            (ls_2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 1280)\n",
       "    (ln_final): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "#     arch=\"ViT-bigG-14\",\n",
    "#     version=\"laion2b_s39b_b160k\",\n",
    "#     output_tokens=True,\n",
    "#     only_tokens=True,\n",
    "# )\n",
    "# clip_seq_dim = 256\n",
    "# clip_emb_dim = 1664\n",
    "# clip_img_embedder.to(device)\n",
    "\n",
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=\"ViT-bigG-14\",\n",
    "    version=\"laion2b_s39b_b160k\",\n",
    "    output_tokens=False,\n",
    "    only_tokens=False,\n",
    "    init_device=device,\n",
    "    device=device,\n",
    ")\n",
    "clip_seq_dim = 1\n",
    "clip_emb_dim = 1280\n",
    "clip_img_embedder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d89d95f0-b52e-439f-ba5d-c13e2897fe31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 32 num_iterations_per_epoch = 11 num_samples_per_epoch = 375\n",
      "Training with 40 sessions\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/train/{0..39}.tar\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/new_test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subj = s = 1\n",
    "subj_list = [subj]\n",
    "\n",
    "# if multi_subject:\n",
    "#     nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "#     num_samples_per_epoch = (750*40) // num_devices \n",
    "# else:\n",
    "#     num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "\n",
    "num_samples_per_epoch = 375 #(750*num_sessions) // num_devices\n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)\n",
    "\n",
    "train_data = {}\n",
    "train_dl = {}\n",
    "\n",
    "print(f\"Training with {num_sessions} sessions\")\n",
    "train_url = f\"{nsd_wds_path}/subj0{s}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "print(train_url)\n",
    "    \n",
    "train_data[f'subj0{s}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=utils.my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "# train_dl[f'subj0{s}'] = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "train_dl[f'subj0{s}'] = wds.WebLoader(\n",
    "    train_data[f'subj0{s}'].batched(batch_size), \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    batch_size=None,\n",
    "    num_workers=num_workers, \n",
    "    persistent_workers=num_workers>0,\n",
    ").with_epoch(num_iterations_per_epoch)\n",
    "\n",
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "if subj==3:\n",
    "    num_test=2371\n",
    "elif subj==4:\n",
    "    num_test=2188\n",
    "elif subj==6:\n",
    "    num_test=2371\n",
    "elif subj==8:\n",
    "    num_test=2188\n",
    "else:\n",
    "    num_test=3000\n",
    "test_url = f\"{nsd_wds_path}/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "test_data = wds.WebDataset(test_url,resampled=True,nodesplitter=utils.my_split_by_node)\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "# test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "test_dl = wds.WebLoader(\n",
    "    test_data.batched(num_test),\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    batch_size=None,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=num_workers>0,\n",
    ").with_epoch(10)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e100e116-4b98-4c15-bd8e-252871e72517",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "507,388,672 total\n",
      "507,388,672 trainable\n",
      "param counts:\n",
      "507,388,672 total\n",
      "507,388,672 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "507388672"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "# class RidgeRegression(torch.nn.Module):\n",
    "#     # make sure to add weight_decay when initializing optimizer\n",
    "#     def __init__(self, input_sizes, out_features, seq_len=1): \n",
    "#         super(RidgeRegression, self).__init__()\n",
    "#         self.seq_len = seq_len\n",
    "#         self.out_features = out_features\n",
    "#         self.linears = torch.nn.ModuleList([\n",
    "#                 torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "#             ])\n",
    "#     def forward(self, x, subj_idx):\n",
    "#         out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(self.seq_len)], dim=1)\n",
    "#         return out\n",
    "    \n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_sizes, out_features, seq_len=0): \n",
    "        super(MLP, self).__init__()\n",
    "        self.input_sizes = input_sizes[0]\n",
    "        self.out_features = out_features\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(self.input_sizes),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.input_sizes, out_features),\n",
    "            nn.LayerNorm(out_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.LayerNorm(out_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(out_features, out_features)\n",
    "        )\n",
    "    def forward(self, x, z=None):\n",
    "        out = self.mlp(x[:,0])\n",
    "        return out\n",
    "\n",
    "\n",
    "mindeye = MindEyeModule()\n",
    "mindeye.ridge = MLP(np.array([in_dim]), out_features=clip_emb_dim*clip_seq_dim)\n",
    "# mindeye.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, n_blocks=4, drop=drop,\n",
    "#                           clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim, clip_scale=1)\n",
    "utils.count_params(mindeye.ridge)\n",
    "# utils.count_params(mindeye.backbone)\n",
    "utils.count_params(mindeye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47ad9ec0-e0b5-4d2c-9d9b-027a85d176b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 132\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "507,388,672 total\n",
      "507,388,672 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in mindeye.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    # {'params': [p for n, p in mindeye.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    # {'params': [p for n, p in mindeye.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "total_steps = num_epochs * num_iterations_per_epoch\n",
    "print(\"total_steps\", total_steps)\n",
    "pct_start = 2/num_epochs if num_epochs>1 else 1.\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    ")\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(mindeye)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661e670-89b3-4598-a0ba-774e376d0047",
   "metadata": {},
   "source": [
    "# Start wandb (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eae9f959-4457-428a-afb5-45e852490ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerator.is_main_process and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found_downstream'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"mae_model_name\": mae_model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"in_dim\": in_dim,\n",
    "      \"hidden_dim\": hidden_dim,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_params\": num_params,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230f232-1700-4967-951d-fb566ae637b3",
   "metadata": {},
   "source": [
    "# Train MindEye model using foundation model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8b849b8-04b2-4908-9b46-a4fa9a565d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b91b0e3c-c939-4717-8abc-2f3b8c39d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if masking_strategy==\"MNI\":\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "MNI_brain = nib.load(\"/weka/proj-fmri/paulscotti/fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "brain_pos_voxels = MNI_brain[6:94,8:112,10:82]\n",
    "\n",
    "# brain_pos_voxels = brain_pos_voxels[:,30:31,:]\n",
    "\n",
    "brain_pos_pats = Rearrange(\n",
    "        \"b c (f pf) (d pd) (h ph) (w pw) -> b f d h w (pd ph pw pf c)\",\n",
    "        pd=patch_depth,\n",
    "        ph=patch_height,\n",
    "        pw=patch_width,\n",
    "        pf=1,\n",
    "    )(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "\n",
    "brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "    \n",
    "tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "tube_idx = mask_idx_candidates[:int(num_patches / num_frames * (1 - tube_end_masking_ratio))]\n",
    "tube_mask[tube_idx] = True\n",
    "tube_mask = tube_mask.tile(num_frames//frame_patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cac228e7-bdf8-4cc7-ab9d-891b5bd5abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multisubject stage1 ckpt if set\n",
    "if multisubject_ckpt!=\"None\" and not resume_from_ckpt:\n",
    "    load_ckpt(\"last\",outdir=multisubject_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,multisubj_loading=True)\n",
    "    \n",
    "# load saved ckpt model weights into current model\n",
    "if resume_from_ckpt:\n",
    "    load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)\n",
    "elif wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0a23e2f-eb06-4940-9b41-ca0e36dd210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "\n",
    "mindeye, optimizer, *train_dls, lr_scheduler = accelerator.prepare(mindeye, optimizer, *train_dls, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd08879b-0177-4cac-9438-a82ce0805bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downstream_framepatchsize4_bs8_300ep_contr_nolinear starting with epoch 0 / 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a889b1ee2d4d7a8acff2ffaf3a1ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc02dd1cf08486892e038df7de47cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'err' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m## Process it through pretrained MAE ##\u001b[39;00m\n\u001b[1;32m     50\u001b[0m encoder_out \u001b[38;5;241m=\u001b[39m model(voxels_raw, encoder_mask\u001b[38;5;241m=\u001b[39mtube_mask)\n\u001b[0;32m---> 51\u001b[0m \u001b[43merr\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# encoder_out = bn(encoder_out)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m voxel0 \u001b[38;5;241m=\u001b[39m encoder_out\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'err' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch, num_epochs), disable=not accelerator.is_main_process)\n",
    "mse = nn.MSELoss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "bn = nn.BatchNorm1d(512,affine=False).to(device)\n",
    "\n",
    "test_image=None\n",
    "num_test_eval=batch_size # should instead be average same-image 300 to mimic MindEye2 retrieval evaluation\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    mindeye.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    loss_clip_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    annot_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    for s, train_dl in enumerate(train_dls):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            for iter, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(tqdm(train_dl,total=num_iterations_per_epoch)):\n",
    "                image0 = images[behav0[:,0,0].cpu().long()].float()\n",
    "                image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "\n",
    "                # if images are not fully preloaded, then can do this inefficient but more memory friendly approach\n",
    "                # for ib,b in enumerate(behav0[:,0,0].cpu().long()):\n",
    "                #     if ib==0:\n",
    "                #         image0 = torch.Tensor(images[[b]])\n",
    "                #     else:\n",
    "                #         image0 = torch.vstack((image0, torch.Tensor(images[[b]])))\n",
    "                # image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "                \n",
    "                # get the corresponding raw voxel time series\n",
    "                for ib,b in enumerate(behav0[:,0,5].cpu().long().numpy()):\n",
    "                    tr = (nsddata_raw_stimuli[nsddata_raw_stimuli['global_trial'].isin([b.item()])]['global_TR_onsets'].values + TR_delay).astype(np.int32).item()\n",
    "                    if ib==0:\n",
    "                        voxels_raw = mindeye_funcs[tr-2:tr+2][None][None]\n",
    "                    else:\n",
    "                        voxels_raw = np.vstack((voxels_raw, mindeye_funcs[tr-2:tr+2][None][None]))\n",
    "                voxels_raw = torch.Tensor(voxels_raw).clamp(0,1).to(device)\n",
    "                \n",
    "                ## Process it through pretrained MAE ##\n",
    "                encoder_out = model(voxels_raw, encoder_mask=tube_mask)\n",
    "                err\n",
    "                # encoder_out = bn(encoder_out)\n",
    "                \n",
    "                voxel0 = encoder_out.flatten(1).unsqueeze(1)\n",
    "                voxel0 = nn.functional.normalize(voxel0,dim=-1).cpu()\n",
    "                \n",
    "                assert len(voxel0) == batch_size\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                    perm_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = perm\n",
    "                    betas_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = betas\n",
    "                    select_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = select\n",
    "\n",
    "                voxel_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = voxel0\n",
    "\n",
    "                if iter >= num_iterations_per_epoch:\n",
    "                    break\n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i in range(num_iterations_per_epoch):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss=0.\n",
    "\n",
    "            voxel_list = [voxel_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "            image = image_iters[train_i].detach()\n",
    "            image = image.to(device)\n",
    "\n",
    "            clip_target = clip_img_embedder(image)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                perm_list = [perm_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                perm = torch.cat(perm_list, dim=0)\n",
    "                betas_list = [betas_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                betas = torch.cat(betas_list, dim=0)\n",
    "                select_list = [select_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                select = torch.cat(select_list, dim=0)\n",
    "\n",
    "            voxel_ridge_list = [mindeye.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "            clip_voxels = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "#             backbone, clip_voxels = mindeye.backbone(voxel_ridge)\n",
    "\n",
    "            clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "            clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):                \n",
    "                loss_clip = utils.mixco_nce(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006,\n",
    "                    perm=perm, betas=betas, select=select)\n",
    "            else:\n",
    "                epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=epoch_temp)\n",
    "\n",
    "            loss_clip_total += loss_clip.item()\n",
    "            loss += loss_clip\n",
    "\n",
    "            # forward and backward top 1 accuracy        \n",
    "            labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "            fwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "            bwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    mindeye.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):  \n",
    "                loss=0.     \n",
    "\n",
    "                coco_idx = behav[:,0,0].cpu().long()\n",
    "                _,test_indices = np.unique(coco_idx, return_index=True)\n",
    "                test_indices = np.random.permutation(test_indices)[:num_test_eval]\n",
    "                image = images[coco_idx[test_indices]].float().to(device)\n",
    "                \n",
    "                # get the corresponding raw voxel time series\n",
    "                for ib,b in enumerate(behav[test_indices,0,5].cpu().long().numpy()):\n",
    "                    tr = (nsddata_raw_stimuli[nsddata_raw_stimuli['global_trial'].isin([b.item()])]['global_TR_onsets'].values + TR_delay).astype(np.int32).item()\n",
    "                    if ib==0:\n",
    "                        voxels_raw = mindeye_funcs[tr-2:tr+2][None][None]\n",
    "                    else:\n",
    "                        voxels_raw = np.vstack((voxels_raw, mindeye_funcs[tr-2:tr+2][None][None]))\n",
    "                voxels_raw = torch.Tensor(voxels_raw).clamp(0,1).to(device)\n",
    "                \n",
    "                ## Process it through pretrained MAE ##\n",
    "                encoder_out = model(voxels_raw, encoder_mask=tube_mask)\n",
    "                # encoder_out = bn(encoder_out)\n",
    "                \n",
    "                voxel = encoder_out.flatten(1).unsqueeze(1)\n",
    "                voxel = nn.functional.normalize(voxel,dim=-1)\n",
    "\n",
    "                assert len(image) == num_test_eval\n",
    "\n",
    "                clip_target = clip_img_embedder(image.float())\n",
    "                \n",
    "                clip_voxels_norm = nn.functional.normalize(voxel.flatten(1), dim=-1)\n",
    "\n",
    "                clip_voxels = mindeye.ridge(voxel,0) # 0th index of subj_list\n",
    "                \n",
    "#                 backbone, clip_voxels = mindeye.backbone(voxel_ridge)\n",
    "\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006)\n",
    "\n",
    "                test_loss_clip_total += loss_clip.item()\n",
    "                loss += loss_clip\n",
    "\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                test_fwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                test_bwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt()\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277d8b8-63ab-4741-942f-69bb0caff816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(image0,\"image0.pt\")\n",
    "# torch.save(encoder_out,\"encoder_out0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06050bc-86a2-4134-a661-ead4afe3e1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training losses\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(test_losses)\n",
    "plt.title(\"Test losses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38371ee-6e16-4db4-a096-f3f9ebdd73cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voxel_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885cc21-c8b2-4b6f-950d-913d6317ea40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "found",
   "language": "python",
   "name": "found"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
