{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import io\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "seed = 0\n",
    "import utils\n",
    "\n",
    "if utils.is_interactive():\n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2b2ad-e1ef-4262-8263-6ae9a0766caa",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127deb2-bf23-4a2e-8f86-fa8b22183546",
   "metadata": {},
   "source": [
    "### betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7063dcb1-5ae5-4a9d-a7b0-cb3323224840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5_path: /weka/proj-fmri/paulscotti/fMRI-foundation-model/src/checkpoints/NSDflat_large_gsrFalse__gpTrue_visualTrue/epoch99/test.h5\n",
      "<KeysViewHDF5 ['events', 'features', 'key', 'run', 'ses', 'start', 'sub']>\n"
     ]
    }
   ],
   "source": [
    "if utils.is_interactive():\n",
    "    # NSDflat_large_gsrFalse__gpFalse_visualTrue\n",
    "    # NSDflat_large_gsrFalse__visualTrue_RAW\n",
    "    hdf5_path = '/weka/proj-fmri/paulscotti/fMRI-foundation-model/src/checkpoints/NSDflat_large_gsrFalse__gpTrue_visualTrue/epoch99/test.h5'\n",
    "else:\n",
    "    hdf5_path = os.getenv('hdf5_path')\n",
    "    \n",
    "print(f\"hdf5_path: {hdf5_path}\")\n",
    "model_name = hdf5_path.split('/test.h5')[0].split('checkpoints')[-1].replace(\"/\",\"\")\n",
    "\n",
    "data_h5 = h5py.File(f'{hdf5_path}', 'r')\n",
    "print(data_h5.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3fb2efd8-c74a-4bec-9713-00a896e2b35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Subject: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions for sub 1: 100%|████████████| 40/40 [00:11<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Complete.\n",
      "Data Shape: (30000, 1024)\n",
      "Image NSD73K Indices Length: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load integer datasets\n",
    "sub = data_h5['sub'][:]        # Subject IDs\n",
    "ses = data_h5['ses'][:]        # Session IDs\n",
    "run = data_h5['run'][:]        # Run IDs\n",
    "start = data_h5['start'][:]    # Start indices\n",
    "\n",
    "# Load string datasets and decode them\n",
    "key = data_h5['key'][:]\n",
    "key = [k.decode('utf-8') if isinstance(k, bytes) else k for k in key]\n",
    "\n",
    "# Load and deserialize the 'events' dataset\n",
    "events_raw = data_h5['events'][:]\n",
    "events = [json.loads(e.decode('utf-8')) if isinstance(e, bytes) else json.loads(e) for e in events_raw]\n",
    "\n",
    "# Access the 'features' dataset without loading it into memory\n",
    "features = data_h5['features']\n",
    "\n",
    "num_TRs_per_image = 1  \n",
    "TR_delay = 3            \n",
    "data = [] \n",
    "image_NSD73K_indices = [] \n",
    "\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "for sub_val in [1]:\n",
    "    print(f\"\\nProcessing Subject: {sub_val}\")\n",
    "    \n",
    "    # Get indices where sub == sub_val\n",
    "    indices_sub = np.where(sub == sub_val)[0]\n",
    "    \n",
    "    # Get unique sessions for this subject\n",
    "    unique_sess = np.unique(ses[indices_sub])\n",
    "    \n",
    "    # Iterate over each session with a progress bar\n",
    "    for sess_val in tqdm(unique_sess, desc=f\"Processing sessions for sub {sub_val}\"):\n",
    "        # Get indices for current session\n",
    "        indices_sess = indices_sub[ses[indices_sub] == sess_val]\n",
    "        \n",
    "        # Get unique runs within this session\n",
    "        unique_runs = np.unique(run[indices_sess])\n",
    "        \n",
    "        # Iterate over each run\n",
    "        for run_val in unique_runs:\n",
    "            # Get indices for current run\n",
    "            indices_run = indices_sess[run[indices_sess] == run_val]\n",
    "            \n",
    "            # Find events where start == 0\n",
    "            indices_start0 = indices_run[start[indices_run] == 0]\n",
    "            \n",
    "            if len(indices_start0) == 0:\n",
    "                # No events found for this run\n",
    "                print(f\"  Run {run_val}: No events found (start == 0). Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Assuming events are consistent within a run, take the first occurrence\n",
    "            events_list = events[indices_start0[0]]\n",
    "            \n",
    "            # Extract timepoints and nsd_ids from events\n",
    "            timepoints = [event['index'] for event in events_list]\n",
    "            nsd_ids = [event['nsd_id'] - 1 for event in events_list]  # Adjusting nsd_id as per original code\n",
    "            \n",
    "            # Append nsd_ids to the main list\n",
    "            image_NSD73K_indices.extend(nsd_ids)\n",
    "            \n",
    "            # Iterate over each timepoint to extract sliding windows\n",
    "            for time in timepoints:\n",
    "                sliding_windows = []\n",
    "                \n",
    "                for i in range(num_TRs_per_image):\n",
    "                    # Calculate the adjusted time with delay\n",
    "                    time_ = time + i + TR_delay\n",
    "                    \n",
    "                    # Find the index where start == time_\n",
    "                    indices_time = indices_run[start[indices_run] == time_]\n",
    "                    \n",
    "                    if len(indices_time) == 0:\n",
    "                        # Handle missing data: Skip this sliding window\n",
    "                        print(f\"    Time {time_}: No feature found. Skipping this sliding window.\")\n",
    "                        break  # Exit the inner loop if any TR is missing\n",
    "                    else:\n",
    "                        # Access feature by index without loading the entire dataset\n",
    "                        sliding_window = features[indices_time[0]]\n",
    "                        sliding_windows.append(sliding_window)\n",
    "                \n",
    "                # Only append if the required number of TRs were found\n",
    "                if len(sliding_windows) == num_TRs_per_image:\n",
    "                    # Concatenate sliding windows if more than one TR per image\n",
    "                    if num_TRs_per_image > 1:\n",
    "                        sliding_window_array = np.concatenate(sliding_windows)\n",
    "                    else:\n",
    "                        sliding_window_array = sliding_windows[0]\n",
    "                    \n",
    "                    data.append(sliding_window_array)\n",
    "                else:\n",
    "                    print(f\"    Time {time}: Incomplete sliding window. Skipping.\")\n",
    "    \n",
    "# Convert the collected data and indices into NumPy arrays\n",
    "data = np.array(data)\n",
    "image_NSD73K_indices = np.array(image_NSD73K_indices)\n",
    "\n",
    "print(\"\\nProcessing Complete.\")\n",
    "print(\"Data Shape:\", data.shape)\n",
    "print(\"Image NSD73K Indices Length:\", len(image_NSD73K_indices))\n",
    "\n",
    "# Close the HDF5 file to free up resources\n",
    "data_h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d05c11e-b4f9-4411-8ab1-d2d515696a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sessions: 40\n",
      "n_runs: 13\n",
      "n_TRs (ses=1 & run=2): 285\n",
      "n_features: 1024\n"
     ]
    }
   ],
   "source": [
    "n_sessions = len(np.unique(ses)) \n",
    "n_runs = len(np.unique(run))\n",
    "n_TRs = np.sum((ses == 1) & (run == 2))\n",
    "n_features = len(sliding_window)\n",
    "print(f\"n_sessions: {n_sessions}\")\n",
    "print(f\"n_runs: {n_runs}\")\n",
    "print(f\"n_TRs (ses=1 & run=2): {n_TRs}\")\n",
    "print(f\"n_features: {n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda83a7-803f-4d30-9301-0b0a533409fb",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ddf09b0-18fa-4c02-b609-2f6083a159d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 73k NSD images\n",
    "data_path = \"/weka/proj-medarc/shared/mindeyev2_dataset\"\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:]\n",
    "images = torch.Tensor(images).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "27ba8ffe-0c74-4571-85f3-65fbcd0a42cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "images = images[image_NSD73K_indices]\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b80aeb2d-6d53-431c-90ed-658dca7ecebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 1024)\n",
      "torch.Size([30000, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "vox = data\n",
    "print(vox.shape)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc8fea19-420a-4dd3-9994-fad69fef7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared1000_indices = np.load(\"/weka/proj-medarc/shared/mindeyev2_dataset/shared1000.npy\")\n",
    "train_or_test = np.array([not shared1000_indices[im] for im in image_NSD73K_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8f554db1-f7cd-40d2-ab62-5d1e282c2bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "27000 3000\n"
     ]
    }
   ],
   "source": [
    "all_indices = np.arange(len(images))\n",
    "print(len(all_indices))\n",
    "train_image_indices = all_indices[train_or_test]\n",
    "test_image_indices = all_indices[~train_or_test]\n",
    "print(len(train_image_indices), len(test_image_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd088c32-0dc6-4607-9f93-a392e355b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using Connor's old NSD-Flat that uses GLM betas, for comparison\n",
    "# directory = '/weka/proj-medarc/shared/Betas-NSD-Flat/data'\n",
    "# train_parquet_files = []\n",
    "# test_parquet_files = []\n",
    "# for filename in os.listdir(directory):\n",
    "#     if (filename.endswith(\".parquet\")) and (\"train\" in filename):\n",
    "#         train_parquet_files.append(os.path.join(directory, filename))\n",
    "#     elif (filename.endswith(\".parquet\")) and (\"test\" in filename):\n",
    "#         test_parquet_files.append(os.path.join(directory, filename))\n",
    "\n",
    "# train_nsd_ids = []\n",
    "# train_vox = []\n",
    "# flatmask = None\n",
    "# for file in tqdm(train_parquet_files):\n",
    "#     df = pd.read_parquet(file)\n",
    "#     df_filtered = df[df['subject'] == 'subj01']\n",
    "#     if len(df_filtered)>0:\n",
    "#         if flatmask is None:\n",
    "#             flatmask = np.array(Image.open(io.BytesIO(df_filtered['activity'][0]['bytes']))) - 127\n",
    "#             flatmask[flatmask!=0] = 1\n",
    "        \n",
    "#         train_nsd_ids.extend(df_filtered['nsd_id'].values)\n",
    "#         for d in df_filtered['activity']:\n",
    "#             pixels = (np.array(Image.open(io.BytesIO(d['bytes'])))[flatmask.astype(np.bool)] / 255)\n",
    "#             train_vox.append(pixels)\n",
    "# train_nsd_ids = np.array(train_nsd_ids)\n",
    "# train_vox = np.array(train_vox)\n",
    "# print(\"==Train==\")\n",
    "# print(train_nsd_ids.shape)\n",
    "# print(train_vox.shape)\n",
    "\n",
    "# test_nsd_ids = []\n",
    "# test_vox = []\n",
    "# for file in tqdm(test_parquet_files):\n",
    "#     df = pd.read_parquet(file)\n",
    "#     df_filtered = df[df['subject'] == 'subj01']\n",
    "#     if len(df_filtered)>0:\n",
    "#         test_nsd_ids.extend(df_filtered['nsd_id'].values)\n",
    "#         for d in df_filtered['activity']:\n",
    "#             pixels = (np.array(Image.open(io.BytesIO(d['bytes'])))[flatmask.astype(np.bool)] / 255)\n",
    "#             test_vox.append(pixels)\n",
    "# test_nsd_ids = np.array(test_nsd_ids)\n",
    "# test_vox = np.array(test_vox)\n",
    "\n",
    "# print(\"==Test==\")\n",
    "# print(test_nsd_ids.shape)\n",
    "# print(test_vox.shape)\n",
    "\n",
    "# # discard same-image repeats for test set\n",
    "# unique_ids, first_indices = np.unique(test_nsd_ids, return_index=True)\n",
    "# sorted_indices = np.sort(first_indices)\n",
    "# test_nsd_ids = train_nsd_ids[sorted_indices]\n",
    "# test_vox = train_vox[sorted_indices]\n",
    "\n",
    "# # # group same-image repeats for test set\n",
    "# # unique_ids, inverse_indices = np.unique(test_nsd_ids, return_inverse=True)\n",
    "# # num_unique_ids = len(unique_ids)\n",
    "\n",
    "# # sum_vox = np.zeros((num_unique_ids, test_vox.shape[1]))\n",
    "# # counts = np.zeros(num_unique_ids)\n",
    "\n",
    "# # np.add.at(sum_vox, inverse_indices, test_vox)\n",
    "# # np.add.at(counts, inverse_indices, 1)\n",
    "\n",
    "# # vox_aggregated = sum_vox / counts[:, np.newaxis]\n",
    "\n",
    "# # test_nsd_ids = unique_ids\n",
    "# # test_vox = vox_aggregated\n",
    "# # print(\"   after grouping:\")\n",
    "# print(test_nsd_ids.shape)\n",
    "# print(test_vox.shape)\n",
    "\n",
    "# # Converting nsd_ids to actual images, and converting to torch tensors\n",
    "# test_images = images[test_nsd_ids]\n",
    "# images = images[train_nsd_ids]\n",
    "\n",
    "# train_mean = np.mean(train_vox,axis=0)\n",
    "# train_std = np.std(train_vox,axis=0)\n",
    "\n",
    "# vox = utils.zscore(train_vox,train_mean=train_mean,train_std=train_std)\n",
    "# test_vox = utils.zscore(test_vox,train_mean=train_mean,train_std=train_std)\n",
    "\n",
    "# vox = torch.Tensor(vox)\n",
    "# test_vox = torch.Tensor(test_vox)\n",
    "\n",
    "# train_image_indices = np.arange(len(vox))\n",
    "# test_image_indices = np.arange(len(test_vox))\n",
    "\n",
    "# print(\"\\n ready!\")\n",
    "# print(vox.shape, images.shape)\n",
    "# print(test_vox.shape, test_images.shape)\n",
    "\n",
    "# model_name = \"Betas_NSD_Flat_testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "536879cd-8863-4a70-aff4-06cb55b0b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(vox[train_image_indices],axis=0)\n",
    "train_std = np.std(vox[train_image_indices],axis=0)\n",
    "\n",
    "vox = utils.zscore(vox,train_mean=train_mean,train_std=train_std)\n",
    "print(\"inputs have been zscored according to training set\")\n",
    "\n",
    "images = torch.Tensor(images)\n",
    "vox = torch.Tensor(vox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "24bc4088-a1a9-418e-82ed-096f82487471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 3, 224, 224]) (1000, 1024)\n"
     ]
    }
   ],
   "source": [
    "# discard same-image repeats for test set\n",
    "test_images_flat = images[test_image_indices].flatten(1).numpy()\n",
    "hashes = [hashlib.sha256(im.tobytes()).hexdigest() for im in test_images_flat]\n",
    "    \n",
    "unique_ids, first_indices = np.unique(hashes, return_index=True)\n",
    "sorted_indices = np.sort(first_indices)\n",
    "test_images = images[sorted_indices]\n",
    "test_vox = vox[sorted_indices]\n",
    "\n",
    "# new test_image_indices\n",
    "test_image_indices = np.arange(len(test_images))\n",
    "\n",
    "print(test_images.shape, test_vox.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3c956cf0-b3da-435d-b406-79041899be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group same-image repeats in test set\n",
    "\n",
    "# test_images = images[test_image_indices]\n",
    "# test_vox = vox[test_image_indices]\n",
    "# print(test_images.shape, test_vox.shape)\n",
    "\n",
    "# test_images_flat = images[test_image_indices].flatten(1).numpy()\n",
    "# hashes = [hashlib.sha256(im.tobytes()).hexdigest() for im in test_images_flat]\n",
    "# hash_to_indices = defaultdict(list)\n",
    "# for idx, img_hash in enumerate(hashes):\n",
    "#     hash_to_indices[img_hash].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0a118fbd-750c-4790-b5db-482f2108cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_unique_images = len(hash_to_indices)\n",
    "# new_test_images = torch.zeros((num_unique_images, 3, 3, images.shape[-1], images.shape[-1]))  # [1000, 3, 3, 224, 224]\n",
    "# new_test_vox = torch.zeros((num_unique_images, 3, vox.shape[-1]))  # [1000, 3, vox.shape[-1]]\n",
    "\n",
    "# # Map hashes to indices\n",
    "# for new_idx, (img_hash, indices) in enumerate(hash_to_indices.items()):\n",
    "#     imgs = test_images[indices]  # Shape: [3, 3, 256, 256]\n",
    "#     datas = test_vox[indices]   # Shape: [3, 100]\n",
    "\n",
    "#     # Assign to new tensors\n",
    "#     new_test_images[new_idx] = imgs\n",
    "#     new_test_vox[new_idx] = datas\n",
    "\n",
    "# # Replace old with new tensors\n",
    "# test_images = new_test_images\n",
    "# test_vox = new_test_vox\n",
    "# del new_test_images, new_test_vox\n",
    "# print(test_images.shape, test_vox.shape)\n",
    "\n",
    "# # new test_image_indices\n",
    "# test_image_indices = np.arange(len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cc5d2e32-6027-4a19-bef4-5ca068db35bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float32 # change depending on your mixed_precision\n",
    "\n",
    "accelerator = Accelerator(split_batches=False)# mixed_precision=\"fp16\") # ['no', 'fp8', 'fp16', 'bf16']\n",
    "if utils.is_interactive(): # set batch size here if using interactive notebook instead of submitting job\n",
    "    global_batch_size = batch_size = 24\n",
    "else:\n",
    "    batch_size = int(os.environ[\"BATCH_SIZE\"])\n",
    "    global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 3653494\n",
      "device: cuda\n",
      "global_batch_size 24\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "print(\"global_batch_size\", global_batch_size)\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "# set data_type to match your mixed precision (automatically set based on deepspeed config)\n",
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    data_type = torch.bfloat16\n",
    "elif accelerator.mixed_precision == \"fp16\":\n",
    "    data_type = torch.float16\n",
    "else:\n",
    "    data_type = torch.float32\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: NSDflat_large_gsrFalse__gpTrue_visualTrueepoch99\n",
      "--data_path=/weka/proj-medarc/shared/mindeyev2_dataset                     --no-multi_subject --subj=1 --batch_size=24                     --hidden_dim=1024 --clip_scale=1.                     --no-blurry_recon --blur_scale=.5                     --no-use_prior --prior_scale=30                     --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=15 --no-use_image_aug                     --ckpt_interval=999 --no-ckpt_saving --no-wandb_log --new_test\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/weka/proj-medarc/shared/mindeyev2_dataset \\\n",
    "                    --no-multi_subject --subj=1 --batch_size={batch_size} \\\n",
    "                    --hidden_dim=1024 --clip_scale=1. \\\n",
    "                    --no-blurry_recon --blur_scale=.5 \\\n",
    "                    --no-use_prior --prior_scale=30 \\\n",
    "                    --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=15 --no-use_image_aug \\\n",
    "                    --ckpt_interval=999 --no-ckpt_saving --no-wandb_log --new_test\"# \\\n",
    "                    #--multisubject_ckpt=../../train_logs/multisubject_subj01_1024hid_nolow_300ep_seed0\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 0\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/weka/proj-fmri/shared/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=0,\n",
    "    help=\"Number of training sessions to include (if multi_subject, this variable doesnt matter)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=32,\n",
    "    help=\"Batch size can be increased by 10x if only training v2c and not diffusion diffuser\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--low_mem\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to preload images to cpu to speed things up but consume more memory\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=2,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    import kornia.augmentation as K\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    # Define the blurring augmentations\n",
    "    blur_augment = K.RandomGaussianBlur(kernel_size=(21, 21), sigma=(51.0, 51.0), p=1.)\n",
    "    \n",
    "if multi_subject:\n",
    "    subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 24 num_iterations_per_epoch = 1125 num_samples_per_epoch = 27000\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    # num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "    num_samples_per_epoch = len(train_image_indices)\n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e1942b0e-1223-40e6-b543-2f7ff2e8ebcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "\n",
    "train_data[f'subj0{subj}'] = torch.utils.data.TensorDataset(torch.tensor(train_image_indices))\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(torch.tensor(test_image_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "81084834-035f-4465-ad59-59e6b806a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 0 sessions\n",
      "num_voxels for subj01: 1024\n",
      "Loaded all subj train dls and vox!\n",
      "\n",
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for s in subj_list:\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    train_dl = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "\n",
    "    num_voxels_list.append(vox[0].shape[-1])\n",
    "    num_voxels[f'subj0{s}'] = vox[0].shape[-1]\n",
    "    voxels[f'subj0{s}'] = vox\n",
    "    print(f\"num_voxels for subj0{s}: {num_voxels[f'subj0{s}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and vox!\\n\")\n",
    "\n",
    "# Validate only on one subject\n",
    "if multi_subject: \n",
    "    subj = subj_list[0] # cant validate on the actual held out person so picking first in subj_list\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=1000, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenOpenCLIPImageEmbedder(\n",
      "  (model): CLIP(\n",
      "    (visual): VisionTransformer(\n",
      "      (conv1): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (patch_dropout): Identity()\n",
      "      (ln_pre): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "      (transformer): Transformer(\n",
      "        (resblocks): ModuleList(\n",
      "          (0-47): 48 x ResidualAttentionBlock(\n",
      "            (ln_1): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1664, out_features=1664, bias=True)\n",
      "            )\n",
      "            (ls_1): Identity()\n",
      "            (ln_2): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1664, out_features=8192, bias=True)\n",
      "              (gelu): GELU(approximate='none')\n",
      "              (c_proj): Linear(in_features=8192, out_features=1664, bias=True)\n",
      "            )\n",
      "            (ls_2): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_post): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (token_embedding): Embedding(49408, 1280)\n",
      "    (ln_final): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## USING OpenCLIP ViT-bigG ###\n",
    "sys.path.append('mindeye_utils/')\n",
    "import mindeye_utils.generative_models.sgm\n",
    "from mindeye_utils.generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "from mindeye_utils.generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "try:\n",
    "    print(clip_img_embedder)\n",
    "except:\n",
    "    # # last hidden\n",
    "    # clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    #     arch=\"ViT-bigG-14\",\n",
    "    #     version=\"laion2b_s39b_b160k\",\n",
    "    #     output_tokens=True,\n",
    "    #     only_tokens=True,\n",
    "    # )\n",
    "    # clip_img_embedder.to(device)\n",
    "    # clip_seq_dim = 256\n",
    "    # clip_emb_dim = 1664\n",
    "\n",
    "    # final\n",
    "    clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "        arch=\"ViT-bigG-14\",\n",
    "        version=\"laion2b_s39b_b160k\",\n",
    "        output_tokens=False,\n",
    "        only_tokens=False,\n",
    "    )\n",
    "    clip_img_embedder.to(device)\n",
    "    clip_seq_dim = 1\n",
    "    clip_emb_dim = 1280\n",
    "\n",
    "# ## USING OPEN AI CLIP ViT-L ###\n",
    "# import clip\n",
    "# try:\n",
    "#     print(clip_model)\n",
    "# except:\n",
    "#     clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "#     preprocess = transforms.Compose([\n",
    "#         transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "#         transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "#                              std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "#     ])\n",
    "# def clip_img_embedder(image):\n",
    "#     preproc_img = preprocess(image)\n",
    "#     return clip_model.encode_image(preproc_img)\n",
    "# clip_seq_dim = 1\n",
    "# clip_emb_dim = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule()"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "1,049,600 total\n",
      "1,049,600 trainable\n",
      "param counts:\n",
      "1,049,600 total\n",
      "1,049,600 trainable\n",
      "torch.Size([2, 1, 1024]) torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len=1): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx=0):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(self.seq_len)], dim=1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test on subject 1 with fake data\n",
    "b = torch.randn((2,1,num_voxels_list[0]))\n",
    "print(b.shape, model.ridge(b,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7b8de65a-6d3b-4248-bea9-9b6f4d562321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "14,643,736 total\n",
      "14,643,736 trainable\n",
      "param counts:\n",
      "15,693,336 total\n",
      "15,693,336 trainable\n",
      "b.shape torch.Size([2, 1, 1024])\n",
      "torch.Size([2, 1, 1280]) torch.Size([2, 1, 1280]) torch.Size([1]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, h=4096, in_dim=15724, out_dim=768, seq_len=1, n_blocks=n_blocks, drop=.15, \n",
    "                 clip_size=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "        \n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.backbone_linear = nn.Linear(h * seq_len, out_dim, bias=True) \n",
    "        if clip_scale>0:\n",
    "            self.clip_proj = self.projector(clip_size, clip_size, h=clip_size)\n",
    "            \n",
    "    def projector(self, in_dim, out_dim, h=2048):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, out_dim)\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "    \n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make empty tensors\n",
    "        c,b = torch.Tensor([0.]), torch.Tensor([[0.],[0.]])\n",
    "        \n",
    "        # Mixer blocks\n",
    "        residual1 = x\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.backbone_linear(x).reshape(len(x), -1, self.clip_size)\n",
    "        if clip_scale>0:\n",
    "            c = self.clip_proj(backbone)\n",
    "        \n",
    "        return backbone, c, b\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, \n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,1,hidden_dim))\n",
    "print(\"b.shape\",b.shape)\n",
    "\n",
    "backbone_, clip_, blur_ = model.backbone(b)\n",
    "print(backbone_.shape, clip_.shape, blur_[0].shape, blur_[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397c0d7-52a3-4153-823b-c27d2eb3eeba",
   "metadata": {},
   "source": [
    "### Adding diffusion prior + unCLIP if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 52\n",
    "    heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = PriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 16875\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "88,110,616 total\n",
      "88,110,616 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "# model.backbone.requires_grad_(False)\n",
    "\n",
    "if use_prior:\n",
    "    opt_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ])\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    if num_iterations_per_epoch==0:\n",
    "        num_iterations_per_epoch=1\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ea5cd-f7ba-4f15-8056-3cd2535bca97",
   "metadata": {},
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc7b19fa-0c75-4786-b24a-3b51e1737918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inside interactive notebook. Disabling wandb and ckpt saving...\n"
     ]
    }
   ],
   "source": [
    "if utils.is_interactive():\n",
    "    print(\"Running inside interactive notebook. Disabling wandb and ckpt saving...\")\n",
    "    wandb_log = False\n",
    "    ckpt_saving = False\n",
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found_mindeye'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "607a7c7b-fe5e-41a4-80bf-d2814b3a57cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load multisubject stage1 ckpt if set\n",
    "if multisubject_ckpt is not None and not resume_from_ckpt:\n",
    "    load_ckpt(\"last\",outdir=multisubject_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,multisubj_loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "\n",
    "model, optimizer, train_dl, lr_scheduler = accelerator.prepare(model, optimizer, train_dl, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSDflat_large_gsrFalse__visualTrue_RAWepoch99 starting with epoch 0 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_3653494/3131344150.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=data_type):\n",
      "/tmp/ipykernel_3653494/3131344150.py:144: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type):\n",
      "  7%|██▊                                        | 1/15 [04:14<59:29, 254.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(2.791205801539951), 'test/loss': np.float64(4.756008148193359), 'train/lr': 0.00015589942434296661, 'train/num_steps': 1125, 'test/num_steps': 1, 'train/fwd_pct_correct': 0.23281482119692695, 'train/bwd_pct_correct': 0.19677778357598516, 'test/test_fwd_pct_correct': 0.11800000816583633, 'test/test_bwd_pct_correct': 0.05000000074505806, 'train/loss_clip_total': 2.791205801539951, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 4.756008148193359, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████▋                                     | 2/15 [08:30<55:21, 255.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(1.4240021441777546), 'test/loss': np.float64(4.202593803405762), 'train/lr': 0.0003, 'train/num_steps': 2250, 'test/num_steps': 2, 'train/fwd_pct_correct': 0.4621481602589289, 'train/bwd_pct_correct': 0.40488889989587995, 'test/test_fwd_pct_correct': 0.16200000047683716, 'test/test_bwd_pct_correct': 0.08700000494718552, 'train/loss_clip_total': 1.4240021441777546, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 4.202593803405762, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▌                                  | 3/15 [12:45<51:00, 255.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(1.0604153644243877), 'test/loss': np.float64(3.6940548419952393), 'train/lr': 0.0002956414469630032, 'train/num_steps': 3375, 'test/num_steps': 3, 'train/fwd_pct_correct': 0.5515926084253523, 'train/bwd_pct_correct': 0.5193703854613834, 'test/test_fwd_pct_correct': 0.22700001299381256, 'test/test_bwd_pct_correct': 0.14100000262260437, 'train/loss_clip_total': 1.0604153644243877, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 3.6940548419952393, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████████▍                               | 4/15 [17:02<46:56, 256.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(0.9127149018181695), 'test/loss': np.float64(3.3637735843658447), 'train/lr': 0.0002828190911118275, 'train/num_steps': 4500, 'test/num_steps': 4, 'train/fwd_pct_correct': 0.5798518682718277, 'train/bwd_pct_correct': 0.5678889056576623, 'test/test_fwd_pct_correct': 0.2770000100135803, 'test/test_bwd_pct_correct': 0.17400000989437103, 'train/loss_clip_total': 0.9127149018181695, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 3.3637735843658447, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████▎                            | 5/15 [21:13<42:19, 253.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(0.5830798175070021), 'test/loss': np.float64(3.328577756881714), 'train/lr': 0.0002622781211611761, 'train/num_steps': 5625, 'test/num_steps': 5, 'train/fwd_pct_correct': 0.8310370570818583, 'train/bwd_pct_correct': 0.7681852051417033, 'test/test_fwd_pct_correct': 0.2850000262260437, 'test/test_bwd_pct_correct': 0.16100001335144043, 'train/loss_clip_total': 0.5830798175070021, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 3.328577756881714, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▏                         | 6/15 [25:25<37:59, 253.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(0.4280092833373282), 'test/loss': np.float64(3.0443568229675293), 'train/lr': 0.00023521230362119294, 'train/num_steps': 6750, 'test/num_steps': 6, 'train/fwd_pct_correct': 0.8765926127433776, 'train/bwd_pct_correct': 0.8232222409778172, 'test/test_fwd_pct_correct': 0.3150000274181366, 'test/test_bwd_pct_correct': 0.2290000170469284, 'train/loss_clip_total': 0.4280092833373282, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 3.0443568229675293, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████                       | 7/15 [29:41<33:54, 254.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(0.33272055599093436), 'test/loss': np.float64(2.7025370597839355), 'train/lr': 0.00020319460542705803, 'train/num_steps': 7875, 'test/num_steps': 7, 'train/fwd_pct_correct': 0.9050370572937859, 'train/bwd_pct_correct': 0.8607407600084941, 'test/test_fwd_pct_correct': 0.3500000238418579, 'test/test_bwd_pct_correct': 0.2510000169277191, 'train/loss_clip_total': 0.33272055599093436, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 2.7025370597839355, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████▉                    | 8/15 [34:00<29:50, 255.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(0.2585171893917852), 'test/loss': np.float64(2.4041671752929688), 'train/lr': 0.00016808577881821687, 'train/num_steps': 9000, 'test/num_steps': 8, 'train/fwd_pct_correct': 0.9225185389518737, 'train/bwd_pct_correct': 0.8917777981228299, 'test/test_fwd_pct_correct': 0.4050000309944153, 'test/test_bwd_pct_correct': 0.28600001335144043, 'train/loss_clip_total': 0.2585171893917852, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 2.4041671752929688, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████▊                 | 9/15 [38:13<25:29, 254.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(0.1952740568055047), 'test/loss': np.float64(2.1908323764801025), 'train/lr': 0.00013192622118178308, 'train/num_steps': 10125, 'test/num_steps': 9, 'train/fwd_pct_correct': 0.9409259461826748, 'train/bwd_pct_correct': 0.9189629832373725, 'test/test_fwd_pct_correct': 0.45000001788139343, 'test/test_bwd_pct_correct': 0.3330000042915344, 'train/loss_clip_total': 0.1952740568055047, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 2.1908323764801025, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████              | 10/15 [42:34<21:23, 256.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(0.1371368974811501), 'test/loss': np.float64(1.8875792026519775), 'train/lr': 9.681739457294188e-05, 'train/num_steps': 11250, 'test/num_steps': 10, 'train/fwd_pct_correct': 0.9594815004666646, 'train/bwd_pct_correct': 0.9402222423553467, 'test/test_fwd_pct_correct': 0.4880000352859497, 'test/test_bwd_pct_correct': 0.4280000329017639, 'train/loss_clip_total': 0.1371368974811501, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 1.8875792026519775, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|██████████████████████████████▊           | 11/15 [46:46<17:01, 255.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(0.09120522881568306), 'test/loss': np.float64(1.761939525604248), 'train/lr': 6.479969637880702e-05, 'train/num_steps': 12375, 'test/num_steps': 11, 'train/fwd_pct_correct': 0.9729629787868923, 'train/bwd_pct_correct': 0.9605926111539205, 'test/test_fwd_pct_correct': 0.5410000085830688, 'test/test_bwd_pct_correct': 0.445000022649765, 'train/loss_clip_total': 0.09120522881568306, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 1.761939525604248, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████▌        | 12/15 [50:57<12:42, 254.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(0.06175109098168711), 'test/loss': np.float64(1.6575655937194824), 'train/lr': 3.773387883882384e-05, 'train/num_steps': 13500, 'test/num_steps': 12, 'train/fwd_pct_correct': 0.9823333454661899, 'train/bwd_pct_correct': 0.9739259412553576, 'test/test_fwd_pct_correct': 0.5700000524520874, 'test/test_bwd_pct_correct': 0.47600001096725464, 'train/loss_clip_total': 0.06175109098168711, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 1.6575655937194824, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████████████████████████████████▍     | 13/15 [55:07<08:25, 252.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(0.04394651980925765), 'test/loss': np.float64(1.4168270826339722), 'train/lr': 1.7192908888172444e-05, 'train/num_steps': 14625, 'test/num_steps': 13, 'train/fwd_pct_correct': 0.9875185278786553, 'train/bwd_pct_correct': 0.9818518639140659, 'test/test_fwd_pct_correct': 0.6270000338554382, 'test/test_bwd_pct_correct': 0.5300000309944153, 'train/loss_clip_total': 0.04394651980925765, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 1.4168270826339722, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████▏  | 14/15 [59:21<04:13, 253.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(0.03444159144993561), 'test/loss': np.float64(1.3119802474975586), 'train/lr': 4.370553036996754e-06, 'train/num_steps': 15750, 'test/num_steps': 14, 'train/fwd_pct_correct': 0.9907407484584384, 'train/bwd_pct_correct': 0.9855185293091668, 'test/test_fwd_pct_correct': 0.6530000567436218, 'test/test_bwd_pct_correct': 0.5630000233650208, 'train/loss_clip_total': 0.03444159144993561, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 1.3119802474975586, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 15/15 [1:03:29<00:00, 254.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(0.028494710391490825), 'test/loss': np.float64(1.2958446741104126), 'train/lr': 1.1999999999999998e-08, 'train/num_steps': 16875, 'test/num_steps': 15, 'train/fwd_pct_correct': 0.9926296353340149, 'train/bwd_pct_correct': 0.9884444528685675, 'test/test_fwd_pct_correct': 0.6540000438690186, 'test/test_bwd_pct_correct': 0.5680000185966492, 'train/loss_clip_total': 0.028494710391490825, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 1.2958446741104126, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n",
      "NSDflat_large_gsrFalse__visualTrue_RAWepoch99\n",
      "\n",
      "===Finished!===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "for epoch in tqdm(range(epoch,num_epochs)):\n",
    "    model.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    loss_blurry_cont_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. \n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i, behav in enumerate(train_dl):  \n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0.\n",
    "            \n",
    "            behav = behav[0]\n",
    "\n",
    "            image = images[behav.long().cpu()].to(device)\n",
    "            voxel = vox[behav.long().cpu()]\n",
    "            voxel = torch.Tensor(voxel).unsqueeze(1).to(device)\n",
    "\n",
    "            if use_image_aug: \n",
    "                image = img_augment(image)\n",
    "\n",
    "            clip_target = clip_img_embedder(image)\n",
    "            if clip_target.ndim == 2: clip_target = clip_target.unsqueeze(1)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                voxel, perm, betas, select = utils.mixco(voxel)\n",
    "\n",
    "            voxel_ridge = model.ridge(voxel,0) #[model.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "            # voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "            backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge) #voxel)#voxel_ridge)\n",
    "\n",
    "            if clip_scale>0:\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if use_prior:\n",
    "                loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                loss_prior_total += loss_prior.item()\n",
    "                loss_prior *= prior_scale\n",
    "                loss += loss_prior\n",
    "\n",
    "                recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                recon_mse += mse(prior_out, clip_target).item()\n",
    "\n",
    "            if clip_scale>0:\n",
    "                if epoch < int(mixup_pct * num_epochs):                \n",
    "                    loss_clip = utils.mixco_nce(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006,\n",
    "                        perm=perm, betas=betas, select=select)\n",
    "                else:\n",
    "                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=epoch_temp)\n",
    "\n",
    "                loss_clip_total += loss_clip.item()\n",
    "                loss_clip *= clip_scale\n",
    "                loss += loss_clip\n",
    "\n",
    "            if blurry_recon:     \n",
    "                image_enc_pred, transformer_feats = blurry_image_enc_\n",
    "\n",
    "                image_enc = autoenc.encode(2*image-1).latent_dist.mode() * 0.18215\n",
    "                loss_blurry = l1(image_enc_pred, image_enc)\n",
    "                loss_blurry_total += loss_blurry.item()\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    image_enc_shuf = image_enc[perm]\n",
    "                    betas_shape = [-1] + [1]*(len(image_enc.shape)-1)\n",
    "                    image_enc[select] = image_enc[select] * betas[select].reshape(*betas_shape) + \\\n",
    "                        image_enc_shuf[select] * (1 - betas[select]).reshape(*betas_shape)\n",
    "\n",
    "                image_norm = (image - mean)/std\n",
    "                image_aug = (blur_augs(image) - mean)/std\n",
    "                _, cnx_embeds = cnx(image_norm)\n",
    "                _, cnx_aug_embeds = cnx(image_aug)\n",
    "\n",
    "                cont_loss = utils.soft_cont_loss(\n",
    "                    nn.functional.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    temp=0.2)\n",
    "                loss_blurry_cont_total += cont_loss.item()\n",
    "\n",
    "                loss += (loss_blurry + 0.1*cont_loss) * blur_scale #/.18215\n",
    "\n",
    "            if clip_scale>0:\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            if blurry_recon:\n",
    "                with torch.no_grad():\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    blurry_pixcorr += pixcorr.item()\n",
    "            \n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "                \n",
    "            if train_i >= num_iterations_per_epoch-1:\n",
    "                break\n",
    "                \n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type):   \n",
    "            for test_i, behav in enumerate(test_dl):\n",
    "                loss=0.\n",
    "                behav = behav[0]\n",
    "            \n",
    "                # image = test_images[behav.long().cpu()][:,0].to(device)\n",
    "                # voxel = test_vox[behav.long().cpu()].mean(1)\n",
    "\n",
    "                image = test_images[behav.long().cpu()].to(device)\n",
    "                voxel = test_vox[behav.long().cpu()]\n",
    "                    \n",
    "                voxel = torch.Tensor(voxel).unsqueeze(1).to(device)\n",
    "            \n",
    "                assert len(image) == 1000 #300\n",
    "            \n",
    "                clip_target = clip_img_embedder(image.float())\n",
    "                if clip_target.ndim == 2: clip_target = clip_target.unsqueeze(1)\n",
    "            \n",
    "                voxel_ridge = model.ridge(voxel,0)\n",
    "    \n",
    "                backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge)#voxel) #voxel_ridge)\n",
    "    \n",
    "                if clip_scale>0:\n",
    "                    clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                    clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                # for some evals, only doing a subset of the samples per batch because of computational cost\n",
    "                if use_prior or blurry_recon:\n",
    "                    random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                \n",
    "                if use_prior:\n",
    "                    loss_prior, contaminated_prior_out = model.diffusion_prior(text_embed=backbone[random_samps], image_embed=clip_target[random_samps])\n",
    "                    test_loss_prior_total += loss_prior.item()\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                        \n",
    "                if clip_scale>0:\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006)\n",
    "    \n",
    "                    test_loss_clip_total += loss_clip.item()\n",
    "                    loss_clip = loss_clip * clip_scale\n",
    "                    loss += loss_clip\n",
    "    \n",
    "                if blurry_recon:\n",
    "                    image_enc_pred, _ = blurry_image_enc_\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    test_blurry_pixcorr += pixcorr.item()\n",
    "    \n",
    "                if clip_scale>0:\n",
    "                    # forward and backward top 1 accuracy        \n",
    "                    labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                    test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                    test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            # if utils.is_interactive(): clear_output(wait=True)\n",
    "            print(\"---\")\n",
    "\n",
    "            # assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"train/loss_blurry_cont_total\": loss_blurry_cont_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            # if finished training, save jpg recons if they exist\n",
    "            if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                if blurry_recon:    \n",
    "                    image_enc = autoenc.encode(2*image[:4]-1).latent_dist.mode() * 0.18215\n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc_pred[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "\n",
    "                    plt.show()\n",
    "\n",
    "            print(logs)\n",
    "\n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint and reconstruct\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt(f'last')\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"{model_name}\")\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102895bd-2e35-4417-b67b-4cc64ba3e9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation_env",
   "language": "python",
   "name": "foundation_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
