{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6af0bb-19b2-472a-bd32-cd10bd997132",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a60b5f-c5db-46f5-8569-e10ac40fb8a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "import gc\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from models import *\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "import schedulers\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "print(\"reached5\")\n",
    "### Multi-GPU config ###\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"Number of available CUDA devices: {device_count}\")\n",
    "\n",
    "local_rank = os.getenv('LOCAL_RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(f\"LOCAL RANK={local_rank}\")\n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(f\"NUM GPUS={num_devices}\")\n",
    "distributed = True if num_devices>1 else False\n",
    "if distributed: assert device_count==num_devices\n",
    "\n",
    "node = os.getenv('SLURM_NODEID')\n",
    "if node is None:\n",
    "    node = 0\n",
    "else:\n",
    "    node = int(node)\n",
    "print(f\"NODE={node}\")\n",
    "\n",
    "global_rank = os.getenv('RANK')\n",
    "if global_rank is None:\n",
    "    global_rank = 0\n",
    "else:\n",
    "    global_rank = int(global_rank)\n",
    "print(f\"GLOBAL RANK={global_rank}\")\n",
    "\n",
    "world_size = os.getenv('WORLD_SIZE')\n",
    "if world_size is None: \n",
    "    world_size = 1\n",
    "else:\n",
    "    world_size = int(world_size)\n",
    "print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "# batch_size = global_batch_size // num_devices\n",
    "global_batch_size = batch_size * num_devices\n",
    "\n",
    "# FSDP Setup\n",
    "if distributed:\n",
    "    import torch.distributed as dist\n",
    "    import torch.multiprocessing as mp\n",
    "    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "    from torch.distributed.fsdp.api import BackwardPrefetch, CPUOffload, ShardingStrategy\n",
    "    import functools\n",
    "    from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy, transformer_auto_wrap_policy\n",
    "    print(\"starting init_process_group...\")\n",
    "    dist.init_process_group(\"nccl\", rank=global_rank, world_size=world_size)\n",
    "    print(f\"setting device to cuda:{local_rank}\")\n",
    "    try:\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.device('cuda',local_rank)\n",
    "        print(f\"\\nSuccessfully set cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "    except Exception as error:        \n",
    "        print(f\"\\nFAILED TO SET DEVICE cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "        print(\"An exception occurred:\", error)\n",
    "        \n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "print(\"device =\", device, \"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de5a1c-ee37-4e61-9ce6-6e2fae1dc7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(config)\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "if use_contrastive_loss:\n",
    "    global_batch_size = global_batch_size // 2 # contrastive loss doubles the batch size with the same samples and different masks\n",
    "print(\"global_batch_size\", global_batch_size)\n",
    "\n",
    "use_cls_token = True if use_contrastive_loss else use_cls_token\n",
    "print(\"use_cls_token\", use_cls_token)\n",
    "\n",
    "num_patches = int(\n",
    "    (image_size[0] / patch_size)\n",
    "    * (image_size[1] / patch_size)\n",
    "    * (image_size[2] / patch_size)\n",
    "    * num_frames\n",
    ")\n",
    "num_patches_per_timepoint = num_patches // num_frames\n",
    "num_encoder_patches = int(num_patches_per_timepoint * (1 - x_encoder_start_masking_ratio) * num_frames)\n",
    "num_decoder_patches = int(num_patches_per_timepoint * (1 - y_encoder_mask_ratio) * num_frames)\n",
    "print(\"num_patches\", num_patches)\n",
    "print(\"num_encoder_patches\", num_encoder_patches)\n",
    "print(\"num_decoder_patches\", num_decoder_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee5be68-d850-42e9-a693-b78f22a1d19b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test to Check Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915c828-19d2-49a5-b7bc-b170fcfaecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_depth, image_height, image_width = image_size\n",
    "image_patch_size=(patch_size,patch_size,patch_size)\n",
    "patch_depth, patch_height, patch_width = image_patch_size\n",
    "patch_dim = patch_depth * patch_height * patch_width * frame_patch_size\n",
    "\n",
    "x_encoder = Transformer(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    patch_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"x_encoder\")\n",
    "print(utils.count_params(x_encoder))\n",
    "y_encoder = Transformer(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    patch_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"y_encoder\")\n",
    "print(utils.count_params(y_encoder))\n",
    "predictor = Transformer(\n",
    "    embed_dim,\n",
    "    depth // 2,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    patch_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"predictor\")\n",
    "print(utils.count_params(predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b543f5a-65a9-4294-b708-b5ec67dd0fff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SimpleViT(\n",
    "    x_encoder=x_encoder,\n",
    "    y_encoder=y_encoder,\n",
    "    predictor=predictor,\n",
    "    image_size=image_size, \n",
    "    image_patch_size=image_patch_size, \n",
    "    num_frames=num_frames,\n",
    "    frame_patch_size=frame_patch_size,\n",
    "    channels=1,\n",
    "    use_rope_emb=use_rope_emb,\n",
    "    use_cls_token=use_cls_token,\n",
    ")\n",
    "model = model.to(device)\n",
    "utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ba6bb-150a-498f-98fb-29deef53389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_transform = utils.DataPrepper(\n",
    "    num_frames=num_frames,\n",
    "    masking_strategy=masking_strategy,\n",
    "    patch_depth=patch_size,\n",
    "    patch_height=patch_size,\n",
    "    patch_width=patch_size,\n",
    "    frame_patch_size=frame_patch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f0441c-1e4d-40d2-b2fc-f0dc560e0a56",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce27aac-53e0-4960-a19a-edb634b51d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import create_dataset, create_loader\n",
    "\n",
    "if not is_s3:\n",
    "    print(train_urls)\n",
    "    if distributed: dist.barrier()\n",
    "    train_dp = create_dataset(train_urls, \n",
    "                              is_s3=is_s3, \n",
    "                              sample_shuffle=1, shard_shuffle=100)\n",
    "    train_dl = create_loader(train_dp, batch_size=batch_size, num_workers=num_workers)\n",
    "else:\n",
    "    print(\"Dataloading from s3!\")\n",
    "    train_urls = s3_train_urls\n",
    "    print(train_urls)\n",
    "    train_dp = create_dataset(train_urls, \n",
    "                              is_s3=is_s3, \n",
    "                              sample_shuffle=1, shard_shuffle=100)\n",
    "    train_dl = create_loader(train_dp, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e3be1-6075-42d4-9171-0b2350883f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not distributed:\n",
    "    num_it = 2\n",
    "    print(f\"Yielding {num_it} batches\")\n",
    "    \n",
    "    for i, batch in enumerate(train_dl):\n",
    "        print(\"iter\",i)\n",
    "        input_func = batch['func.npy']\n",
    "        if i >= (num_it-1):\n",
    "            break\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    print(\"input_func\", input_func.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a5ce1-6dd1-42c9-82c8-d402e7317ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_parameters = [\n",
    "    {'params': (p for n, p in model.x_encoder.named_parameters() if ('bias' not in n) and (len(p.shape) != 1))},\n",
    "    {'params': (p for n, p in model.x_encoder.named_parameters() if ('bias' in n) or (len(p.shape) == 1)), \n",
    "     'WD_exclude': True,'weight_decay': 0,},\n",
    "]\n",
    "\n",
    "predictor_parameters = [\n",
    "     {'params': (p for n, p in model.predictor.named_parameters() if ('bias' not in n) and (len(p.shape) != 1))},\n",
    "    {'params': (p for n, p in model.predictor.named_parameters() if ('bias' in n) or (len(p.shape) == 1)), \n",
    "     'WD_exclude': True,'weight_decay': 0,},\n",
    "]\n",
    "\n",
    "    \n",
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        utils.trunc_normal_(m.weight, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, torch.nn.LayerNorm):\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        torch.nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "for m in model.x_encoder.modules():\n",
    "    init_weights(m)\n",
    "for m in  model.predictor.modules():\n",
    "    init_weights(m)\n",
    "\n",
    "model.y_encoder = copy.deepcopy(model.x_encoder)\n",
    "\n",
    "for p in model.y_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "    \n",
    "#optimizer = torch.optim.AdamW(opt_grouped_parameters)\n",
    "encoder_optimizer = torch.optim.AdamW(encoder_parameters,lr=0.00003)\n",
    "predictor_optimizer = torch.optim.AdamW(predictor_parameters,lr=0.0003)\n",
    "\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // global_batch_size\n",
    "print(\"num_iterations_per_epoch\", num_iterations_per_epoch)\n",
    "total_steps = num_epochs * num_iterations_per_epoch * num_devices\n",
    "print(\"total_steps\", total_steps)\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)\n",
    "\n",
    "momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(total_steps*ipe_scale)\n",
    "                          for i in range(int(total_steps*ipe_scale)+1))\n",
    "count=0\n",
    "print(\"\\nmomentum_scheduler set\")\n",
    "\n",
    "#lr_scheduler = schedulers.WarmupCosineSchedule(\n",
    "        #optimizer,\n",
    "        #warmup_steps=int(warmup * num_iterations_per_epoch),\n",
    "        #start_lr=start_lr,\n",
    "        #ref_lr=lr,\n",
    "        #final_lr=final_lr,\n",
    "        #T_max=int(ipe_scale * num_epochs * num_iterations_per_epoch),\n",
    "# )\n",
    "#print(\"\\nlr_scheduler set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0f425-076c-46e4-9184-4475a44dad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if masking_strategy==\"MNI\":\n",
    "    from einops.layers.torch import Rearrange\n",
    "    \n",
    "    MNI_brain = nib.load(\"/scratch/gpfs/ks9249/fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "    brain_pos_voxels = MNI_brain[6:94,8:112,10:82]\n",
    "    #brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "    #brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "    brain_pos_pats = Rearrange(\n",
    "            \"b c (f pf) (d pd) (h ph) (w pw) -> b f d h w (pd ph pw pf c)\",\n",
    "            pd=patch_size,\n",
    "            ph=patch_size,\n",
    "            pw=patch_size,\n",
    "            pf=1,\n",
    "        )(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "    \n",
    "    brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233c03de-3475-4ffa-872b-ca6d9c98103d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_ckpt_path = outdir+f'/last.pth'\n",
    "\n",
    "def save_ckpt(model,tag=\"last\"):\n",
    "    if distributed: dist.barrier()\n",
    "    model_states = model.state_dict()\n",
    "    if global_rank == 0:\n",
    "        ckpt_path = outdir+f'/{tag}.pth'\n",
    "        os.makedirs(outdir,exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_states,\n",
    "            #'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, ckpt_path)\n",
    "        print(f\"\\n---saved {ckpt_path}!---\\n\")\n",
    "        # save config.yaml copy\n",
    "        with open(f'{outdir}/config.yaml', 'w') as file:\n",
    "            yaml.dump(config, file)\n",
    "\n",
    "def resume_ckpt(model, optimizer, device, ckpt_path=default_ckpt_path):\n",
    "    if global_rank == 0:\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        #lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        epoch = 0\n",
    "    if distributed: dist.barrier()\n",
    "    torch.cuda.empty_cache()\n",
    "    return model, optimizer, lr_scheduler, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e45f79-e050-4173-b42d-005fc66a2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"]=\"offline\"\n",
    "\n",
    "if utils.is_interactive():\n",
    "    wandb_log = False\n",
    "if global_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"depth\": depth,\n",
    "      \"mlp_dim\": mlp_dim,\n",
    "      \"x_encoder_start_masking_ratio\": x_encoder_start_masking_ratio,\n",
    "      \"x_encoder_end_masking_ratio\": x_encoder_end_masking_ratio,\n",
    "      \"y_encoder_mask_ratio\": y_encoder_mask_ratio,\n",
    "      \"num_frames\": num_frames,\n",
    "      \"patch_size\": patch_size,\n",
    "      \"frame_patch_size\": frame_patch_size,\n",
    "      \"use_contrastive_loss\": use_contrastive_loss,\n",
    "      \"use_cls_token\": use_cls_token,\n",
    "      \"constrastive_loss_weight\": constrastive_loss_weight,\n",
    "      \"num_params\": num_params,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_urls\": train_urls,\n",
    "      \"is_s3\": is_s3,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5102a6b4-8587-4121-b422-a8232ea9898c",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7ed4a-2abf-42af-b1ca-798c703d00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "lrs, recon_losses, contrastive_losses, train_reg, grad_norms_encoder, grad_norms_predictor, train_cos = [], [], [], [], [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a97f07-9781-45e3-bb49-76a667a3dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume_from_ckpt is True:\n",
    "    if os.path.exists(default_ckpt_path):\n",
    "        print(f\"Resuming from {default_ckpt_path}...\")\n",
    "        model, optimizer, lr_scheduler, epoch = resume_ckpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e4373-0b1b-444d-8b81-749e387096a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(z, h):\n",
    "    loss = 0.\n",
    "    # Compute loss and accumulate for each mask-enc/mask-pred pair\n",
    "    for zi, hi in zip(z, h):\n",
    "        loss += torch.mean(torch.abs(zi - hi)**loss_exp) / loss_exp\n",
    "    loss /= len(z)\n",
    "    return loss\n",
    "\n",
    "def reg_fn(z):\n",
    "    return sum([torch.sqrt(zi.var(dim=1) + 0.0001) for zi in z]) / len(z)\n",
    "\n",
    "def cos_fn(z):\n",
    "    # Flatten z from [batch, channels, height, width] to [batch, -1]\n",
    "    z_flat = z.flatten(1)\n",
    "    # Compute normalized vectors to prevent division by zero\n",
    "    z_norm = torch.nn.functional.normalize(z_flat, p=2, dim=1)\n",
    "    # Compute cosine similarity matrix\n",
    "    cossim_matrix = torch.mm(z_norm, z_norm.transpose(0, 1))\n",
    "    # Compute the sum of cosine similarities of distinct pairs\n",
    "    cossim_sum = (cossim_matrix.sum() - len(z))/2\n",
    "    # Compute the total number of distinct pairs\n",
    "    cossim_num = len(z) * (len(z) - 1) / 2\n",
    "    # Return the average cosine similarity for distinct pairs\n",
    "    return cossim_sum / cossim_num\n",
    "\n",
    "l1 = nn.L1Loss() #Following VJEPA architecture, which uses L1 loss not L2 loss\n",
    "\n",
    "if use_contrastive_loss:\n",
    "    logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))  # learned logit scale\n",
    "    \n",
    "if distributed: dist.barrier()\n",
    "progress_bar = tqdm(range(epoch, num_epochs), disable=global_rank!=0)\n",
    "for epoch in progress_bar:\n",
    "    # get the masking ratio for the current epoch\n",
    "    tube_mask_ratio = utils.get_masking_ratio(\n",
    "        current_epoch=epoch, \n",
    "        total_epochs=num_epochs, \n",
    "        start_masking_ratio=x_encoder_start_masking_ratio, \n",
    "        end_masking_ratio=x_encoder_end_masking_ratio\n",
    "    )\n",
    "    with torch.cuda.amp.autocast(dtype=data_type):\n",
    "        model.train()\n",
    "        for train_i, batch in enumerate(train_dl):\n",
    "            #optimizer.zero_grad()\n",
    "            encoder_optimizer.zero_grad()\n",
    "            predictor_optimizer.zero_grad()\n",
    "            \n",
    "            input_func = batch['func.npy']\n",
    "            \n",
    "            if masking_strategy==\"MNI\":\n",
    "                func, _ = aug_transform(input_func)\n",
    "            else:\n",
    "                func, brain_pos_voxels = aug_transform(input_func)\n",
    "                brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "                brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "                \n",
    "            if use_contrastive_loss:  # create positive pairs by duplicating the batch\n",
    "                func = torch.cat([func, func], dim=0)\n",
    "                meansd = torch.cat([meansd, meansd], dim=0)\n",
    "                brain_pos_pats = torch.cat([brain_pos_pats, brain_pos_pats], dim=0)\n",
    "                \n",
    "            func = func.unsqueeze(1).to(device)\n",
    "\n",
    "            # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "            \n",
    "            tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "            mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "            mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "            tube_idx = mask_idx_candidates[:int(num_patches / num_frames * (1 - tube_mask_ratio))]\n",
    "            tube_mask[tube_idx] = True\n",
    "            tube_mask = tube_mask.tile(num_frames//frame_patch_size)\n",
    "            # print(\"before encoder\");utils.print_cuda_memory_usage()\n",
    "            \n",
    "            # feed into x-encoder\n",
    "            xencoder_out = model(func, encoder_mask=tube_mask, encoder_type = \"x\", device = device)\n",
    "            # print(\"x_encoder\");utils.print_cuda_memory_usage()\n",
    "            \n",
    "            # feed entire func into y-encoder\n",
    "            with torch.no_grad():\n",
    "                yencoder_out = model(func, encoder_mask=tube_mask, encoder_type = \"y\", device = device)\n",
    "            # print(\"y_encoder\");utils.print_cuda_memory_usage()\n",
    "            \n",
    "            # feed output of x-encoder into predictor\n",
    "            predictor_out = model(xencoder_out, encoder_mask=tube_mask, encoder_type=\"p\", device = device)\n",
    "            # print(\"predictor\");utils.print_cuda_memory_usage()\n",
    "\n",
    "\n",
    "            # compare output of predictor to output of y-encoder and calculate L1 Loss\n",
    "            loss = l1(predictor_out,yencoder_out)  # jepa prediction loss\n",
    "            loss_cos = cos_fn(yencoder_out)   # cosine similarity accross batch\n",
    "\n",
    "            \n",
    "            # contrastive loss\n",
    "            if use_contrastive_loss:\n",
    "                n_b = len(func) // 2\n",
    "                cls_token1 = enc_cls_token[:n_b, 0, :]  # first half of batch, cls_token shape B, 1, d_model\n",
    "                cls_token2 = enc_cls_token[n_b:, 0, :]\n",
    "                contrastive_loss = utils.contrastive_loss(cls_token1, cls_token2, temperature=logit_scale)\n",
    "                loss += constrastive_loss_weight * contrastive_loss\n",
    "                contrastive_losses.append(contrastive_loss.item())\n",
    "\n",
    "            \n",
    "\n",
    "            if train_i==0 and epoch==0:\n",
    "                print(\"calculated first loss\")\n",
    "            if train_i==1 and epoch==0:\n",
    "                print(\"reached train_i=1\")\n",
    "            if train_i==0 and epoch==1:\n",
    "                print(\"reached epoch1\")\n",
    "            \n",
    "            # backwards + step\n",
    "            loss.backward()\n",
    "            # clip gradient\n",
    "            #torch.nn.utils.clip_grad_norm_(model.x_encoder.parameters(), 1)\n",
    "            #torch.nn.utils.clip_grad_norm_(model.predictor.parameters(), 1)\n",
    "            #optimizer.step()\n",
    "            encoder_optimizer.step()\n",
    "            predictor_optimizer.step()\n",
    "            #lr_scheduler.step()\n",
    "\n",
    "            \n",
    "            recon_losses.append(loss.item())\n",
    "            #lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "            train_cos.append(loss_cos.item())\n",
    "\n",
    "            #calculate gradient norms to monitor for instability\n",
    "            grads_encoder = [\n",
    "                param.grad.detach().flatten()\n",
    "                for param in model.x_encoder.parameters()\n",
    "                if param.grad is not None\n",
    "            ]\n",
    "            grads_predictor = [\n",
    "                param.grad.detach().flatten()\n",
    "                for param in model.predictor.parameters()\n",
    "                if param.grad is not None\n",
    "            ]\n",
    "            norm_encoder = torch.cat(grads_encoder).norm()\n",
    "            norm_predictor = torch.cat(grads_predictor).norm()\n",
    "            grad_norms_encoder.append(norm_encoder.item())\n",
    "            grad_norms_predictor.append(norm_predictor.item())\n",
    "                \n",
    "            # update y-encoder using exponential-moving average of x-encoder params to prevent collapse\n",
    "            m = next(momentum_scheduler)\n",
    "            with torch.no_grad():\n",
    "                for param_q, param_k in zip(model.x_encoder.parameters(), model.y_encoder.parameters()):\n",
    "                    param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
    "\n",
    "            if train_i==0 and epoch==0:\n",
    "                print(\"finished first epoch!\")\n",
    "\n",
    "            if train_i >= (num_iterations_per_epoch-1):\n",
    "                print(\"train_i\", train_i, \"local_rank\", local_rank, \"global_rank\", global_rank)\n",
    "                break\n",
    "\n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(recon_losses[-(train_i + 1) :]),\n",
    "            \"train/num_steps\": len(recon_losses),\n",
    "            #\"lr\": np.mean(lrs[-(train_i + 1) :]),\n",
    "            \"epoch\": epoch,\n",
    "            \"tube_mask_ratio\": tube_mask_ratio,\n",
    "            #\"train/loss_reg\": np.mean(train_reg[-(train_i + 1) :]),\n",
    "            \"train/loss_cos\": np.mean(train_cos[-(train_i + 1) :]),\n",
    "            \"grad_norm_encoder\": np.mean(grad_norms_encoder[-(train_i + 1) :]),\n",
    "            \"grad_norm_predictor\": np.mean(grad_norms_predictor[-(train_i + 1) :]),\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        if distributed: print(logs)\n",
    "            \n",
    "        if global_rank==0:\n",
    "            if wandb_log: wandb.log(logs)\n",
    "                    \n",
    "        # Save model checkpoint\n",
    "        if (ckpt_saving) and ((epoch % ckpt_interval == 0) or (epoch==num_epochs-1)):\n",
    "            save_ckpt(model,\"last\")\n",
    "            \n",
    "        # wait for other GPUs to catch up if needed\n",
    "        if distributed: dist.barrier()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "if distributed:\n",
    "    dist.barrier()\n",
    "    dist.destroy_process_group()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
