{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0859a181-0681-489e-9939-8d47614b9fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CUDA devices: 1\n",
      "LOCAL RANK=0\n",
      "NUM GPUS=1\n",
      "NODE=0\n",
      "GLOBAL RANK=0\n",
      "WORLD_SIZE=1\n",
      "PID of this process = 1604904\n",
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from models import get_vit\n",
    "from mindeye_models import *\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('LOCAL_RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(f\"LOCAL RANK={local_rank}\")\n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(f\"NUM GPUS={num_devices}\")\n",
    "distributed = True if num_devices>1 else False\n",
    "\n",
    "global_rank = os.getenv('RANK')\n",
    "if global_rank is None:\n",
    "    global_rank = 0\n",
    "else:\n",
    "    global_rank = int(global_rank)\n",
    "print(f\"GLOBAL RANK={global_rank}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "# mae_model_name = model_name\n",
    "mae_model_name = \"MAE_2node_2gpu_32gbs_b_hybridshard\"\n",
    "    \n",
    "# Load MindEye parameters from yaml config (will override any params with same name)\n",
    "mindeye_config = yaml.load(open('mindeye_config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in mindeye_config.keys():\n",
    "    globals()[attribute_name] = mindeye_config[f'{attribute_name}']\n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "\n",
    "if utils.is_interactive(): # set batch size here if using interactive notebook instead of submitting job\n",
    "    global_batch_size = batch_size = 8\n",
    "else:\n",
    "    batch_size = global_batch_size // num_devices\n",
    "    \n",
    "# First use \"accelerate config\" in terminal and setup using deepspeed stage 2 with CPU offloading\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "\n",
    "from accelerate.state import AcceleratorState\n",
    "try:\n",
    "    AcceleratorState().deepspeed_plugin.deepspeed_config['train_micro_batch_size_per_gpu'] = batch_size\n",
    "    print(\"deepspeed reconfigured, train_micro_batch_size_per_gpu = \", batch_size)\n",
    "except:\n",
    "    print(\"skipping deepspeed reconfiguration...\")\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b35bc5-8674-4061-a606-f44249d167df",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9fa256a-cc44-4346-8d6c-1bed3779d5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae config\n",
      "\n",
      " {'model_name': 'MAE_2node_2gpu_32gbs_b_hybridshard', 'use_cls_token': False, 'use_contrastive_loss': False, 'constrastive_loss_weight': 1.0, 'global_batch_size': 128, 'num_workers': 4, 'num_epochs': 30, 'seed': 42, 'max_lr': 3e-05, 'num_samples_per_epoch': 1024, 'ckpt_saving': True, 'ckpt_interval': 50, 'resume_from_ckpt': True, 'wandb_log': True, 'tube_start_masking_ratio': 0.95, 'tube_end_masking_ratio': 0.95, 'decoder_mask_ratio': 0.95, 'encoder_model': 'vit_base', 'decoder_model': 'vit_small', 'patch_size': 8, 'frame_patch_size': 1, 'use_rope_emb': False, 'masking_strategy': 'MNI', 'img_size': [88, 104, 72], 'num_frames': 4, 'is_s3': False, 'train_urls': '/weka/proj-fmri/paulscotti/old_fMRI-foundation-model/dataset_creation/wds_creation/wds/{000001..000240}.tar'}\n",
      "mindeye_config\n",
      " {'model_name': 'ME_patch8_60ep_nopretrain', 'global_batch_size': 4, 'mixed_precision': 'fp16', 'num_epochs': 60, 'seed': 42, 'max_lr': 0.0003, 'num_samples_per_epoch': 512, 'multi_subject': False, 'multisubject_ckpt': 'None', 'ckpt_saving': False, 'ckpt_interval': 99, 'resume_from_ckpt': False, 'wandb_log': False, 'in_dim': 1480704, 'hidden_dim': 512, 'drop': 0.15, 'mixup_pct': 0.33, 'nsd_wds_path': '/weka/proj-fmri/shared/mindeyev2_dataset/wds', 'nsd_raw_path': '/weka/proj-fmri/shared/mindeyev2_dataset', 'nsd_image_path': '/weka/proj-fmri/shared/mindeyev2_dataset', 'num_sessions': 3}\n",
      "mae_ckpt_pth /weka/proj-fmri/paulscotti/fMRI-foundation-model/ckpts/MAE_2node_2gpu_32gbs_b_hybridshard/last.pth\n",
      "outdir /weka/proj-fmri/paulscotti/fMRI-foundation-model/ckpts/ME_patch8_60ep_nopretrain\n",
      "use_cls_token False\n",
      "num_patches 5148\n"
     ]
    }
   ],
   "source": [
    "print(\"mae config\\n\\n\",config)\n",
    "print(\"mindeye_config\\n\",mindeye_config)\n",
    "\n",
    "if utils.is_interactive():\n",
    "    ckpt_saving = False\n",
    "    wandb_log = False\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "mae_ckpt_pth = os.path.abspath(f'../ckpts/{mae_model_name}/last.pth')\n",
    "print(\"mae_ckpt_pth\", mae_ckpt_pth)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "use_cls_token = True if use_contrastive_loss else use_cls_token\n",
    "print(\"use_cls_token\", use_cls_token)\n",
    "\n",
    "num_patches = int(\n",
    "    (img_size[0] / patch_size)\n",
    "    * (img_size[1] / patch_size)\n",
    "    * (img_size[2] / patch_size)\n",
    "    * num_frames\n",
    ")\n",
    "print(\"num_patches\", num_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8115109a-30a1-448e-a1ea-1b115ed8cb6d",
   "metadata": {},
   "source": [
    "# Load pretrained foundation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b40287e-bab2-4791-9407-e63ad5b0a3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "107,184,768 total\n",
      "107,184,768 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "107184768"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_size = {\n",
    "    \"encoder\": encoder_model,\n",
    "    \"decoder\": decoder_model\n",
    "}\n",
    "    \n",
    "model = get_vit(\n",
    "    size=vit_size,\n",
    "    image_size=img_size,  # depth, height, width\n",
    "    image_patch_size=(patch_size,patch_size,patch_size),  # depth, height, width patch size\n",
    "    frames=num_frames,\n",
    "    frame_patch_size=frame_patch_size,\n",
    "    channels=1,\n",
    "    use_rope_emb=use_rope_emb,\n",
    "    use_cls_token=use_cls_token,\n",
    ")\n",
    "utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b45ad3-e8d5-40cd-b041-1defdbf15155",
   "metadata": {},
   "source": [
    "## Load pretrained ckpt for MAE foundation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62bb3f5f-cf7f-42c5-ab8c-d4bf143ba727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_mae_ckpt(model, ckpt_path=mae_ckpt_pth):\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c426853-d268-4daf-ac19-faa40c2cd362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = load_mae_ckpt(model, ckpt_path=mae_ckpt_pth)\n",
    "print(\"NOT PRETRAINED\")\n",
    "\n",
    "# set foundation model to evaluation\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd89a4-955f-439d-b3c9-8f261c06eed2",
   "metadata": {},
   "source": [
    "# Setup MindEye model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dae53d1-abff-4ddb-98cd-2cab04b9f0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nsddata_raw_stimuli = pd.read_csv(f\"{nsd_raw_path}/nsddata_rawdata.csv\")\n",
    "TR_delay = 3 # to account for bold hrf\n",
    "train_TRs = np.round(nsddata_raw_stimuli[nsddata_raw_stimuli['shared1000'] == False]['global_TR_onsets'].values + TR_delay).astype(np.int32)\n",
    "test_TRs = np.round(nsddata_raw_stimuli[nsddata_raw_stimuli['shared1000'] == True]['global_TR_onsets'].values + TR_delay).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d5015e9-120e-4240-886c-6959796e8faf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images! torch.Size([73000, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{nsd_image_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:] \n",
    "images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images!\", images.shape)\n",
    "\n",
    "# Load MindEye hdf5\n",
    "f = h5py.File(f'{nsd_raw_path}/subj01_mnidata.h5', 'r') #subj01_rawdata_old.h5\n",
    "mindeye_global_trs = f['global_trs'][:]\n",
    "mindeye_funcs = f['funcs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d79bcf35-45a6-4dc5-90da-c96c3a596401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=\"ViT-bigG-14\",\n",
    "    version=\"laion2b_s39b_b160k\",\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "clip_img_embedder.to(device)\n",
    "clip_seq_dim, clip_emb_dim = 256, 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d89d95f0-b52e-439f-ba5d-c13e2897fe31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 8 num_iterations_per_epoch = 281 num_samples_per_epoch = 2250\n",
      "Training with 3 sessions\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/train/{0..2}.tar\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/new_test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subj = s = 1\n",
    "subj_list = [subj]\n",
    "\n",
    "num_samples_per_epoch = (750*num_sessions) // num_devices\n",
    "\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)\n",
    "\n",
    "train_data = {}\n",
    "train_dl = {}\n",
    "\n",
    "print(f\"Training with {num_sessions} sessions\")\n",
    "train_url = f\"{nsd_wds_path}/subj0{s}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "print(train_url)\n",
    "    \n",
    "train_data[f'subj0{s}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=utils.my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "# train_dl[f'subj0{s}'] = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "train_dl[f'subj0{s}'] = wds.WebLoader(\n",
    "    train_data[f'subj0{s}'].batched(batch_size), \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    batch_size=None,\n",
    "    num_workers=num_workers, \n",
    "    persistent_workers=num_workers>0,\n",
    ").with_epoch(num_iterations_per_epoch)\n",
    "\n",
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "if subj==3:\n",
    "    num_test=2371\n",
    "elif subj==4:\n",
    "    num_test=2188\n",
    "elif subj==6:\n",
    "    num_test=2371\n",
    "elif subj==8:\n",
    "    num_test=2188\n",
    "else:\n",
    "    num_test=3000\n",
    "test_url = f\"{nsd_wds_path}/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "test_data = wds.WebDataset(test_url,resampled=True,nodesplitter=utils.my_split_by_node)\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "# test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "test_dl = wds.WebLoader(\n",
    "    test_data.batched(num_test), \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    batch_size=None,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=num_workers>0,\n",
    ").with_epoch(num_iterations_per_epoch)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e100e116-4b98-4c15-bd8e-252871e72517",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "758,120,960 total\n",
      "758,120,960 trainable\n",
      "param counts:\n",
      "228,956,824 total\n",
      "228,956,824 trainable\n",
      "param counts:\n",
      "987,077,784 total\n",
      "987,077,784 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "987077784"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len=1): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(self.seq_len)], dim=1)\n",
    "        return out\n",
    "\n",
    "mindeye = MindEyeModule()\n",
    "mindeye.ridge = RidgeRegression(np.array([in_dim]), out_features=hidden_dim)\n",
    "mindeye.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, n_blocks=4, drop=drop,\n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim, clip_scale=1)\n",
    "utils.count_params(mindeye.ridge)\n",
    "utils.count_params(mindeye.backbone)\n",
    "utils.count_params(mindeye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47ad9ec0-e0b5-4d2c-9d9b-027a85d176b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 16860\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "987,077,784 total\n",
      "987,077,784 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in mindeye.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in mindeye.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in mindeye.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "total_steps = num_epochs * num_iterations_per_epoch\n",
    "print(\"total_steps\", total_steps)\n",
    "pct_start = 2/num_epochs if num_epochs>1 else 1.\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    ")\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(mindeye)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661e670-89b3-4598-a0ba-774e376d0047",
   "metadata": {},
   "source": [
    "# Start wandb (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eae9f959-4457-428a-afb5-45e852490ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerator.is_main_process and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found_downstream'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"mae_model_name\": mae_model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"in_dim\": in_dim,\n",
    "      \"hidden_dim\": hidden_dim,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_params\": num_params,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230f232-1700-4967-951d-fb566ae637b3",
   "metadata": {},
   "source": [
    "# Train MindEye model using foundation model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8b849b8-04b2-4908-9b46-a4fa9a565d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b91b0e3c-c939-4717-8abc-2f3b8c39d012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1928) tensor(0.3745)\n"
     ]
    }
   ],
   "source": [
    "MNI_brain = nib.load(\"/weka/proj-fmri/paulscotti/old_fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "brain_pos_voxels = MNI_brain[6:94,8:112,10:82]\n",
    "brain_pos_voxels[6:94,:(112-60),10:62] = 0\n",
    "brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "tube_mask = (brain_pos_pats_vit > 0).tile(num_frames)\n",
    "print(tube_mask.sum(), tube_mask.sum() / len(tube_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cac228e7-bdf8-4cc7-ab9d-891b5bd5abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multisubject stage1 ckpt if set\n",
    "if multisubject_ckpt!=\"None\" and not resume_from_ckpt:\n",
    "    load_ckpt(\"last\",outdir=multisubject_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,multisubj_loading=True)\n",
    "    \n",
    "# load saved ckpt model weights into current model\n",
    "if resume_from_ckpt:\n",
    "    load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)\n",
    "elif wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0a23e2f-eb06-4940-9b41-ca0e36dd210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "\n",
    "mindeye, optimizer, *train_dls, lr_scheduler = accelerator.prepare(mindeye, optimizer, *train_dls, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd08879b-0177-4cac-9438-a82ce0805bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ME_patch8_60ep_nopretrain starting with epoch 0 / 60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2310fe8191d545c7b5de8949ee0b28f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m## Process it through pretrained MAE ##\u001b[39;00m\n\u001b[1;32m     48\u001b[0m encoder_out \u001b[38;5;241m=\u001b[39m model(voxels_raw, encoder_mask\u001b[38;5;241m=\u001b[39mtube_mask)\n\u001b[0;32m---> 49\u001b[0m voxel0 \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(voxel0) \u001b[38;5;241m==\u001b[39m batch_size\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mint\u001b[39m(mixup_pct \u001b[38;5;241m*\u001b[39m num_epochs):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch, num_epochs), disable=not accelerator.is_main_process)\n",
    "mse = nn.MSELoss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "test_image=None\n",
    "num_test_eval=batch_size # should instead be average same-image 300 to mimic MindEye2 retrieval evaluation\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    mindeye.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    loss_clip_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    annot_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    for s, train_dl in enumerate(train_dls):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            for iter, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(train_dl):\n",
    "                image0 = images[behav0[:,0,0].cpu().long()].float()\n",
    "                image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "\n",
    "                # if images are not fully preloaded, then can do this inefficient but more memory friendly approach\n",
    "                # for ib,b in enumerate(behav0[:,0,0].cpu().long()):\n",
    "                #     if ib==0:\n",
    "                #         image0 = torch.Tensor(images[[b]])\n",
    "                #     else:\n",
    "                #         image0 = torch.vstack((image0, torch.Tensor(images[[b]])))\n",
    "                # image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "                \n",
    "                # get the corresponding raw voxel time series\n",
    "                for ib,b in enumerate(behav0[:,0,5].cpu().long().numpy()):\n",
    "                    tr = (nsddata_raw_stimuli[nsddata_raw_stimuli['global_trial'].isin([b.item()])]['global_TR_onsets'].values + TR_delay).astype(np.int32).item()\n",
    "                    if ib==0:\n",
    "                        voxels_raw = mindeye_funcs[tr-2:tr+2][None][None]\n",
    "                    else:\n",
    "                        voxels_raw = np.vstack((voxels_raw, mindeye_funcs[tr-2:tr+2][None][None]))\n",
    "                voxels_raw = torch.Tensor(voxels_raw).to(device)\n",
    "                \n",
    "                ## Process it through pretrained MAE ##\n",
    "                encoder_out = model(voxels_raw, encoder_mask=tube_mask)\n",
    "                voxel0 = encoder_out.flatten(1).unsqueeze(1).cpu()\n",
    "                \n",
    "                assert len(voxel0) == batch_size\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                    perm_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = perm\n",
    "                    betas_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = betas\n",
    "                    select_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = select\n",
    "\n",
    "                voxel_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = voxel0\n",
    "\n",
    "                if iter >= num_iterations_per_epoch:\n",
    "                    break\n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i in range(num_iterations_per_epoch):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss=0.\n",
    "\n",
    "            voxel_list = [voxel_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "            image = image_iters[train_i].detach()\n",
    "            image = image.to(device)\n",
    "\n",
    "            clip_target = clip_img_embedder(image)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                perm_list = [perm_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                perm = torch.cat(perm_list, dim=0)\n",
    "                betas_list = [betas_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                betas = torch.cat(betas_list, dim=0)\n",
    "                select_list = [select_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                select = torch.cat(select_list, dim=0)\n",
    "\n",
    "            voxel_ridge_list = [mindeye.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "            voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "            backbone, clip_voxels = mindeye.backbone(voxel_ridge)\n",
    "\n",
    "            clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "            clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):                \n",
    "                loss_clip = utils.mixco_nce(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006,\n",
    "                    perm=perm, betas=betas, select=select)\n",
    "            else:\n",
    "                epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=epoch_temp)\n",
    "\n",
    "            loss_clip_total += loss_clip.item()\n",
    "            loss += loss_clip\n",
    "\n",
    "            # forward and backward top 1 accuracy        \n",
    "            labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "            fwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "            bwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    mindeye.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):  \n",
    "                loss=0.     \n",
    "\n",
    "                coco_idx = behav[:,0,0].cpu().long()\n",
    "                _,test_indices = np.unique(coco_idx, return_index=True)\n",
    "                test_indices = np.random.permutation(test_indices)[:num_test_eval]\n",
    "                image = images[coco_idx[test_indices]].float().to(device)\n",
    "                \n",
    "                # get the corresponding raw voxel time series\n",
    "                for ib,b in enumerate(behav[test_indices,0,5].cpu().long().numpy()):\n",
    "                    tr = (nsddata_raw_stimuli[nsddata_raw_stimuli['global_trial'].isin([b.item()])]['global_TR_onsets'].values + TR_delay).astype(np.int32).item()\n",
    "                    if ib==0:\n",
    "                        voxels_raw = mindeye_funcs[tr-2:tr+2][None][None]\n",
    "                    else:\n",
    "                        voxels_raw = np.vstack((voxels_raw, mindeye_funcs[tr-2:tr+2][None][None]))\n",
    "                voxels_raw = torch.Tensor(voxels_raw).to(device)\n",
    "                \n",
    "                ## Process it through pretrained MAE ##\n",
    "                encoder_out = model(voxels_raw, encoder_mask=tube_mask)\n",
    "                voxel = encoder_out.flatten(1).unsqueeze(1)\n",
    "\n",
    "                assert len(image) == num_test_eval\n",
    "\n",
    "                clip_target = clip_img_embedder(image.float())\n",
    "\n",
    "                voxel_ridge = mindeye.ridge(voxel,0) # 0th index of subj_list\n",
    "                backbone, clip_voxels = mindeye.backbone(voxel_ridge)\n",
    "\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006)\n",
    "\n",
    "                test_loss_clip_total += loss_clip.item()\n",
    "                loss += loss_clip\n",
    "\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                test_fwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                test_bwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt()\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06050bc-86a2-4134-a661-ead4afe3e1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training losses\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(test_losses)\n",
    "plt.title(\"Test losses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181634e-216b-4aaf-9fec-251d0342ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the corresponding raw voxel time series\n",
    "# ordered_vox_idx = np.argsort(behav0[:,0,5].cpu().long().numpy())\n",
    "# tr = (nsddata_raw_stimuli[nsddata_raw_stimuli['global_trial'].isin(behav0[:,0,5].cpu().long().numpy())]['global_TR_onsets'].values + TR_delay).astype(np.int32)\n",
    "# if len(tr) < batch_size:\n",
    "#     continue\n",
    "# if len(np.sort(np.hstack((tr-2, tr-1, tr, tr+1))))!=len(np.unique(np.sort(np.hstack((tr-2, tr-1, tr, tr+1))))):\n",
    "#     continue\n",
    "# voxels_raw = np.zeros((batch_size, 1, num_frames, img_size[0], img_size[1], img_size[2]))\n",
    "# for r in range(num_frames):\n",
    "#     voxels_raw0 = mindeye_funcs[tr-2+r]\n",
    "#     voxels_raw0 = voxels_raw0[ordered_vox_idx]\n",
    "#     voxels_raw[:,:,r] = voxels_raw0[:,None]\n",
    "# voxels_raw = torch.Tensor(voxels_raw).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
