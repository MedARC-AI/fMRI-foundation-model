{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CUDA devices: 1\n",
      "LOCAL RANK=0\n",
      "NUM GPUS=1\n",
      "NODE=0\n",
      "GLOBAL RANK=0\n",
      "WORLD_SIZE=1\n",
      "PID of this process = 3914273\n",
      "device = cuda distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "import models\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "### Multi-GPU config ###\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"Number of available CUDA devices: {device_count}\")\n",
    "\n",
    "local_rank = os.getenv('LOCAL_RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(f\"LOCAL RANK={local_rank}\")\n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(f\"NUM GPUS={num_devices}\")\n",
    "distributed = True if num_devices>1 else False\n",
    "if distributed: assert device_count==num_devices\n",
    "\n",
    "node = os.getenv('SLURM_NODEID')\n",
    "if node is None:\n",
    "    node = 0\n",
    "else:\n",
    "    node = int(node)\n",
    "print(f\"NODE={node}\")\n",
    "\n",
    "global_rank = os.getenv('RANK')\n",
    "if global_rank is None:\n",
    "    global_rank = 0\n",
    "else:\n",
    "    global_rank = int(global_rank)\n",
    "print(f\"GLOBAL RANK={global_rank}\")\n",
    "\n",
    "world_size = os.getenv('WORLD_SIZE')\n",
    "if world_size is None: \n",
    "    world_size = 1\n",
    "else:\n",
    "    world_size = int(world_size)\n",
    "print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "# batch_size = global_batch_size // num_devices\n",
    "global_batch_size = batch_size * world_size\n",
    "\n",
    "# FSDP Setup\n",
    "if distributed:\n",
    "    import torch.distributed as dist\n",
    "    import torch.multiprocessing as mp\n",
    "    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "    from torch.distributed.fsdp.api import BackwardPrefetch, CPUOffload, ShardingStrategy\n",
    "    import functools\n",
    "    from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy, transformer_auto_wrap_policy\n",
    "    print(\"starting init_process_group...\")\n",
    "    dist.init_process_group(\"nccl\", rank=global_rank, world_size=world_size)\n",
    "    print(f\"setting device to cuda:{local_rank}\")\n",
    "    try:\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.device('cuda',local_rank)\n",
    "        print(f\"\\nSuccessfully set cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "    except Exception as error:        \n",
    "        print(f\"\\nFAILED TO SET DEVICE cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "        print(\"An exception occurred:\", error)\n",
    "        \n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "print(\"device =\", device, \"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08179db-9c6a-4bc6-a245-79fae6884ca2",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b1c3fe-28ab-40b7-8906-c6a9c8070d00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'apr17_encoder32_decoder32_randomtubemask_fixdecodemask_learnableposemb_lr4e-6', 'use_cls_token': False, 'use_contrastive_loss': False, 'constrastive_loss_weight': 1.0, 'batch_size': 8, 'num_workers': 10, 'num_epochs': 1000, 'seed': 42, 'max_lr': 4e-06, 'num_samples_per_epoch': 1024, 'cache_dir': 'cache', 'ckpt_saving': True, 'ckpt_interval': 50, 'resume_from_ckpt': True, 'wandb_log': True, 'wandb_group_name': 'mamba', 'tube_start_masking_ratio': 0.75, 'tube_end_masking_ratio': 0.75, 'decoder_mask_ratio': 0.75, 'encoder_depth': 32, 'decoder_depth': 32, 'patch_size': 8, 'frame_patch_size': 1, 'encoder_outdim': 6, 'decoder_outdim': 512, 'use_rope_emb': False, 'masking_strategy': 'MNI', 'img_size': [88, 104, 72], 'is_random_tube_mask_ratio': True, 'num_frames': 4, 'is_s3': False, 'train_urls': ['/scratch/gpfs/KNORMAN/nsdfoundation/wds/{000005..000099}.tar'], 'test_urls': ['/scratch/gpfs/KNORMAN/nsdfoundation/wds/{000000..000004}.tar'], 'test_num_iterations_per_epoch': 10, 'debug': False}\n",
      "outdir /scratch/gpfs/qanguyen/mamba_fmri/ckpts/apr17_encoder32_decoder32_randomtubemask_fixdecodemask_learnableposemb_lr4e-6\n",
      "num_patches 5148\n",
      "num_encoder_patches 1287\n"
     ]
    }
   ],
   "source": [
    "print(config)\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "num_patches = int(\n",
    "    (img_size[0] / patch_size)\n",
    "    * (img_size[1] / patch_size)\n",
    "    * (img_size[2] / patch_size)\n",
    "    * num_frames\n",
    ")\n",
    "num_patches_per_timepoint = num_patches // num_frames\n",
    "num_encoder_patches = int(num_patches_per_timepoint * (1 - tube_start_masking_ratio) * num_frames)\n",
    "# num_decoder_patches = int(num_patches_per_timepoint * (1 - decoder_mask_ratio) * num_frames)\n",
    "print(\"num_patches\", num_patches)\n",
    "print(\"num_encoder_patches\", num_encoder_patches)\n",
    "# print(\"num_decoder_patches\", num_decoder_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8419d-988f-42c6-acb6-b258e0694eee",
   "metadata": {},
   "source": [
    "# Prep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e37b6b9-5b91-4c4a-af85-ac2af69704e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use checkpoint: False\n",
      "Checkpoint number: 0\n",
      "512 rms_norm True residual_in_fp32 True fused_add_norm True bimamba True ssm_cfg None\n",
      "param counts:\n",
      "117,486,598 total\n",
      "117,486,598 trainable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# vit_size = {\n",
    "#     \"encoder\": encoder_model,\n",
    "#     \"decoder\": decoder_model\n",
    "# }\n",
    "    \n",
    "# vit_model = models.get_vit(\n",
    "#     size=vit_size,\n",
    "#     image_size=img_size,  # depth, height, width\n",
    "#     image_patch_size=(patch_size,patch_size,patch_size),  # depth, height, width patch size\n",
    "#     frames=num_frames,\n",
    "#     frame_patch_size=frame_patch_size,\n",
    "#     channels=1,\n",
    "#     use_rope_emb=use_rope_emb,\n",
    "#     use_cls_token=use_cls_token,\n",
    "# )\n",
    "model = models.get_mamba(\"middle\",\n",
    "                        channels=1,\n",
    "                        img_size=img_size,  # depth, height, width\n",
    "                        patch_size=(patch_size,patch_size,patch_size),\n",
    "                        num_frames=num_frames,\n",
    "                        frame_patch_size=frame_patch_size,\n",
    "                        device=device,\n",
    "                        embed_dim=512, \n",
    "                        encoder_outdim=encoder_outdim,\n",
    "                        decoder_outdim=decoder_outdim,\n",
    "                        encoder_depth=encoder_depth, \n",
    "                        decoder_depth=decoder_depth,\n",
    "                        )\n",
    "\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works without error\n",
    "model = model.to(device)\n",
    "# encoder_mask = torch.zeros(num_patches).to(device).to(torch.bool)\n",
    "# encoder_mask[:num_encoder_patches] = True\n",
    "# decoder_mask = torch.zeros(num_patches).to(device).to(torch.bool)\n",
    "# decoder_mask[-num_decoder_patches:] = True\n",
    "# with torch.no_grad():\n",
    "#     print(\"\\nencoder\")\n",
    "#     encoder_out = model(\n",
    "#                 torch.randn(6, 1, 4, 64, 64, 48).to(device),\n",
    "#                 mask=encoder_mask)\n",
    "#     print(\"\\ndecoder\")\n",
    "#     decoder_out = model(\n",
    "#                 encoder_out, \n",
    "#                 encoder_mask=encoder_mask, \n",
    "#                 decoder_mask=decoder_mask)\n",
    "#     if use_cls_token:\n",
    "#         enc_cls_token = encoder_out[:, :1, :]\n",
    "#         encoder_patches = encoder_out[:, 1:, :]\n",
    "#         dec_cls_token = decoder_out[:, :1, :]\n",
    "#         decoder_patches = decoder_out[:, 1:, :]\n",
    "#         print(\"\\nenc_cls_token\", enc_cls_token.shape)\n",
    "#         print(\"encoder_patches\", encoder_patches.shape)\n",
    "#         print(\"dec_cls_token\", dec_cls_token.shape)\n",
    "#         print(\"decoder_patches\", decoder_patches.shape)\n",
    "\n",
    "aug_transform = utils.DataPrepper(\n",
    "    masking_strategy=\"conservative\",\n",
    "    patch_depth=patch_size,\n",
    "    patch_height=patch_size,\n",
    "    patch_width=patch_size,\n",
    "    frame_patch_size=frame_patch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c055614",
   "metadata": {},
   "source": [
    "# Create dataset and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75dd0398-0eb8-464e-8bb8-13e7ce1a8480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/gpfs/KNORMAN/nsdfoundation/wds/{000005..000099}.tar']\n"
     ]
    }
   ],
   "source": [
    "def log_and_continue(exn):\n",
    "    \"\"\"Call in an exception handler to ignore any exception, issue a warning, and continue.\"\"\"\n",
    "    print(f'Handling webdataset error ({repr(exn)}). Ignoring.')\n",
    "    return True\n",
    "\n",
    "def filter_corrupted_images(sample):\n",
    "    \"\"\"If all the required files are not present don't use them.\"\"\"\n",
    "    correct_data = (\"func.npy\" in sample)\n",
    "    return correct_data\n",
    "\n",
    "### ================      Train Dataset and DataLoader    ====================\n",
    "from braceexpand import braceexpand\n",
    "print(train_urls)\n",
    "\n",
    "if is_s3:\n",
    "    expanded_urls = [f\"pipe:aws s3 cp {url} -\" for pattern in train_urls for url in braceexpand(pattern)]\n",
    "else:\n",
    "    expanded_urls = [str(url) for pattern in train_urls for url in braceexpand(pattern)]\n",
    "train_data = (\n",
    "    wds.WebDataset(expanded_urls, resampled=True, nodesplitter=wds.split_by_node, handler=log_and_continue)\n",
    "    .shuffle(100, initial=100, rng=random.Random(seed))\n",
    "    .select(filter_corrupted_images)\n",
    "    .decode(\"torch\")\n",
    "    .rename(key=\"__key__\", func=\"func.npy\")\n",
    "    .to_tuple(*(\"key\",\"func\"))\n",
    ")\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "#    .map_dict(func=utils.numpy_decoder)\n",
    "# train_dl = wds.WebLoader(\n",
    "#     train_data.batched(batch_size), \n",
    "#     pin_memory=True,\n",
    "#     shuffle=False,\n",
    "#     batch_size=None,\n",
    "#     num_workers=num_workers, \n",
    "#     persistent_workers=num_workers>0,\n",
    "# ).with_epoch(num_samples_per_epoch//batch_size)\n",
    "\n",
    "if is_s3:\n",
    "    expanded_urls = [f\"pipe:aws s3 cp {url} -\" for pattern in test_urls for url in braceexpand(pattern)]\n",
    "else:\n",
    "    expanded_urls = [str(url) for pattern in test_urls for url in braceexpand(pattern)]\n",
    "\n",
    "test_data = (\n",
    "    wds.WebDataset(expanded_urls, resampled=True, nodesplitter=wds.split_by_node, handler=log_and_continue)\n",
    "    .shuffle(100, initial=100, rng=random.Random(seed))\n",
    "    .select(filter_corrupted_images)\n",
    "    .decode(\"torch\")\n",
    "    .rename(key=\"__key__\", func=\"func.npy\")\n",
    "    .to_tuple(*(\"key\",\"func\"))\n",
    ")\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d05a32-12eb-494a-82df-dce4c7e9924c",
   "metadata": {},
   "source": [
    "### Check data loaders work and calculate number of iterations per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "480e7c0b-f58e-4c35-80fe-1284ee0e3966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yielding 2 batches\n",
      "iter 0\n",
      "iter 1\n",
      "Done!\n",
      "input_func torch.Size([8, 32, 88, 104, 72])\n",
      "Execution time: 4.834169149398804 seconds\n"
     ]
    }
   ],
   "source": [
    "if not distributed:\n",
    "    start_time = time.time() \n",
    "    num_it = 2\n",
    "    print(f\"Yielding {num_it} batches\")\n",
    "    \n",
    "    for i, batch in enumerate(train_dl):\n",
    "        print(\"iter\",i)\n",
    "        key, input_func = batch\n",
    "        if i >= (num_it-1):\n",
    "            break\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    print(\"input_func\", input_func.shape)\n",
    "\n",
    "    end_time = time.time()  \n",
    "    execution_time = end_time - start_time  \n",
    "    print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c6fba-5494-4964-b03d-e01c3afe47db",
   "metadata": {},
   "source": [
    "# Playing with the data, visualization of patching + masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94cd7913-707a-44ba-b031-afc883fb357c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if masking_strategy==\"MNI\":\n",
    "    # MNI_brain = nib.load(f\"/weka/home-alexnguyen/mamba_fmri/fMRI-MAE/cache/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "    MNI_brain = nib.load(f\"/scratch/gpfs/qanguyen/mamba_fmri/fMRI-MAE/cache/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "    brain_pos_voxels = MNI_brain[6:94,8:112,10:82]\n",
    "    brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "    brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e3e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if utils.is_interactive():\n",
    "#     # extract func volumes and their reference mean and standard deviation volumes\n",
    "#     if masking_strategy==\"MNI\":\n",
    "#         func, _ = aug_transform(input_func)\n",
    "#     else:\n",
    "#         func, brain_pos_voxels = aug_transform(input_func)\n",
    "#         brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "#         brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "#     func = func.reshape(-1, num_frames, func.shape[-3], func.shape[-2], func.shape[-1])\n",
    "#     func = func.unsqueeze(1)  # add empty first dimension to serve as 1d channel dimension\n",
    "\n",
    "#     # patchify func samples\n",
    "#     print(\"func\", func.shape)\n",
    "#     patches = model.patchify(func)\n",
    "#     print(\"patches\", patches.shape)\n",
    "#     patches_vit = rearrange(patches, \"b ... d -> b (...) d\")\n",
    "#     print(\"patches_vit\", patches_vit.shape)\n",
    "#     print(\"num patches in one timepoint\", patches_vit.shape[1] // num_frames)\n",
    "\n",
    "#     # start by masking everything (aka include nothing)\n",
    "#     tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "#     # approximate brain positive patches for the whole batch\n",
    "#     batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "#     mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "#     mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "#     print(\"Percentage of brain positive patches\", len(mask_idx_candidates) / len(batch_positive_approx))\n",
    "#     tube_idx = mask_idx_candidates[: int(num_patches / num_frames * (1 - tube_start_masking_ratio))]\n",
    "#     print(\"num tube patches =\", len(tube_idx))\n",
    "#     tube_mask[tube_idx] = True  # Trues mean to include the patch, False means to remove the patch\n",
    "#     tube_mask = tube_mask.tile(num_frames)  # repeat masking for the other timepoints\n",
    "#     print(\"tube mask percent\", tube_mask.sum().item() / len(tube_mask))\n",
    "\n",
    "#     # create decoder mask similar to tube mask, but ensure no overlap\n",
    "#     decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)  # start by masking everything (aka include nothing)\n",
    "#     remaining_mask_idx = mask_idx_candidates[int(num_patches / num_frames * (1 - tube_start_masking_ratio)) :]  # brain positive tokens not selected for the encoder tokens\n",
    "#     decoder_mask_idx = remaining_mask_idx[:int(num_patches / num_frames * (1 - decoder_mask_ratio))]\n",
    "#     print(\"num decoder patches =\", len(decoder_mask_idx))\n",
    "#     decoder_mask[decoder_mask_idx] = True\n",
    "#     decoder_mask = decoder_mask.tile(num_frames)  # repeat masking for the other timepoints\n",
    "#     print(\"decoder_mask percent\", decoder_mask.sum().item() / len(decoder_mask))\n",
    "\n",
    "#     # apply masks to patches_vit\n",
    "#     tube_patches_vit = copy.deepcopy(patches_vit.detach())\n",
    "#     decoder_patches_vit = copy.deepcopy(patches_vit.detach())\n",
    "#     # tube_patches_vit[:, tube_mask] = 1\n",
    "#     # decoder_patches_vit[:, decoder_mask] = 1\n",
    "#     tube_patches_vit[:, ~tube_mask] = 0\n",
    "#     decoder_patches_vit[:, ~decoder_mask] = 0\n",
    "\n",
    "#     # undo patchification so we can visualize\n",
    "#     tube_unpatches = rearrange(\n",
    "#         tube_patches_vit,\n",
    "#         \"b (f d h w) c -> b f d h w c\",\n",
    "#         d=img_size[0]//patch_size,\n",
    "#         h=img_size[1]//patch_size,\n",
    "#         w=img_size[2]//patch_size,\n",
    "#     )\n",
    "#     decoder_unpatches = rearrange(\n",
    "#         decoder_patches_vit,\n",
    "#         \"b (f d h w) c -> b f d h w c\",\n",
    "#         d=img_size[0]//patch_size,\n",
    "#         h=img_size[1]//patch_size,\n",
    "#         w=img_size[2]//patch_size,\n",
    "#     )\n",
    "#     print(\"tube_unpatches\", tube_unpatches.shape)\n",
    "#     print(\"decoder_unpatches\", decoder_unpatches.shape)\n",
    "    \n",
    "#     encoder_func = rearrange(\n",
    "#         tube_unpatches,\n",
    "#         \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "#         b=len(func),\n",
    "#         f=num_frames,\n",
    "#         d=img_size[0] // patch_size,\n",
    "#         h=img_size[1] // patch_size,\n",
    "#         w=img_size[2] // patch_size,\n",
    "#         pd=patch_size,\n",
    "#         ph=patch_size,\n",
    "#         pw=patch_size,\n",
    "#         pf=frame_patch_size,\n",
    "#     )\n",
    "#     decoder_func = rearrange(\n",
    "#         decoder_unpatches,\n",
    "#         \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "#         b=len(func),\n",
    "#         f=num_frames,\n",
    "#         d=img_size[0] // patch_size,\n",
    "#         h=img_size[1] // patch_size,\n",
    "#         w=img_size[2] // patch_size,\n",
    "#         pd=patch_size,\n",
    "#         ph=patch_size,\n",
    "#         pw=patch_size,\n",
    "#         pf=frame_patch_size,\n",
    "#     )\n",
    "#     print(\"encoder_func\", encoder_func.shape)\n",
    "#     print(\"decoder_func\", decoder_func.shape)\n",
    "    \n",
    "#     brain_pos_vit = copy.deepcopy(patches_vit.detach())\n",
    "#     brain_pos_vit[:,batch_positive_approx.repeat(num_frames)] = 1\n",
    "#     brain_pos_vit[:,~batch_positive_approx.repeat(num_frames)] = 0\n",
    "#     brain_pos_unpatches = rearrange(\n",
    "#         brain_pos_vit,\n",
    "#         \"b (f d h w) c -> b f d h w c\",\n",
    "#         d=img_size[0]//patch_size,\n",
    "#         h=img_size[1]//patch_size,\n",
    "#         w=img_size[2]//patch_size,\n",
    "#     )\n",
    "#     brain_pos_func = rearrange(\n",
    "#         brain_pos_unpatches,\n",
    "#         \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "#         b=len(func),\n",
    "#         f=num_frames,\n",
    "#         d=img_size[0] // patch_size,\n",
    "#         h=img_size[1] // patch_size,\n",
    "#         w=img_size[2] // patch_size,\n",
    "#         pd=patch_size,\n",
    "#         ph=patch_size,\n",
    "#         pw=patch_size,\n",
    "#         pf=frame_patch_size,\n",
    "#     )\n",
    "\n",
    "#     # Visualize\n",
    "#     idx = 0\n",
    "#     print(\"original func\")\n",
    "#     display(transforms.ToPILImage()(utils.reshape_to_2d(func[idx].clamp(0,1))))\n",
    "    \n",
    "#     print(\"\\nbrain-positive patches\")\n",
    "#     display(transforms.ToPILImage()(utils.reshape_to_2d(brain_pos_func[idx].clamp(0,1))))\n",
    "\n",
    "#     print(\"\\nencoder func\")\n",
    "#     display(transforms.ToPILImage()(utils.reshape_to_2d(encoder_func[idx].clamp(0,1))))\n",
    "\n",
    "#     print(\"\\ndecoder func\")\n",
    "#     display(transforms.ToPILImage()(utils.reshape_to_2d(decoder_func[idx].clamp(0,1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604b380-0126-49b2-ab4e-59b1e463d6ad",
   "metadata": {},
   "source": [
    "# FSDP / optimizer / saving functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f057be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:    \n",
    "    # my_auto_wrap_policy = functools.partial(\n",
    "    #     size_based_auto_wrap_policy, min_num_params=200000\n",
    "    # )\n",
    "    from mamba_ssm.modules.mamba_simple import Block\n",
    "    my_auto_wrap_policy = functools.partial(\n",
    "        transformer_auto_wrap_policy, \n",
    "        transformer_layer_cls={\n",
    "            Block, # <--- Your Transformer layer class\n",
    "        },\n",
    "    )\n",
    "    print(f\"\\nPrepping FSDP on {global_rank} {node}...\\n\")\n",
    "    model = FSDP(\n",
    "        model,\n",
    "        sharding_strategy=ShardingStrategy.HYBRID_SHARD,\n",
    "        auto_wrap_policy=my_auto_wrap_policy,\n",
    "        use_orig_params=False,\n",
    "        cpu_offload=None, #CPUOffload(offload_params=True)\n",
    "        sync_module_states=True,\n",
    "        limit_all_gathers=True, # See https://github.com/pytorch/pytorch/issues/91165\n",
    "        device_id=device,\n",
    "    )\n",
    "    print(f\"\\nSuccessfully loaded FSDP model to device on global_rank {global_rank}\\n\")\n",
    "    dist.barrier()\n",
    "    print(f\"\\nSuccessfully loaded FSDP model to device on global_rank {global_rank}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4da73c08-ca61-48ef-9e63-b70db6f07a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 128000\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "117,486,598 total\n",
      "117,486,598 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "if distributed:\n",
    "    max_lr = max_lr * global_batch_size\n",
    "    print(f\"multiply lr {max_lr} by global batch size: max_lr={max_lr}\")\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "num_iterations_per_epoch = num_samples_per_epoch // global_batch_size\n",
    "\n",
    "total_steps = num_epochs * num_iterations_per_epoch * num_devices\n",
    "print(\"total_steps\", total_steps)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    ")\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f51f674d-7cd4-49d1-bd8d-c697d9614f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_ckpt_path = outdir+f'/last.pth'\n",
    "\n",
    "def save_ckpt(model,tag=\"last\"):\n",
    "    if distributed: dist.barrier()\n",
    "    model_states = model.state_dict()\n",
    "    if global_rank == 0:\n",
    "        os.makedirs(outdir,exist_ok=True)\n",
    "        ckpt_path = outdir+f'/{tag}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_states,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "        }, ckpt_path)\n",
    "        print(f\"\\n---saved {ckpt_path}!---\\n\")\n",
    "\n",
    "def resume_ckpt(model, optimizer, lr_scheduler, device, ckpt_path=default_ckpt_path):\n",
    "    if global_rank == 0:\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        epoch = 0\n",
    "    if distributed: dist.barrier()\n",
    "    torch.cuda.empty_cache()\n",
    "    return model, optimizer, lr_scheduler, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b4262-435b-4872-ab4a-424cb9dfd37a",
   "metadata": {},
   "source": [
    "# Start wandb (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56431733-4fb5-4072-840e-22536608f9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if utils.is_interactive():\n",
    "    ckpt_saving = False\n",
    "    wandb_log = False\n",
    "if global_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found' \n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "    #   \"encoder_model\": encoder_model,\n",
    "    #   \"decoder_model\": decoder_model,\n",
    "      \"tube_start_masking_ratio\": tube_start_masking_ratio,\n",
    "      \"tube_end_masking_ratio\": tube_end_masking_ratio,\n",
    "      \"decoder_mask_ratio\": decoder_mask_ratio,\n",
    "      \"num_frames\": num_frames,\n",
    "      \"patch_size\": patch_size,\n",
    "      \"frame_patch_size\": frame_patch_size,\n",
    "      \"use_contrastive_loss\": use_contrastive_loss,\n",
    "      \"use_cls_token\": use_cls_token,\n",
    "      \"constrastive_loss_weight\": constrastive_loss_weight,\n",
    "      \"num_params\": num_params,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_urls\": train_urls,\n",
    "      \"test_urls\": test_urls,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        # id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "        group=wandb_group_name\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a5055-8afd-468a-93bf-32f94bd1d042",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e394dd-2745-41fa-a5aa-54b7b1373a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "lrs, train_losses, recon_losses, contrastive_losses, test_losses = [], [], [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b9a5605-fd81-41d4-964e-1752ed6e1289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug=True\n",
    "\n",
    "if (resume_from_ckpt==True):# and (debug!=True):\n",
    "    if os.path.exists(default_ckpt_path):\n",
    "        print(f\"Resuming from {default_ckpt_path}...\")\n",
    "        model, optimizer, lr_scheduler, epoch = resume_ckpt(model, optimizer, lr_scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12e258c4-8477-48ed-81af-d415cee246f3",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e7c582ed424a168f2c011719555bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patched_emb torch.Size([1, 1287, 512]) tensor([[-0.0054,  0.0035,  0.0132,  ...,  0.0053,  0.0061, -0.0405],\n",
      "        [ 0.0187, -0.0197, -0.0276,  ...,  0.0096, -0.0321,  0.0140],\n",
      "        [-0.0166,  0.0359, -0.0093,  ...,  0.0124,  0.0023,  0.0261],\n",
      "        ...,\n",
      "        [-0.0110,  0.0134,  0.0288,  ...,  0.0074,  0.0040,  0.0210],\n",
      "        [ 0.0089,  0.0277, -0.0107,  ..., -0.0220,  0.0190, -0.0158],\n",
      "        [ 0.0232, -0.0165, -0.0057,  ..., -0.0431,  0.0079, -0.0061]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "encoder_out torch.Size([8, 2556, 6])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VisionMamba' object has no attribute 'encoder_to_decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoder_out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# decode both the encoder_out patches and masked decoder patches\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m decoder_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtube_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# decoder_out = model(func, encoder_mask=tube_mask, decoder_mask=decoder_mask)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# subset only the reconstructed decoder patches\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# print (\"decoder_out\", decoder_out.shape, \"num_decoder_patches\", num_decoder_patches)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# the decoder mask gets concatenated to the end\u001b[39;00m\n\u001b[1;32m     65\u001b[0m output \u001b[38;5;241m=\u001b[39m decoder_out[:, \u001b[38;5;241m-\u001b[39mdecoder_mask\u001b[38;5;241m.\u001b[39msum():]\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/scratch/gpfs/qanguyen/mamba_fmri/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/gpfs/qanguyen/mamba_fmri/fMRI-MAE/models/vision_mamba.py:515\u001b[0m, in \u001b[0;36mVisionMamba.forward\u001b[0;34m(self, x, inference_params, encoder_mask, decoder_mask, verbose)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, inference_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, encoder_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, decoder_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 515\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# x = self.head(self.head_drop(x))\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/scratch/gpfs/qanguyen/mamba_fmri/fMRI-MAE/models/vision_mamba.py:468\u001b[0m, in \u001b[0;36mVisionMamba.forward_features\u001b[0;34m(self, x, inference_params, encoder_mask, decoder_mask, verbose)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# DECODER\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder input\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 468\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_to_decoder\u001b[49m(x)\n\u001b[1;32m    469\u001b[0m     B, _, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    470\u001b[0m     pos_emd_encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposemb_sincos_4d[encoder_mask]\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/scratch/gpfs/qanguyen/mamba_fmri/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VisionMamba' object has no attribute 'encoder_to_decoder'"
     ]
    }
   ],
   "source": [
    "if distributed: dist.barrier()\n",
    "mse = nn.MSELoss()\n",
    "if use_contrastive_loss:\n",
    "    contrastive_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs)\n",
    "model.train()\n",
    "progress_bar = tqdm(range(epoch, num_epochs), disable=global_rank!=0, desc=\"Overall\")\n",
    "for epoch in progress_bar:\n",
    "    # get the masking ratio for the current epoch\n",
    "    tube_mask_ratio = utils.get_masking_ratio(\n",
    "        current_epoch=epoch, \n",
    "        total_epochs=num_epochs, \n",
    "        start_masking_ratio=tube_start_masking_ratio, \n",
    "        end_masking_ratio=tube_end_masking_ratio\n",
    "    )\n",
    "    num_decoder_patches = int(num_patches * tube_mask_ratio)\n",
    "    with torch.cuda.amp.autocast(dtype=data_type):\n",
    "        model.train()\n",
    "        for train_i, batch in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            key, input_func = batch \n",
    "            if masking_strategy==\"MNI\":\n",
    "                func, _ = aug_transform(input_func)\n",
    "            else:\n",
    "                func, brain_pos_voxels = aug_transform(input_func)\n",
    "                brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "                brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "            if is_random_tube_mask_ratio==True: tube_mask_ratio = np.random.uniform(low=0.001,high=0.75) # random tube mask ratio to train with different #s of patches\n",
    "            func = func.reshape(-1, num_frames, func.shape[-3], func.shape[-2], func.shape[-1]).float()\n",
    "            func = func.unsqueeze(1).clamp(0,1).to(device)\n",
    "\n",
    "            # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "            tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "            mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "            mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "            num_masked_voxels = min(int(len(mask_idx_candidates) * (1 - tube_mask_ratio)), len(mask_idx_candidates) - 1)\n",
    "            tube_idx = mask_idx_candidates[:num_masked_voxels]\n",
    "            tube_mask[tube_idx] = True\n",
    "            tube_mask = tube_mask.tile(num_frames)#.to(device)\n",
    "             \n",
    "\n",
    "            # create decoder mask\n",
    "            decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            remaining_mask_idx = mask_idx_candidates[num_masked_voxels:]\n",
    "            decoder_mask_idx = remaining_mask_idx#[:int(num_patches / num_frames * (1 - decoder_mask_ratio))]\n",
    "            decoder_mask[decoder_mask_idx] = True\n",
    "            decoder_mask = decoder_mask.tile(num_frames)#.to(device)\n",
    " \n",
    "            # decoder_mask = ~tube_mask\n",
    "            \n",
    "            # encode the tube patches\n",
    "            encoder_out = model(func, encoder_mask=tube_mask)\n",
    "            if use_cls_token:\n",
    "                enc_cls_token = encoder_out[:,:1,:]\n",
    "            # print (\"tube_mask\", tube_mask.shape)\n",
    "            print (\"encoder_out\", encoder_out.shape)\n",
    "            \n",
    "            # decode both the encoder_out patches and masked decoder patches\n",
    "            decoder_out = model(encoder_out, encoder_mask=tube_mask, decoder_mask=decoder_mask)\n",
    "            # decoder_out = model(func, encoder_mask=tube_mask, decoder_mask=decoder_mask)\n",
    "            # subset only the reconstructed decoder patches\n",
    "            # print (\"decoder_out\", decoder_out.shape, \"num_decoder_patches\", num_decoder_patches)\n",
    "            # the decoder mask gets concatenated to the end\n",
    "            output = decoder_out[:, -decoder_mask.sum():].clamp(0,1)\n",
    "            \n",
    "\n",
    "            # compare to ground truth and calculate loss \n",
    "            target_patches = model.patchify(func)\n",
    "            target_patches_vit = rearrange(target_patches, \"b ... d -> b (...) d\")\n",
    "            target = target_patches_vit[:, decoder_mask]\n",
    "            #print(\"encoder_out\",encoder_out.shape,\"output\",output.shape,\"target\",target.shape)\n",
    "            # print(\"output\")\n",
    "            # #print(output[0,:10,:10])\n",
    "\n",
    "            # #print(target[0,:10,:10])\n",
    "            # plt.imshow(torch.corrcoef(output[0]).detach().cpu(),vmin=0,vmax=1 )\n",
    "            # plt.show()\n",
    "            # plt.hist(torch.corrcoef(output[0]).detach().cpu().flatten())\n",
    "            # plt.show()\n",
    "            # print(\"target\")\n",
    "            # plt.imshow(torch.corrcoef(target[0]).detach().cpu(),vmin=0,vmax=1 )\n",
    "            # plt.show()\n",
    "            # plt.hist(torch.corrcoef(target[0]).detach().cpu().flatten())\n",
    "            # plt.show()\n",
    "            \n",
    "            # print(\"torch.corrcoef(output[0])\",torch.corrcoef(output[0]))\n",
    "            # ## visualize\n",
    "            # decode_vis = torch.zeros_like(target_patches_vit)\n",
    "            # print(\"output\",output.shape, \"-decoder_mask.sum()\",-decoder_mask.sum())\n",
    "            # decode_vis[:, decoder_mask] = output.to(decode_vis.device).to(decode_vis.dtype)\n",
    "            # decoder_unpatches = rearrange(\n",
    "            #     decode_vis,\n",
    "            #     \"b (f d h w) c -> b f d h w c\",\n",
    "            #     d=img_size[0]//patch_size,\n",
    "            #     h=img_size[1]//patch_size,\n",
    "            #     w=img_size[2]//patch_size,\n",
    "            # )\n",
    "            # decoder_func = rearrange(\n",
    "            #     decoder_unpatches,\n",
    "            #     \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "            #     b=batch_size,\n",
    "            #     f=num_frames,\n",
    "            #     d=img_size[0]//patch_size,\n",
    "            #     h=img_size[1]//patch_size,\n",
    "            #     w=img_size[2]//patch_size,\n",
    "            #     pd=patch_size,\n",
    "            #     ph=patch_size,\n",
    "            #     pw=patch_size,\n",
    "            #     pf=frame_patch_size,\n",
    "            # )\n",
    "            # if train_i%2 == 1:\n",
    "            #     orig_image = torch.zeros_like(utils.reshape_to_2d(func[idx]))\n",
    "            # else:\n",
    "            #     orig_image =  (utils.reshape_to_2d(func[idx]))\n",
    "            # recon_image = utils.reshape_to_2d(decoder_func[idx])\n",
    "            # print(\"orig_image\",orig_image.shape, orig_image.min(),orig_image.max(), \"recon_image\",recon_image.shape,recon_image.min(),recon_image.max())\n",
    "            # combined_image = orig_image.clone()\n",
    "            # combined_image[recon_image!=0] = recon_image[recon_image!=0]\n",
    "\n",
    "            # random_start = np.random.randint(recon_image.shape[1]-400)\n",
    "            # orig_image = transforms.ToPILImage()(orig_image[:,random_start:random_start+100])\n",
    "            # recon_image = transforms.ToPILImage()(recon_image[:,random_start:random_start+100])\n",
    "            # combined_image = transforms.ToPILImage()(combined_image[:,random_start:random_start+100])\n",
    "\n",
    "\n",
    "            # plt.imshow( (utils.reshape_to_2d(func[idx])).detach().cpu() [:,random_start:random_start+100] ,vmin=0,vmax=1)\n",
    "            # plt.show()\n",
    "            # plt.imshow(recon_image.detach().cpu()[:,random_start:random_start+100],vmin=0,vmax=1)\n",
    "            # plt.show()\n",
    "            # plt.imshow(combined_image.detach().cpu() [:,random_start:random_start+100] ,vmin=0,vmax=1)\n",
    "            # plt.show() \n",
    "            # print(\"func\",func.shape,\"output\",output.shape,\"target\",target.shape)\n",
    "            loss = mse(output, target)\n",
    "            # print ( \"decoder_mask.sum()\", decoder_mask.sum(), \"loss\", loss.item())\n",
    "            recon_losses.append(loss.item())\n",
    " \n",
    "            # contrastive loss\n",
    "            if use_contrastive_loss:\n",
    "                enc_norm = nn.functional.normalize(encoder_out.flatten(1), dim=-1)\n",
    "                cosine_similarities = enc_norm @ enc_norm.T\n",
    "                \n",
    "                softmax_scores = nn.functional.softmax(cosine_similarities / contrastive_temps[epoch], dim=-1)\n",
    "                contrastive_loss = nn.functional.cross_entropy(softmax_scores, torch.arange(len(cosine_similarities)).to(device))\n",
    "                \n",
    "                loss += constrastive_loss_weight * contrastive_loss\n",
    "                contrastive_losses.append(contrastive_loss.item())\n",
    "\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            train_losses.append(loss.item())\n",
    "            # print(\"loss\",loss.item(), \"tube_mask_ratio\", tube_mask_ratio)\n",
    "            lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "            if (train_i >= (num_iterations_per_epoch-1)) or debug:\n",
    "                print(\"train_i\", train_i, \"local_rank\", local_rank, \"global_rank\", global_rank)\n",
    "                break\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for test_i, batch in enumerate(test_dl):\n",
    "                key, input_func = batch \n",
    "\n",
    "                if masking_strategy==\"MNI\":\n",
    "                    func, _ = aug_transform(input_func)\n",
    "                else:\n",
    "                    func, brain_pos_voxels = aug_transform(input_func)\n",
    "                    brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "                    brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "                    \n",
    "                if is_random_tube_mask_ratio==True: tube_mask_ratio = np.random.uniform(low=0.001,high=0.75) # random tube mask ratio to train with different #s of patches\n",
    "                func = func.reshape(-1, num_frames, func.shape[-3], func.shape[-2], func.shape[-1]).float()\n",
    "                func = func.unsqueeze(1).clamp(0,1).to(device)\n",
    "    # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "                tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "                batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "                mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "                mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "                num_masked_voxels = min(int(len(mask_idx_candidates) * (1 - tube_mask_ratio)), len(mask_idx_candidates) - 1)\n",
    "                tube_idx = mask_idx_candidates[:num_masked_voxels]\n",
    "                tube_mask[tube_idx] = True\n",
    "                tube_mask = tube_mask.tile(num_frames)#.to(device)\n",
    "                \n",
    "\n",
    "                # create decoder mask \n",
    "                decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "                remaining_mask_idx = mask_idx_candidates[num_masked_voxels:]\n",
    "                decoder_mask_idx = remaining_mask_idx#[:int(num_patches / num_frames * (1 - decoder_mask_ratio))]\n",
    "                decoder_mask[decoder_mask_idx] = True\n",
    "                decoder_mask = decoder_mask.tile(num_frames)#.to(device)\n",
    "\n",
    "                # decoder_mask = ~tube_mask\n",
    "                \n",
    "                # encode the tube patches\n",
    "                encoder_out = model(func, encoder_mask=tube_mask)\n",
    "                if use_cls_token:\n",
    "                    enc_cls_token = encoder_out[:,:1,:]\n",
    "                # print (\"test encoder_out\", encoder_out.shape, \"test decoder_mask.sum()\", decoder_mask.sum())\n",
    "                # print (\"tube_mask\", tube_mask.shape)\n",
    "                # print (\"encoder_out\", encoder_out.shape)\n",
    "                # decode both the encoder_out patches and masked decoder patches\n",
    "                decoder_out = model(encoder_out, encoder_mask=tube_mask, decoder_mask=decoder_mask)\n",
    "                # decoder_out = model(func, encoder_mask=tube_mask, decoder_mask=decoder_mask)\n",
    "                # subset only the reconstructed decoder patches\n",
    "                # print (\"decoder_out\", decoder_out.shape, \"num_decoder_patches\", num_decoder_patches)\n",
    "                # the decoder mask gets concatenated to the end\n",
    "                output = decoder_out[:, -decoder_mask.sum():].clamp(0,1)\n",
    "    \n",
    "\n",
    "                # compare to ground truth and calculate loss\n",
    "                target_patches = model.patchify(func)\n",
    "                target_patches_vit = rearrange(target_patches, \"b ... d -> b (...) d\")\n",
    "                target = target_patches_vit[:, decoder_mask]\n",
    "                loss = mse(output, target)\n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "                if test_i >= (test_num_iterations_per_epoch-1):\n",
    "                    break\n",
    " \n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(train_losses[-(train_i + 1) :]),\n",
    "            \"train/recon_losses\": np.mean(recon_losses[-(train_i + 1) :]),\n",
    "            \"train/contrastive_losses\": np.mean(contrastive_losses[-(train_i + 1) :]),\n",
    "            \"train/num_steps\": len(recon_losses),\n",
    "            \"test/loss\": np.mean(test_losses[-(test_i + 1) :]),\n",
    "            \"test/num_steps\": len(test_losses),\n",
    "            \"lr\": np.mean(lrs[-(train_i + 1) :]),\n",
    "            \"epoch\": epoch,\n",
    "            \"tube_mask_ratio\": tube_mask_ratio,\n",
    "            \"decoder_mask_ratio\": decoder_mask_ratio,\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        if distributed: print(logs)\n",
    "\n",
    "#         if global_rank==0:\n",
    "           # Plot progress (first sample in batch)\n",
    "#             with torch.no_grad():\n",
    "#                 if utils.is_interactive():\n",
    "#                     idx = 0\n",
    "#                     if epoch % 5 == 0:\n",
    "#                         decode_vis = torch.zeros_like(target_patches_vit)\n",
    "#                         decode_vis[:, decoder_mask] = output.to(decode_vis.device).to(decode_vis.dtype)\n",
    "#                         decoder_unpatches = rearrange(\n",
    "#                             decode_vis,\n",
    "#                             \"b (f d h w) c -> b f d h w c\",\n",
    "#                             d=img_size[0]//patch_size,\n",
    "#                             h=img_size[1]//patch_size,\n",
    "#                             w=img_size[2]//patch_size,\n",
    "#                         )\n",
    "#                         decoder_func = rearrange(\n",
    "#                             decoder_unpatches,\n",
    "#                             \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "#                             b=batch_size*2,\n",
    "#                             f=num_frames,\n",
    "#                             d=img_size[0]//patch_size,\n",
    "#                             h=img_size[1]//patch_size,\n",
    "#                             w=img_size[2]//patch_size,\n",
    "#                             pd=patch_size,\n",
    "#                             ph=patch_size,\n",
    "#                             pw=patch_size,\n",
    "#                             pf=frame_patch_size,\n",
    "#                         )\n",
    "#                         orig_image = utils.reshape_to_2d(func[idx])\n",
    "#                         recon_image = utils.reshape_to_2d(decoder_func[idx])\n",
    "    \n",
    "#                         combined_image = orig_image.clone()\n",
    "#                         combined_image[recon_image!=0] = recon_image[recon_image!=0]\n",
    "                        \n",
    "#                         random_start = np.random.randint(recon_image.shape[1]-400)\n",
    "#                         orig_image = transforms.ToPILImage()(orig_image[:,random_start:random_start+400])\n",
    "#                         recon_image = transforms.ToPILImage()(recon_image[:,random_start:random_start+400])\n",
    "#                         combined_image = transforms.ToPILImage()(combined_image[:,random_start:random_start+400])\n",
    "    \n",
    "#                         if wandb_log:\n",
    "#                             logs[f\"train/orig\"] = wandb.Image(orig_image, caption=f\"epoch{epoch:03d}\")\n",
    "#                             logs[f\"train/recon\"] = wandb.Image(recon_image, caption=f\"epoch{epoch:03d}\")\n",
    "#                             logs[f\"train/combined\"] = wandb.Image(combined_image, caption=f\"epoch{epoch:03d}\")\n",
    "#                         else:\n",
    "#                             # display(orig_image)\n",
    "#                             # display(recon_image)\n",
    "#                             display(combined_image)\n",
    "#             if wandb_log: wandb.log(logs)\n",
    "        \n",
    "        # wait for other GPUs to catch up if needed\n",
    "#         if distributed: dist.barrier()\n",
    "\n",
    "        # Save model checkpoint\n",
    "        if (ckpt_saving) and ((epoch % ckpt_interval == 0) or (epoch==num_epochs-1)) and (debug==False):\n",
    "            save_ckpt(model,\"last\")\n",
    "            \n",
    "        # wait for other GPUs to catch up if needed\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "if distributed:\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6e847-2511-4f69-aa70-f1471a5b7d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(recon_losses)\n",
    "plt.title(\"Training re-construction losses\")\n",
    "plt.show()\n",
    "if use_contrastive_loss:\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(contrastive_losses)\n",
    "    plt.title(\"Training contrastive losses\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mamba_fmri)",
   "language": "python",
   "name": "mamba_fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
