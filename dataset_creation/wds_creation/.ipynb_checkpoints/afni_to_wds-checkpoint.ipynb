{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d078f7e7-ccbc-4a88-9d12-93510fe1814b",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56bbd92-27d1-4f94-a579-029661eb72f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtio\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpolate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interp1d\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchio'"
     ]
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import sys\n",
    "from subprocess import call\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import re\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import webdataset as wds\n",
    "import nibabel as nib\n",
    "import pickle as pkl\n",
    "from einops import rearrange\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchio as tio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c1ba3-39ff-458c-bc3c-e452445eb10c",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46adfbe0-91ef-43ae-87a9-c2e4ea89033a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape_to_2d(tensor):\n",
    "    return rearrange(tensor, 'b h w c -> (b h) (c w)')\n",
    "\n",
    "def reshape_to_original(tensor_2d, b=300, h=64, w=64, c=48):\n",
    "    return rearrange(tensor_2d, '(b h) (c w) -> b h w c', b=b, h=h, w=w, c=c)\n",
    "\n",
    "def header_to_dict(header):\n",
    "    readable_header = {}\n",
    "    for key, value in header.items():\n",
    "        readable_header[key] = value\n",
    "    return readable_header\n",
    "\n",
    "def temporal_interp1d(fmri_data, change_TR):\n",
    "    original_time_points = np.arange(fmri_data.shape[0])  # Time points: 0, 1, 2, ..., T-1\n",
    "    new_time_points = np.arange(0, fmri_data.shape[0], change_TR)  # New time points: 0, 2, 4, ...\n",
    "\n",
    "    reshaped_data = fmri_data.reshape(fmri_data.shape[0], -1)  # Reshape to (T, X*Y*Z)\n",
    "    interpolate = interp1d(original_time_points, reshaped_data, kind='linear', axis=0, bounds_error=False, fill_value=\"extrapolate\")\n",
    "    resampled_fmri_data = interpolate(new_time_points).reshape((len(new_time_points),) + fmri_data.shape[1:])\n",
    "    return resampled_fmri_data\n",
    "\n",
    "def torchio_slice(data,xslice=None,yslice=None,zslice=None):    \n",
    "    if xslice is None: xslice = data.shape[1] // 2\n",
    "    if yslice is None: yslice = data.shape[2] // 2\n",
    "    if zslice is None: zslice = data.shape[3] // 2\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(5,5))\n",
    "\n",
    "    # Plot the three different slices\n",
    "    axs[0].imshow(data[0, xslice], cmap='gray')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(f'Slice [0, {xslice}]', fontsize=8)\n",
    "\n",
    "    axs[1].imshow(data[0, :, yslice], cmap='gray')\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title(f'Slice [0, :, {yslice}]', fontsize=8)\n",
    "\n",
    "    axs[2].imshow(data[0, :, :, zslice], cmap='gray')\n",
    "    axs[2].axis('off')\n",
    "    axs[2].set_title(f'Slice [0, :, :, {zslice}]', fontsize=8)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8f6f9-ce59-46fc-9b77-43d6ce5a3920",
   "metadata": {},
   "source": [
    "## Create dir to save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852d060-955e-47ec-ad04-1a285ee38059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_folder = os.getcwd()+'/temp_MNIs'\n",
    "os.makedirs(temp_folder, exist_ok=True)\n",
    "print(temp_folder)\n",
    "\n",
    "wds_folder = os.getcwd()+'/openneuro_wds'\n",
    "prefix = 'fmri_foundation_datasets/openneuro_MNI/'\n",
    "\n",
    "# wds_folder = os.getcwd()+'/nsd_wds'\n",
    "# prefix = 'fmri_foundation_datasets/NSD_MNI/'\n",
    "\n",
    "os.makedirs(wds_folder, exist_ok=True)\n",
    "print(wds_folder)\n",
    "\n",
    "s3_output_folder_name = prefix.split('/')[1]+'_wds'\n",
    "print(s3_output_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd6967f-586c-4193-9731-e3672d6ecfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete saved files\n",
    "command = f\"rm {temp_folder}/*\"\n",
    "call(command,shell=True)\n",
    "\n",
    "# delete saved files\n",
    "command = f\"rm {wds_folder}/*\"\n",
    "call(command,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c684fb6-30f5-4413-97bc-91eac75e1aa7",
   "metadata": {},
   "source": [
    "## Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3a7f2-4677-416c-9068-01691c69ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'proj-fmri'\n",
    "\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "file_name_list = []\n",
    "for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "    for obj in page.get('Contents', []):\n",
    "        file_name = obj['Key']\n",
    "        file_name_list.append(file_name)\n",
    "print(\"len(file_name_list) =\", len(file_name_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d8524-bdd1-4382-83bf-431f70e08a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a dictionary to hold the categorized file paths\n",
    "datasets = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "for file_name in file_name_list:\n",
    "    parts = file_name.split('/')\n",
    "    dataset_id = parts[2]\n",
    "    subject_parts = parts[3].split('_')\n",
    "    subject_id = subject_parts[0]  # Extract the subject identifier\n",
    "\n",
    "    # Check for session identifier, default to \"ses-01\" if not present\n",
    "    session_id = \"ses-01\"\n",
    "    for part in subject_parts:\n",
    "        if part.startswith(\"ses-\"):\n",
    "            session_id = part\n",
    "            break\n",
    "    \n",
    "    datasets[dataset_id][subject_id][session_id].append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8cd67c0-011b-4d49-9fc6-4c14a21d41f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Starting from scratch!\")\n",
    "\n",
    "try:\n",
    "    sink.close()\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "tar_count = 0\n",
    "TR_count = 0\n",
    "subj_count = 0\n",
    "dataset_count = 0\n",
    "dataset_list = []\n",
    "\n",
    "print(f\"TR_count: {TR_count}\")\n",
    "print(f\"subj_count: {subj_count}\")\n",
    "print(f\"tar_count: {tar_count}\")\n",
    "print(f\"dataset_count: {dataset_count}\")\n",
    "\n",
    "TRs_per_sample = 32\n",
    "max_samples_per_tar = 30 # try to translate to around 1 Gb per tar\n",
    "max_TRs_per_tar = max_samples_per_tar * TRs_per_sample\n",
    "\n",
    "sample_idx = 0\n",
    "current_dataset = None\n",
    "current_subject = None\n",
    "\n",
    "MNI_mask = nib.load(\"/weka/proj-fmri/paulscotti/fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "MNI_mask[MNI_mask>0]=1\n",
    "MNI_mask = MNI_mask.astype(bool)\n",
    "\n",
    "sink = wds.TarWriter(f\"{wds_folder}/{tar_count:06d}.tar\")\n",
    "\n",
    "for dataset_id in list(datasets.keys()):\n",
    "    first = True\n",
    "    for subject_id in list(datasets[dataset_id].keys()):\n",
    "        subj_count += 1\n",
    "        for session_id in list(datasets[dataset_id][subject_id].keys()):\n",
    "            print(f\"Processing {dataset_id} {subject_id} {session_id} | TR_count {TR_count}\")\n",
    "            \n",
    "            # first get min and max values across all runs in session\n",
    "            run_count = 0\n",
    "            for file_name in datasets[dataset_id][subject_id][session_id]:\n",
    "                temp_file_path = temp_folder + '/' + file_name.split('/')[2] + '_' + file_name.split('/')[-1]\n",
    "            \n",
    "                if not os.path.exists(temp_file_path):\n",
    "                    # s3.download_file(bucket_name, file_name, temp_file_path)\n",
    "                    command = f\"aws s3 cp s3://proj-fmri/{file_name} {temp_file_path}\"\n",
    "                    call(command,shell=True)\n",
    "\n",
    "                if not os.path.exists(temp_file_path):\n",
    "                    raise Exception(\"s3 file not found\")\n",
    "            \n",
    "                func_nii = nib.load(temp_file_path).get_fdata()\n",
    "                func_nii = np.moveaxis(func_nii, -1, 0)\n",
    "                data = func_nii[:,MNI_mask] # find normalization values only inside of the MNI brain mask\n",
    "\n",
    "                # ignore outliers via standard deviation exclusion\n",
    "                low = data.mean() - 2 * data.std()\n",
    "                high = data.mean() + 2 * data.std()\n",
    "                filtered_data = data[(data > low) & (data < high)]\n",
    "                min_val = np.min(filtered_data)\n",
    "                max_val = np.max(filtered_data)\n",
    "                \n",
    "                run_count +=1\n",
    "                if run_count==1: \n",
    "                    min = min_val\n",
    "                    max = max_val\n",
    "                else:\n",
    "                    min += min_val\n",
    "                    max += max_val\n",
    "                    \n",
    "            min /= run_count\n",
    "            max /= run_count\n",
    "            print(f\"min = {min} | max = {max}\")\n",
    "            \n",
    "            for file_name in datasets[dataset_id][subject_id][session_id]:\n",
    "                temp_file_path = temp_folder + '/' + file_name.split('/')[2] + '_' + file_name.split('/')[-1]\n",
    "    \n",
    "                func_nii = nib.load(temp_file_path).get_fdata()\n",
    "                func_nii = np.moveaxis(func_nii, -1, 0)\n",
    "                func_nii = func_nii[:,6:94,8:112,10:82].astype(np.float16) # [T, 97, 115, 97] to [T, 88, 104, 72]\n",
    "\n",
    "                if first and is_interactive():\n",
    "                    display(torchio_slice(func_nii))\n",
    "                    first = False\n",
    "    \n",
    "                # normalize by min max\n",
    "                func_nii = (func_nii - min) / (max - min)\n",
    "            \n",
    "                # create samples of TRs_per_sample TRs\n",
    "                for batch in range(0,len(func_nii),TRs_per_sample):\n",
    "                    out = func_nii[batch:batch+TRs_per_sample]\n",
    "                    if len(out)!=TRs_per_sample:\n",
    "                        out = func_nii[-TRs_per_sample:]\n",
    "                        print(\"last batch | len(out) =\",len(out))\n",
    "\n",
    "                    # further sanity check (maybe it was really short run)\n",
    "                    if len(out)!=TRs_per_sample:\n",
    "                        continue\n",
    "            \n",
    "                    sink.write({\n",
    "                        \"__key__\": \"%06d\" % sample_idx,\n",
    "                        \"func.npy\": out,\n",
    "                        \"dataset_id.txt\": dataset_id,\n",
    "                        \"subject_id.txt\": subject_id,\n",
    "                        \"session_id.txt\": session_id,\n",
    "                    })\n",
    "            \n",
    "                    TR_count += len(out)\n",
    "                    sample_idx += 1\n",
    "            \n",
    "                    if sample_idx >= max_samples_per_tar:\n",
    "                        print(\"HIT MAX SAMPLES PER TAR\")\n",
    "                        sink.close()\n",
    "                        sample_idx = 0\n",
    "            \n",
    "                        # make metadata file and save progress to aws s3\n",
    "                        data = {\n",
    "                            \"TR_count\": TR_count,\n",
    "                        }\n",
    "                        with open(f\"{wds_folder}/metadata.json\", \"w\") as file:\n",
    "                            json.dump(data, file)\n",
    "            \n",
    "                        # send to aws s3\n",
    "                        command = f\"aws s3 sync {wds_folder} s3://proj-fmri/fmri_foundation_datasets/{s3_output_folder_name}/\"\n",
    "                        call(command,shell=True)\n",
    "\n",
    "                        # remove local copy\n",
    "                        command = f\"rm {wds_folder}/{tar_count:06d}.tar\"\n",
    "                        call(command,shell=True)\n",
    "            \n",
    "                        tar_count += 1\n",
    "                        sink = wds.TarWriter(f\"{wds_folder}/{tar_count:06d}.tar\")\n",
    "\n",
    "            # delete saved files\n",
    "            command = f\"rm {temp_folder}/*\"\n",
    "            call(command,shell=True)\n",
    "\n",
    "print(\"TR_count\",TR_count)   \n",
    "print(\"subj_count\", subj_count)\n",
    "try:\n",
    "    sink.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "data = {\n",
    "    \"TR_count\": TR_count,\n",
    "}\n",
    "\n",
    "with open(f\"{wds_folder}/metadata.json\", \"w\") as file:\n",
    "    json.dump(data, file)\n",
    "\n",
    "# send to aws s3\n",
    "command = f\"aws s3 sync {wds_folder} s3://proj-fmri/fmri_foundation_datasets/{s3_output_folder_name}/\"\n",
    "call(command,shell=True)\n",
    "\n",
    "# delete saved files\n",
    "command = f\"rm {temp_folder}/*\"\n",
    "call(command,shell=True)\n",
    "\n",
    "last_tar_count = tar_count - 1\n",
    "command = f\"rm {wds_folder}/{last_tar_count:06d}.tar\"\n",
    "call(command,shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "found",
   "language": "python",
   "name": "found"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
