{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d078f7e7-ccbc-4a88-9d12-93510fe1814b",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56bbd92-27d1-4f94-a579-029661eb72f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import sys\n",
    "from subprocess import call, DEVNULL\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import re\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import webdataset as wds\n",
    "import nibabel as nib\n",
    "import pickle as pkl\n",
    "from einops import rearrange\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from litdata import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c1ba3-39ff-458c-bc3c-e452445eb10c",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46adfbe0-91ef-43ae-87a9-c2e4ea89033a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape_to_2d(tensor):\n",
    "    return rearrange(tensor, 'b h w c -> (b h) (c w)')\n",
    "\n",
    "def reshape_to_original(tensor_2d, b=300, h=64, w=64, c=48):\n",
    "    return rearrange(tensor_2d, '(b h) (c w) -> b h w c', b=b, h=h, w=w, c=c)\n",
    "\n",
    "def header_to_dict(header):\n",
    "    readable_header = {}\n",
    "    for key, value in header.items():\n",
    "        readable_header[key] = value\n",
    "    return readable_header\n",
    "\n",
    "def temporal_interp1d(fmri_data, change_TR):\n",
    "    original_time_points = np.arange(fmri_data.shape[0])  # Time points: 0, 1, 2, ..., T-1\n",
    "    new_time_points = np.arange(0, fmri_data.shape[0], change_TR)  # New time points: 0, 2, 4, ...\n",
    "\n",
    "    reshaped_data = fmri_data.reshape(fmri_data.shape[0], -1)  # Reshape to (T, X*Y*Z)\n",
    "    interpolate = interp1d(original_time_points, reshaped_data, kind='linear', axis=0, bounds_error=False, fill_value=\"extrapolate\")\n",
    "    resampled_fmri_data = interpolate(new_time_points).reshape((len(new_time_points),) + fmri_data.shape[1:])\n",
    "    return resampled_fmri_data\n",
    "    \n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8f6f9-ce59-46fc-9b77-43d6ce5a3920",
   "metadata": {},
   "source": [
    "## Create dir to save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9852d060-955e-47ec-ad04-1a285ee38059",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/temp_MNIs\n",
      "/weka/proj-fmri/paulscotti/fMRI-foundation-model/dataset_creation/wds_creation/nsd_litdata\n",
      "NSD_MNI_litdata\n"
     ]
    }
   ],
   "source": [
    "temp_folder = '/scratch/temp_MNIs'\n",
    "os.makedirs(temp_folder, exist_ok=True)\n",
    "print(temp_folder)\n",
    "\n",
    "# wds_folder = os.getcwd()+'/openneuro_wds'\n",
    "# prefix = 'fmri_foundation_datasets/openneuro_MNI/'\n",
    "\n",
    "wds_folder = os.getcwd()+'/nsd_litdata' #'/nsd_wds'\n",
    "prefix = 'fmri_foundation_datasets/NSD_MNI/'\n",
    "\n",
    "os.makedirs(wds_folder, exist_ok=True)\n",
    "print(wds_folder)\n",
    "\n",
    "s3_output_folder_name = prefix.split('/')[1]+'_litdata'\n",
    "print(s3_output_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd6967f-586c-4193-9731-e3672d6ecfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # delete saved files\n",
    "# command = f\"rm {temp_folder}/*\"\n",
    "# call(command,shell=True)\n",
    "\n",
    "# delete saved files\n",
    "command = f\"rm {wds_folder}/*\"\n",
    "call(command,shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c684fb6-30f5-4413-97bc-91eac75e1aa7",
   "metadata": {},
   "source": [
    "## Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee3a7f2-4677-416c-9068-01691c69ef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(file_name_list) = 3696\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'proj-fmri'\n",
    "\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "file_name_list = []\n",
    "for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "    for obj in page.get('Contents', []):\n",
    "        file_name = obj['Key']\n",
    "        file_name_list.append(file_name)\n",
    "print(\"len(file_name_list) =\", len(file_name_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c772d79-3384-418c-b257-4d59a9ae0842",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# command = f\"aws s3 sync s3://proj-fmri/{prefix[:-1]} {temp_folder}\"\n",
    "# print(command)\n",
    "# call(command, shell=True, stdout=DEVNULL, stderr=DEVNULL)\n",
    "\n",
    "# # print(f\"\\nchecking for file {file_name_list[-1]}...\")\n",
    "# # if not os.path.exists(file_name_list[-1]):\n",
    "# #     time.sleep(20)\n",
    "\n",
    "# print(\"\\nready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "792d8524-bdd1-4382-83bf-431f70e08a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a dictionary to hold the categorized file paths\n",
    "datasets = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "datasets_minmean = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "for file_name in file_name_list:\n",
    "    parts = file_name.split('/')\n",
    "    dataset_id = parts[2]\n",
    "    subject_parts = parts[3].split('_')\n",
    "    subject_id = subject_parts[0]  # Extract the subject identifier\n",
    "\n",
    "    # Check for session identifier, default to \"ses-01\" if not present\n",
    "    session_id = \"ses-01\"\n",
    "    for part in subject_parts:\n",
    "        if part.startswith(\"ses-\"):\n",
    "            session_id = part\n",
    "            break\n",
    "    \n",
    "    datasets[dataset_id][subject_id][session_id].append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8cd67c0-011b-4d49-9fc6-4c14a21d41f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ses-prffloc sub-01 ses-prffloc\n",
      "min = 0.0 | max = 1058.3333333333333\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ses-prffloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m run_count\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | max = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m \u001b[43mdatasets_minmean\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m]\u001b[49m[subject_id][session_id] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmin\u001b[39m,\u001b[38;5;28mmax\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ses-prffloc'"
     ]
    }
   ],
   "source": [
    "# TRs_per_sample = 32\n",
    "# max_samples_per_tar = 30 # try to translate to around 1 Gb per tar\n",
    "# max_TRs_per_tar = max_samples_per_tar * TRs_per_sample\n",
    "\n",
    "# current_dataset = None\n",
    "# current_subject = None\n",
    "\n",
    "# MNI_mask = nib.load(\"/weka/proj-fmri/paulscotti/fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "# MNI_mask[MNI_mask>0]=1\n",
    "# MNI_mask = MNI_mask.astype(bool)\n",
    "\n",
    "# for dataset_id in ['ses-prffloc']:#list(datasets.keys()):\n",
    "#     for subject_id in list(datasets[dataset_id].keys()):\n",
    "#         for session_id in list(datasets[dataset_id][subject_id].keys()):\n",
    "#             print(f\"Processing {dataset_id} {subject_id} {session_id}\")\n",
    "#             # first get min and max values across all runs in session\n",
    "#             run_count = 0\n",
    "#             for file_name in datasets[dataset_id][subject_id][session_id]:\n",
    "#                 temp_file_path = f\"{temp_folder}/{file_name.split('/')[2]}/{file_name.split('/')[3]}\"\n",
    "#                 # temp_folder + '/' + file_name.split('/')[2] + '_' + file_name.split('/')[-1]\n",
    "            \n",
    "#                 # if not os.path.exists(temp_file_path):\n",
    "#                 #     # s3.download_file(bucket_name, file_name, temp_file_path)\n",
    "#                 #     command = f\"aws s3 cp s3://proj-fmri/{file_name} {temp_file_path}\"\n",
    "#                 #     call(command,shell=True)\n",
    "\n",
    "#                 if not os.path.exists(temp_file_path):\n",
    "#                     raise Exception(\"s3 file not found\")\n",
    "            \n",
    "#                 func_nii = nib.load(temp_file_path).get_fdata()\n",
    "#                 func_nii = np.moveaxis(func_nii, -1, 0)\n",
    "#                 data = func_nii[:,MNI_mask] # find normalization values only inside of the MNI brain mask\n",
    "\n",
    "#                 # ignore outliers via standard deviation exclusion\n",
    "#                 low = data.mean() - 2 * data.std()\n",
    "#                 high = data.mean() + 2 * data.std()\n",
    "#                 filtered_data = data[(data > low) & (data < high)]\n",
    "#                 min_val = np.min(filtered_data)\n",
    "#                 max_val = np.max(filtered_data)\n",
    "                \n",
    "#                 run_count +=1\n",
    "#                 if run_count==1: \n",
    "#                     min = min_val\n",
    "#                     max = max_val\n",
    "#                 else:\n",
    "#                     min += min_val\n",
    "#                     max += max_val\n",
    "                    \n",
    "#             min /= run_count\n",
    "#             max /= run_count\n",
    "#             print(f\"min = {min} | max = {max}\")\n",
    "#             datasets_minmean[dataset_id][subject_id][session_id] = [min,max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e339378-efc8-4073-a06f-316dc53f3995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open('minmax_datasets.json', 'w') as file:\n",
    "#     json.dump(datasets_minmean, file)\n",
    "# json_dump = True\n",
    "# print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a718fb1-0860-4e98-9cd1-cfb6173c52c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded!\n"
     ]
    }
   ],
   "source": [
    "with open('minmax_datasets.json', 'r') as file:\n",
    "    datasets_minmean = json.load(file)\n",
    "print(\"loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1df0ef1f-3e7f-419a-82f5-ea575aa0cd9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def making_litdata(file_name):\n",
    "    parts = file_name.split('/')\n",
    "    dataset_id = parts[2]\n",
    "    subject_parts = parts[3].split('_')\n",
    "    subject_id = subject_parts[0]  # Extract the subject identifier\n",
    "    \n",
    "    # Check for session identifier, default to \"ses-01\" if not present\n",
    "    session_id = \"ses-01\"\n",
    "    for part in subject_parts:\n",
    "        if part.startswith(\"ses-\"):\n",
    "            session_id = part\n",
    "            break\n",
    "            \n",
    "    minmax = datasets_minmean[dataset_id][subject_id][session_id]\n",
    "    min, max = minmax[0], minmax[1]\n",
    "    \n",
    "    temp_file_path = f\"{temp_folder}/{file_name.split('/')[2]}/{file_name.split('/')[3]}\"\n",
    "    # temp_file_path = temp_folder + '/' + file_name.split('/')[2] + '_' + file_name.split('/')[-1]\n",
    "\n",
    "    # if not os.path.exists(temp_file_path):\n",
    "    #     # s3.download_file(bucket_name, file_name, temp_file_path)\n",
    "    #     command = f\"aws s3 cp s3://proj-fmri/{file_name} {temp_file_path}\"\n",
    "    #     call(command,shell=True)\n",
    "\n",
    "    if not os.path.exists(temp_file_path):\n",
    "        raise Exception(\"s3 file not found\")\n",
    "    \n",
    "    func_nii = nib.load(temp_file_path).get_fdata()\n",
    "    func_nii = np.moveaxis(func_nii, -1, 0)\n",
    "    func_nii = func_nii[:,6:94,8:112,10:82].astype(np.float16) # [T, 97, 115, 97] to [T, 88, 104, 72]\n",
    "\n",
    "    # normalize by min max\n",
    "    func_nii = (func_nii - min) / (max - min)\n",
    "    \n",
    "    data = {\n",
    "        \"func\": func_nii, \n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cced8489-18a3-453b-af9b-d5897c1a9b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prffloc encountered an error so for sake of time im just skipping it\n",
      "Create an account on https://lightning.ai/ to optimize your data faster using multiple nodes and large machines.\n",
      "Storing the files under /weka/proj-fmri/paulscotti/fMRI-foundation-model/dataset_creation/wds_creation/nsd_litdata\n",
      "Setup started with fast_dev_run=False.\n",
      "Setup finished in 0.146 seconds. Found 3600 items to process.\n",
      "Starting 20 workers with 3600 items.\n",
      "Workers are ready ! Starting data processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b0f2f882e74ceba7288b0f7b4f4ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                 | 0/3600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 5 inferred the following `['numpy']` data format.\n",
      "Rank 6 inferred the following `['numpy']` data format.\n",
      "Rank 12 inferred the following `['numpy']` data format.\n",
      "Rank 3 inferred the following `['numpy']` data format.\n",
      "Rank 7 inferred the following `['numpy']` data format.\n",
      "Rank 2 inferred the following `['numpy']` data format.\n",
      "Rank 8 inferred the following `['numpy']` data format.\n",
      "Rank 17 inferred the following `['numpy']` data format.\n",
      "Rank 18 inferred the following `['numpy']` data format.\n",
      "Rank 11 inferred the following `['numpy']` data format.\n",
      "Rank 1 inferred the following `['numpy']` data format.\n",
      "Rank 13 inferred the following `['numpy']` data format.\n",
      "Rank 10 inferred the following `['numpy']` data format.\n",
      "Rank 15 inferred the following `['numpy']` data format.\n",
      "Rank 9 inferred the following `['numpy']` data format.\n",
      "Rank 4 inferred the following `['numpy']` data format.\n",
      "Rank 19 inferred the following `['numpy']` data format.\n",
      "Rank 16 inferred the following `['numpy']` data format.\n",
      "Rank 14 inferred the following `['numpy']` data format.\n",
      "Rank 0 inferred the following `['numpy']` data format.\n",
      "Worker 6 is terminating.\n",
      "Worker 6 is done.\n",
      "Worker 4 is terminating.\n",
      "Worker 8 is terminating.\n",
      "Worker 4 is done.\n",
      "Worker 8 is done.\n",
      "Worker 19 is terminating.\n",
      "Worker 19 is done.\n",
      "Worker 12 is terminating.\n",
      "Worker 1 is terminating.\n",
      "Worker 12 is done.\n",
      "Worker 1 is done.\n",
      "Worker 3 is terminating.\n",
      "Worker 3 is done.\n",
      "Worker 0 is terminating.\n",
      "Worker 2 is terminating.\n",
      "Worker 0 is done.\n",
      "Worker 2 is done.\n",
      "Worker 14 is terminating.\n",
      "Worker 14 is done.\n",
      "Worker 16 is terminating.\n",
      "Worker 11 is terminating.\n",
      "Worker 16 is done.\n",
      "Worker 11 is done.\n",
      "Worker 13 is terminating.\n",
      "Worker 5 is terminating.\n",
      "Worker 13 is done.\n",
      "Worker 5 is done.\n",
      "Worker 7 is terminating.\n",
      "Worker 7 is done.\n",
      "Worker 9 is terminating.\n",
      "Worker 9 is done.\n",
      "Worker 15 is terminating.\n",
      "Worker 15 is done.\n",
      "Worker 18 is terminating.\n",
      "Worker 18 is done.\n",
      "Worker 10 is terminating.\n",
      "Worker 10 is done.\n",
      "Worker 17 is terminating.\n",
      "Worker 17 is done.\n",
      "Workers are finished.\n",
      "Finished data processing!\n"
     ]
    }
   ],
   "source": [
    "filtered_file_name_list = [file for file in file_name_list if \"prffloc\" not in file]\n",
    "print(\"prffloc encountered an error so for sake of time im just skipping it\")\n",
    "\n",
    "optimize(\n",
    "    fn=making_litdata,  # The function applied over each input.\n",
    "    inputs=filtered_file_name_list,  # Provide any inputs. The fn is applied on each item.\n",
    "    output_dir=wds_folder,  # The directory where the optimized data are stored.\n",
    "    num_workers=16,  # The number of workers. The inputs are distributed among them.\n",
    "    chunk_bytes=\"256MB\"  # The maximum number of bytes to write into a data chunk.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c5b021-8ab8-4533-9979-70f3388319b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: nsd_litdata/chunk-0-0.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-0.bin\n",
      "upload: nsd_litdata/chunk-0-10.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-10.bin\n",
      "upload: nsd_litdata/chunk-0-1.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-1.bin\n",
      "upload: nsd_litdata/chunk-0-100.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-100.bin\n",
      "upload: nsd_litdata/chunk-0-101.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-101.bin\n",
      "upload: nsd_litdata/chunk-0-102.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-102.bin\n",
      "upload: nsd_litdata/chunk-0-104.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-104.bin\n",
      "upload: nsd_litdata/chunk-0-106.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-106.bin\n",
      "upload: nsd_litdata/chunk-0-103.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-103.bin\n",
      "upload: nsd_litdata/chunk-0-105.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-105.bin\n",
      "upload: nsd_litdata/chunk-0-107.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-107.bin\n",
      "upload: nsd_litdata/chunk-0-108.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-108.bin\n",
      "upload: nsd_litdata/chunk-0-109.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-109.bin\n",
      "upload: nsd_litdata/chunk-0-110.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-110.bin\n",
      "upload: nsd_litdata/chunk-0-111.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-111.bin\n",
      "upload: nsd_litdata/chunk-0-11.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-11.bin\n",
      "upload: nsd_litdata/chunk-0-112.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-112.bin\n",
      "upload: nsd_litdata/chunk-0-113.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-113.bin\n",
      "upload: nsd_litdata/chunk-0-114.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-114.bin\n",
      "upload: nsd_litdata/chunk-0-115.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-115.bin\n",
      "upload: nsd_litdata/chunk-0-116.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-116.bin\n",
      "upload: nsd_litdata/chunk-0-117.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-117.bin\n",
      "upload: nsd_litdata/chunk-0-118.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-118.bin\n",
      "upload: nsd_litdata/chunk-0-119.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-119.bin\n",
      "upload: nsd_litdata/chunk-0-12.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-12.bin\n",
      "upload: nsd_litdata/chunk-0-120.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-120.bin\n",
      "upload: nsd_litdata/chunk-0-121.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-121.bin\n",
      "upload: nsd_litdata/chunk-0-122.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-122.bin\n",
      "upload: nsd_litdata/chunk-0-123.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-123.bin\n",
      "upload: nsd_litdata/chunk-0-124.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-124.bin\n",
      "upload: nsd_litdata/chunk-0-125.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-125.bin\n",
      "upload: nsd_litdata/chunk-0-126.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-126.bin\n",
      "upload: nsd_litdata/chunk-0-127.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-127.bin\n",
      "upload: nsd_litdata/chunk-0-128.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-128.bin\n",
      "upload: nsd_litdata/chunk-0-129.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-129.bin\n",
      "upload: nsd_litdata/chunk-0-13.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-13.bin\n",
      "upload: nsd_litdata/chunk-0-130.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-130.bin\n",
      "upload: nsd_litdata/chunk-0-131.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-131.bin\n",
      "upload: nsd_litdata/chunk-0-132.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-132.bin\n",
      "upload: nsd_litdata/chunk-0-133.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-133.bin\n",
      "upload: nsd_litdata/chunk-0-134.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-134.bin\n",
      "upload: nsd_litdata/chunk-0-135.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-135.bin\n",
      "upload: nsd_litdata/chunk-0-136.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-136.bin\n",
      "upload: nsd_litdata/chunk-0-137.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-137.bin\n",
      "upload: nsd_litdata/chunk-0-138.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-138.bin\n",
      "upload: nsd_litdata/chunk-0-139.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-139.bin\n",
      "upload: nsd_litdata/chunk-0-140.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-140.bin\n",
      "upload: nsd_litdata/chunk-0-141.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-141.bin\n",
      "upload: nsd_litdata/chunk-0-142.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-142.bin\n",
      "upload: nsd_litdata/chunk-0-14.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-14.bin\n",
      "upload: nsd_litdata/chunk-0-143.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-143.bin\n",
      "upload: nsd_litdata/chunk-0-144.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-144.bin\n",
      "upload: nsd_litdata/chunk-0-147.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-147.bin\n",
      "upload: nsd_litdata/chunk-0-145.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-145.bin\n",
      "upload: nsd_litdata/chunk-0-146.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-146.bin\n",
      "upload: nsd_litdata/chunk-0-148.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-148.bin\n",
      "upload: nsd_litdata/chunk-0-149.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-149.bin\n",
      "upload: nsd_litdata/chunk-0-15.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-15.bin\n",
      "upload: nsd_litdata/chunk-0-151.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-151.bin\n",
      "upload: nsd_litdata/chunk-0-150.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-150.bin\n",
      "upload: nsd_litdata/chunk-0-152.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-152.bin\n",
      "upload: nsd_litdata/chunk-0-153.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-153.bin\n",
      "upload: nsd_litdata/chunk-0-155.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-155.bin\n",
      "upload: nsd_litdata/chunk-0-156.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-156.bin\n",
      "upload: nsd_litdata/chunk-0-157.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-157.bin\n",
      "upload: nsd_litdata/chunk-0-154.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-154.bin\n",
      "upload: nsd_litdata/chunk-0-158.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-158.bin\n",
      "upload: nsd_litdata/chunk-0-159.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-159.bin\n",
      "upload: nsd_litdata/chunk-0-16.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-16.bin\n",
      "upload: nsd_litdata/chunk-0-160.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-160.bin\n",
      "upload: nsd_litdata/chunk-0-161.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-161.bin\n",
      "upload: nsd_litdata/chunk-0-163.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-163.bin\n",
      "upload: nsd_litdata/chunk-0-162.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-162.bin\n",
      "upload: nsd_litdata/chunk-0-164.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-164.bin\n",
      "upload: nsd_litdata/chunk-0-166.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-166.bin\n",
      "upload: nsd_litdata/chunk-0-165.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-165.bin\n",
      "upload: nsd_litdata/chunk-0-168.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-168.bin\n",
      "upload: nsd_litdata/chunk-0-17.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-17.bin\n",
      "upload: nsd_litdata/chunk-0-169.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-169.bin\n",
      "upload: nsd_litdata/chunk-0-170.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-170.bin\n",
      "upload: nsd_litdata/chunk-0-167.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-167.bin\n",
      "upload: nsd_litdata/chunk-0-171.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-171.bin\n",
      "upload: nsd_litdata/chunk-0-172.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-172.bin\n",
      "upload: nsd_litdata/chunk-0-173.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-173.bin\n",
      "upload: nsd_litdata/chunk-0-174.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-174.bin\n",
      "upload: nsd_litdata/chunk-0-175.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-175.bin\n",
      "upload: nsd_litdata/chunk-0-176.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-176.bin\n",
      "upload: nsd_litdata/chunk-0-177.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-177.bin\n",
      "upload: nsd_litdata/chunk-0-179.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-179.bin\n",
      "upload: nsd_litdata/chunk-0-18.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-18.bin\n",
      "upload: nsd_litdata/chunk-0-178.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-178.bin\n",
      "upload: nsd_litdata/chunk-0-19.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-19.bin\n",
      "upload: nsd_litdata/chunk-0-2.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-2.bin\n",
      "upload: nsd_litdata/chunk-0-20.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-20.bin\n",
      "upload: nsd_litdata/chunk-0-22.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-22.bin\n",
      "upload: nsd_litdata/chunk-0-21.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-21.bin\n",
      "upload: nsd_litdata/chunk-0-23.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-23.bin\n",
      "upload: nsd_litdata/chunk-0-24.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-24.bin\n",
      "upload: nsd_litdata/chunk-0-26.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-26.bin\n",
      "upload: nsd_litdata/chunk-0-25.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-25.bin\n",
      "upload: nsd_litdata/chunk-0-27.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-27.bin\n",
      "upload: nsd_litdata/chunk-0-28.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-28.bin\n",
      "upload: nsd_litdata/chunk-0-30.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-30.bin\n",
      "upload: nsd_litdata/chunk-0-3.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-3.bin\n",
      "upload: nsd_litdata/chunk-0-29.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-29.bin\n",
      "upload: nsd_litdata/chunk-0-31.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-31.bin\n",
      "upload: nsd_litdata/chunk-0-32.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-32.bin\n",
      "upload: nsd_litdata/chunk-0-33.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-33.bin\n",
      "upload: nsd_litdata/chunk-0-34.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-34.bin\n",
      "upload: nsd_litdata/chunk-0-35.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-35.bin\n",
      "upload: nsd_litdata/chunk-0-36.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-36.bin\n",
      "upload: nsd_litdata/chunk-0-37.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-37.bin\n",
      "upload: nsd_litdata/chunk-0-38.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-38.bin\n",
      "upload: nsd_litdata/chunk-0-39.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-39.bin\n",
      "upload: nsd_litdata/chunk-0-4.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-4.bin\n",
      "upload: nsd_litdata/chunk-0-40.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-40.bin\n",
      "upload: nsd_litdata/chunk-0-41.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-41.bin\n",
      "upload: nsd_litdata/chunk-0-42.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-42.bin\n",
      "upload: nsd_litdata/chunk-0-43.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-43.bin\n",
      "upload: nsd_litdata/chunk-0-44.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-44.bin\n",
      "upload: nsd_litdata/chunk-0-45.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-45.bin\n",
      "upload: nsd_litdata/chunk-0-46.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-46.bin\n",
      "upload: nsd_litdata/chunk-0-47.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-47.bin\n",
      "upload: nsd_litdata/chunk-0-48.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-48.bin\n",
      "upload: nsd_litdata/chunk-0-49.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-49.bin\n",
      "upload: nsd_litdata/chunk-0-5.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-5.bin\n",
      "upload: nsd_litdata/chunk-0-50.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-50.bin\n",
      "upload: nsd_litdata/chunk-0-52.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-52.bin\n",
      "upload: nsd_litdata/chunk-0-51.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-51.bin\n",
      "upload: nsd_litdata/chunk-0-53.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-53.bin\n",
      "upload: nsd_litdata/chunk-0-54.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-54.bin\n",
      "upload: nsd_litdata/chunk-0-55.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-55.bin\n",
      "upload: nsd_litdata/chunk-0-56.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-56.bin\n",
      "upload: nsd_litdata/chunk-0-57.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-57.bin\n",
      "upload: nsd_litdata/chunk-0-59.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-59.bin\n",
      "upload: nsd_litdata/chunk-0-58.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-58.bin\n",
      "upload: nsd_litdata/chunk-0-6.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-6.bin\n",
      "upload: nsd_litdata/chunk-0-60.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-60.bin\n",
      "upload: nsd_litdata/chunk-0-61.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-61.bin\n",
      "upload: nsd_litdata/chunk-0-62.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-62.bin\n",
      "upload: nsd_litdata/chunk-0-63.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-63.bin\n",
      "upload: nsd_litdata/chunk-0-64.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-64.bin\n",
      "upload: nsd_litdata/chunk-0-65.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-65.bin\n",
      "upload: nsd_litdata/chunk-0-67.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-67.bin\n",
      "upload: nsd_litdata/chunk-0-66.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-66.bin\n",
      "upload: nsd_litdata/chunk-0-68.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-68.bin\n",
      "upload: nsd_litdata/chunk-0-69.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-69.bin\n",
      "upload: nsd_litdata/chunk-0-70.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-70.bin\n",
      "upload: nsd_litdata/chunk-0-7.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-7.bin\n",
      "upload: nsd_litdata/chunk-0-72.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-72.bin\n",
      "upload: nsd_litdata/chunk-0-71.bin to s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_litdata/chunk-0-71.bin\n",
      "Completed 35.8 GiB/~43.6 GiB (240.0 MiB/s) with ~38 file(s) remaining (calculating...)\r"
     ]
    }
   ],
   "source": [
    "command = f\"aws s3 cp --recursive {wds_folder} s3://proj-fmri/fmri_foundation_datasets/{s3_output_folder_name}\"\n",
    "call(command,shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "found",
   "language": "python",
   "name": "found"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
