{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from models import get_vit\n",
    "from accelerate import Accelerator, load_checkpoint_in_model\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(\"NUM GPUS \", num_devices)\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca731e-8906-4f55-ada1-fd9cc44f3213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "\n",
    "# First use \"accelerate config\" in terminal for setup\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "#accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "batch_size = global_batch_size // num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305bcca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58682c-0cb8-4bea-9a3c-6afbddb49316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08179db-9c6a-4bc6-a245-79fae6884ca2",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1c3fe-28ab-40b7-8906-c6a9c8070d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(config)\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "cache_dir = cache_dir + f'/{np.random.randint(9999)}' # create random subfolder so multiple runs arent using same directory\n",
    "os.makedirs(cache_dir,exist_ok=True)\n",
    "print(\"cache_dir\", cache_dir)\n",
    "\n",
    "if use_contrastive_loss:\n",
    "    global_batch_size = global_batch_size // 2 # contrastive loss doubles the batch size with the same samples and different masks\n",
    "print(\"global_batch_size\", global_batch_size)\n",
    "\n",
    "use_cls_token = True if use_contrastive_loss else use_cls_token\n",
    "print(\"use_cls_token\", use_cls_token)\n",
    "\n",
    "num_patches = int(\n",
    "    (img_size[0] / patch_size)\n",
    "    * (img_size[1] / patch_size)\n",
    "    * (img_size[2] / patch_size)\n",
    "    * num_frames\n",
    ")\n",
    "num_patches_per_timepoint = num_patches // num_frames\n",
    "num_encoder_patches = int(num_patches_per_timepoint * (1 - tube_start_masking_ratio) * num_frames)\n",
    "num_decoder_patches = int(num_patches_per_timepoint * (1 - decoder_mask_ratio) * num_frames)\n",
    "print(\"num_patches\", num_patches)\n",
    "print(\"num_encoder_patches\", num_encoder_patches)\n",
    "print(\"num_decoder_patches\", num_decoder_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8419d-988f-42c6-acb6-b258e0694eee",
   "metadata": {},
   "source": [
    "# Prep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e37b6b9-5b91-4c4a-af85-ac2af69704e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_vit(\n",
    "    size=vit_size,\n",
    "    image_size=img_size,  # depth, height, width\n",
    "    image_patch_size=(patch_size,patch_size,patch_size),  # depth, height, width patch size\n",
    "    frames=num_frames,\n",
    "    frame_patch_size=frame_patch_size,\n",
    "    channels=1,\n",
    "    use_rope_emb=use_rope_emb,\n",
    "    use_cls_token=use_cls_token,\n",
    ")\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works without error\n",
    "model = model.to(device)\n",
    "encoder_mask = torch.zeros(num_patches).to(device).to(torch.bool)\n",
    "encoder_mask[:num_encoder_patches] = True\n",
    "decoder_mask = torch.zeros(num_patches).to(device).to(torch.bool)\n",
    "decoder_mask[-num_decoder_patches:] = True\n",
    "with torch.no_grad():\n",
    "    print(\"\\nencoder\")\n",
    "    encoder_out = model(\n",
    "                torch.randn(6, 1, 4, 64, 64, 48).to(device),\n",
    "                encoder_mask=encoder_mask,\n",
    "                verbose=True)\n",
    "    print(\"\\ndecoder\")\n",
    "    decoder_out = model(\n",
    "                encoder_out, \n",
    "                encoder_mask=encoder_mask, \n",
    "                decoder_mask=decoder_mask, \n",
    "                verbose=True)\n",
    "    if use_cls_token:\n",
    "        enc_cls_token = encoder_out[:, :1, :]\n",
    "        encoder_patches = encoder_out[:, 1:, :]\n",
    "        dec_cls_token = decoder_out[:, :1, :]\n",
    "        decoder_patches = decoder_out[:, 1:, :]\n",
    "        print(\"\\nenc_cls_token\", enc_cls_token.shape)\n",
    "        print(\"encoder_patches\", encoder_patches.shape)\n",
    "        print(\"dec_cls_token\", dec_cls_token.shape)\n",
    "        print(\"decoder_patches\", decoder_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd0398-0eb8-464e-8bb8-13e7ce1a8480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aug_transform = utils.DataPrepper(\n",
    "    masking_strategy=\"conservative\",\n",
    "    patch_depth=patch_size,\n",
    "    patch_height=patch_size,\n",
    "    patch_width=patch_size,\n",
    "    frame_patch_size=frame_patch_size,\n",
    ")\n",
    "\n",
    "def log_and_continue(exn):\n",
    "    \"\"\"Call in an exception handler to ignore any exception, issue a warning, and continue.\"\"\"\n",
    "    print(f'Handling webdataset error ({repr(exn)}). Ignoring.')\n",
    "    return True\n",
    "\n",
    "def filter_corrupted_images(sample):\n",
    "    \"\"\"If all the required files are not present don't use them.\"\"\"\n",
    "    correct_data = (\"func.png\" in sample and \"dataset.txt\" in sample and \"header.npy\" in sample and \"meansd.png\" in sample and \"minmax.npy\" in sample)\n",
    "    return correct_data\n",
    "\n",
    "### ================      Train Dataset and DataLoader    ====================\n",
    "if train_urls[:2] == \"s3\":\n",
    "    train_urls = f\"pipe:aws s3 cp {train_urls} -\"\n",
    "print(train_urls)\n",
    "train_data = (\n",
    "    wds.WebDataset(train_urls, resampled=True, nodesplitter=my_split_by_node, cache_dir=cache_dir, handler=log_and_continue)\n",
    "    .shuffle(100, initial=100, rng=random.Random(seed))\n",
    "    .select(filter_corrupted_images)\n",
    "    .rename(key=\"__key__\",\n",
    "        func=\"func.png\",\n",
    "        header=\"header.npy\",\n",
    "        dataset=\"dataset.txt\",\n",
    "        minmax=\"minmax.npy\",\n",
    "        meansd=\"meansd.png\")\n",
    "    .map_dict(func=utils.grayscale_decoder,\n",
    "        meansd=utils.grayscale_decoder,\n",
    "        minmax=utils.numpy_decoder)\n",
    "    .to_tuple(*(\"func\", \"minmax\", \"meansd\"))\n",
    "    .map(aug_transform)\n",
    "    .with_epoch(num_samples_per_epoch)\n",
    ")\n",
    "train_dl = wds.WebLoader(\n",
    "    train_data.batched(batch_size), \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    batch_size=None,\n",
    "    num_workers=num_workers, \n",
    "    persistent_workers=num_workers>0,\n",
    ").with_epoch(num_samples_per_epoch//batch_size)\n",
    "\n",
    "### ================      Test Dataset and DataLoader    ====================\n",
    "if test_urls[:2] == \"s3\":\n",
    "    test_urls = f\"pipe:aws s3 cp {test_urls} -\"\n",
    "print(test_urls)\n",
    "test_data = (\n",
    "    wds.WebDataset(test_urls, resampled=False, nodesplitter=my_split_by_node, cache_dir=cache_dir, handler=log_and_continue)\n",
    "    .select(filter_corrupted_images)\n",
    "    .rename(key=\"__key__\",\n",
    "        func=\"func.png\",\n",
    "        header=\"header.npy\",\n",
    "        dataset=\"dataset.txt\",\n",
    "        minmax=\"minmax.npy\",\n",
    "        meansd=\"meansd.png\")\n",
    "    .map_dict(func=utils.grayscale_decoder,\n",
    "        meansd=utils.grayscale_decoder,\n",
    "        minmax=utils.numpy_decoder)\n",
    "    .to_tuple(*(\"func\", \"minmax\", \"meansd\"))\n",
    "    .map(aug_transform)\n",
    ")\n",
    "test_dl = wds.WebLoader(\n",
    "    test_data.batched(batch_size), \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    batch_size=None,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=num_workers>0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abf33c-1eae-491d-b055-87abee5be97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.state import AcceleratorState\n",
    "try:\n",
    "    AcceleratorState().deepspeed_plugin.deepspeed_config['train_micro_batch_size_per_gpu'] = global_batch_size\n",
    "    print(\"deepspeed reconfigured, train_micro_batch_size_per_gpu = \", global_batch_size)\n",
    "except:\n",
    "    print(\"skipping deepspeed reconfiguration...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d05a32-12eb-494a-82df-dce4c7e9924c",
   "metadata": {},
   "source": [
    "### Check data loaders work and calculate number of iterations per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e7c0b-f58e-4c35-80fe-1284ee0e3966",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations_per_epoch = num_samples_per_epoch // batch_size\n",
    "print(f\"num_iterations_per_epoch {num_iterations_per_epoch}\")\n",
    "\n",
    "train_batch = next(iter(train_dl))\n",
    "func, meansd, brain_pos_pats = train_batch\n",
    "print(\"Train batch:\", func.shape, meansd.shape, brain_pos_pats.shape)\n",
    "\n",
    "for test_num_iterations_per_epoch, test_batch in enumerate(test_dl):\n",
    "    pass\n",
    "print(f\"test_num_iterations_per_epoch {test_num_iterations_per_epoch}\")\n",
    "func, meansd, brain_pos_pats = test_batch\n",
    "print(\"Test batch:\", func.shape, meansd.shape, brain_pos_pats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c6fba-5494-4964-b03d-e01c3afe47db",
   "metadata": {},
   "source": [
    "# Playing with the data, visualization of patching + masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd7913-707a-44ba-b031-afc883fb357c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if utils.is_interactive():\n",
    "    # extract func volumes and their reference mean and standard deviation volumes\n",
    "    func, meansd, brain_pos_pats = train_batch\n",
    "    func = func.unsqueeze(1)  # add empty first dimension to serve as 1d channel dimension\n",
    "\n",
    "    # patchify func samples\n",
    "    print(\"func\", func.shape)\n",
    "    patches = model.patchify(func)\n",
    "    print(\"patches\", patches.shape)\n",
    "    patches_vit = rearrange(patches, \"b ... d -> b (...) d\")\n",
    "    print(\"patches_vit\", patches_vit.shape)\n",
    "    print(\"num patches in one timepoint\", patches_vit.shape[1] // num_frames)\n",
    "\n",
    "    # start by masking everything (aka include nothing)\n",
    "    tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "    # approximate brain positive patches for the whole batch\n",
    "    batch_positive_approx = brain_pos_pats[:, :num_patches//num_frames].float().mean(dim=0) > 0\n",
    "    mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "    mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "    print(\"Percentage of brain positive patches\",\n",
    "        len(mask_idx_candidates) / len(batch_positive_approx))\n",
    "    tube_idx = mask_idx_candidates[: int(num_patches / num_frames * (1 - tube_start_masking_ratio))]\n",
    "    print(\"num tube patches =\", len(tube_idx))\n",
    "    tube_mask[tube_idx] = True  # Trues mean to include the patch, False means to remove the patch\n",
    "    tube_mask = tube_mask.tile(num_frames)  # repeat masking for the other timepoints\n",
    "    print(\"tube mask percent\", tube_mask.sum().item() / len(tube_mask))\n",
    "\n",
    "    # create decoder mask similar to tube mask, but ensure no overlap\n",
    "    decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)  # start by masking everything (aka include nothing)\n",
    "    remaining_mask_idx = mask_idx_candidates[int(num_patches / num_frames * (1 - tube_start_masking_ratio)) :]  # brain positive tokens not selected for the encoder tokens\n",
    "    decoder_mask_idx = remaining_mask_idx[:int(num_patches / num_frames * (1 - decoder_mask_ratio))]\n",
    "    print(\"num decoder patches =\", len(decoder_mask_idx))\n",
    "    decoder_mask[decoder_mask_idx] = True\n",
    "    decoder_mask = decoder_mask.tile(num_frames)  # repeat masking for the other timepoints\n",
    "    print(\"decoder_mask percent\", decoder_mask.sum().item() / len(decoder_mask))\n",
    "\n",
    "    # apply masks to patches_vit\n",
    "    tube_patches_vit = copy.deepcopy(patches_vit.detach())\n",
    "    decoder_patches_vit = copy.deepcopy(patches_vit.detach())\n",
    "    tube_patches_vit[:, ~tube_mask] = 0.0\n",
    "    decoder_patches_vit[:, ~decoder_mask] = 0.0\n",
    "\n",
    "    # undo patchification so we can visualize\n",
    "    tube_unpatches = rearrange(\n",
    "        tube_patches_vit,\n",
    "        \"b (f d h w) c -> b f d h w c\",\n",
    "        d=img_size[0]//patch_size,\n",
    "        h=img_size[1]//patch_size,\n",
    "        w=img_size[2]//patch_size,\n",
    "    )\n",
    "    decoder_unpatches = rearrange(\n",
    "        decoder_patches_vit,\n",
    "        \"b (f d h w) c -> b f d h w c\",\n",
    "        d=img_size[0]//patch_size,\n",
    "        h=img_size[1]//patch_size,\n",
    "        w=img_size[2]//patch_size,\n",
    "    )\n",
    "    print(\"tube_unpatches\", tube_unpatches.shape)\n",
    "    print(\"decoder_unpatches\", decoder_unpatches.shape)\n",
    "    \n",
    "    tube_func = rearrange(\n",
    "        tube_unpatches,\n",
    "        \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "        b=len(func),\n",
    "        f=num_frames,\n",
    "        d=img_size[0] // patch_size,\n",
    "        h=img_size[1] // patch_size,\n",
    "        w=img_size[2] // patch_size,\n",
    "        pd=patch_size,\n",
    "        ph=patch_size,\n",
    "        pw=patch_size,\n",
    "        pf=frame_patch_size,\n",
    "    )\n",
    "    decoder_func = rearrange(\n",
    "        decoder_unpatches,\n",
    "        \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "        b=len(func),\n",
    "        f=num_frames,\n",
    "        d=img_size[0] // patch_size,\n",
    "        h=img_size[1] // patch_size,\n",
    "        w=img_size[2] // patch_size,\n",
    "        pd=patch_size,\n",
    "        ph=patch_size,\n",
    "        pw=patch_size,\n",
    "        pf=frame_patch_size,\n",
    "    )\n",
    "    print(\"tube_func\", tube_func.shape)\n",
    "    print(\"decoder_func\", decoder_func.shape)\n",
    "    \n",
    "    brain_pos_vit = copy.deepcopy(patches_vit.detach())\n",
    "    brain_pos_vit[:,batch_positive_approx.repeat(num_frames)] = 1\n",
    "    brain_pos_vit[:,~batch_positive_approx.repeat(num_frames)] = 0\n",
    "    brain_pos_unpatches = rearrange(\n",
    "        brain_pos_vit,\n",
    "        \"b (f d h w) c -> b f d h w c\",\n",
    "        d=img_size[0]//patch_size,\n",
    "        h=img_size[1]//patch_size,\n",
    "        w=img_size[2]//patch_size,\n",
    "    )\n",
    "    brain_pos_func = rearrange(\n",
    "        brain_pos_unpatches,\n",
    "        \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "        b=len(func),\n",
    "        f=num_frames,\n",
    "        d=img_size[0] // patch_size,\n",
    "        h=img_size[1] // patch_size,\n",
    "        w=img_size[2] // patch_size,\n",
    "        pd=patch_size,\n",
    "        ph=patch_size,\n",
    "        pw=patch_size,\n",
    "        pf=frame_patch_size,\n",
    "    )\n",
    "\n",
    "    # Visualize\n",
    "    idx = 0\n",
    "    mean, sd = meansd[idx]\n",
    "    print(\"original func without adding mean/sd references\")\n",
    "    display(transforms.ToPILImage()(utils.reshape_to_2d(func[idx])))\n",
    "    print(\"original func\")\n",
    "    display(transforms.ToPILImage()(utils.reshape_to_2d(func[idx] * mean + sd)))\n",
    "    \n",
    "    print(\"\\nbrain-positive patches\")\n",
    "    display(transforms.ToPILImage()(utils.reshape_to_2d(brain_pos_func[idx])))\n",
    "\n",
    "    print(\"\\ntube func without adding mean/sd references\")\n",
    "    display(transforms.ToPILImage()(utils.reshape_to_2d(tube_func[idx])))\n",
    "    print(\"tube func\")\n",
    "    display(transforms.ToPILImage()(utils.reshape_to_2d(tube_func[idx] * mean + sd)))\n",
    "\n",
    "    print(\"\\ndecoder func without adding mean/sd references\")\n",
    "    display(transforms.ToPILImage()(utils.reshape_to_2d(decoder_func[idx])))\n",
    "    print(\"decoder func\")\n",
    "    display(transforms.ToPILImage()(utils.reshape_to_2d(decoder_func[idx] * mean + sd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604b380-0126-49b2-ab4e-59b1e463d6ad",
   "metadata": {},
   "source": [
    "# Set up optimizer and saving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da73c08-ca61-48ef-9e63-b70db6f07a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "total_steps = num_epochs * num_iterations_per_epoch * num_devices\n",
    "print(\"total_steps\", total_steps)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    ")\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f674d-7cd4-49d1-bd8d-c697d9614f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_ckpt(tag=\"last\"):\n",
    "    ckpt_path = outdir+f'/{tag}'\n",
    "    os.makedirs(ckpt_path,exist_ok=True)\n",
    "    accelerator.save_model(model, ckpt_path, max_shard_size=\"2GB\", safe_serialization=True)\n",
    "    print(f\"\\n---saved {ckpt_path}!---\\n\")\n",
    "        \n",
    "def save_progress(tag=\"last\"):\n",
    "    if accelerator.is_main_process:\n",
    "        ckpt_path = outdir+f'/{tag}'\n",
    "        torch.save(\n",
    "                {\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"scheduler\": lr_scheduler.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"recon_losses\": recon_losses,\n",
    "                    \"contrastive_losses\": contrastive_losses,\n",
    "                    \"test_losses\": test_losses,\n",
    "                    \"lrs\": lrs,\n",
    "                },\n",
    "                os.path.join(ckpt_path, f\"params.pt\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b4262-435b-4872-ab4a-424cb9dfd37a",
   "metadata": {},
   "source": [
    "# Start wandb (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56431733-4fb5-4072-840e-22536608f9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if accelerator.is_main_process and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"depth\": depth,\n",
    "      \"heads\": heads,\n",
    "      \"dim\": dim,\n",
    "      \"mlp_dim\": mlp_dim,\n",
    "      \"tube_start_masking_ratio\": tube_start_masking_ratio,\n",
    "      \"tube_end_masking_ratio\": tube_end_masking_ratio,\n",
    "      \"decoder_mask_ratio\": decoder_mask_ratio,\n",
    "      \"num_frames\": num_frames,\n",
    "      \"patch_size\": patch_size,\n",
    "      \"frame_patch_size\": frame_patch_size,\n",
    "      \"use_contrastive_loss\": use_contrastive_loss,\n",
    "      \"use_cls_token\": use_cls_token,\n",
    "      \"constrastive_loss_weight\": constrastive_loss_weight,\n",
    "      \"num_params\": num_params,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_urls\": train_urls,\n",
    "      \"test_urls\": test_urls,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a5055-8afd-468a-93bf-32f94bd1d042",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e394dd-2745-41fa-a5aa-54b7b1373a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "lrs, recon_losses, contrastive_losses, test_losses = [], [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a5605-fd81-41d4-964e-1752ed6e1289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# resume from ckpt (e.g., if you are resuming from a run that got pre-empted)\n",
    "load_progress = False\n",
    "if wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "        load_progress = True\n",
    "elif resume_from_ckpt: # if resuming without using wandb\n",
    "    load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "    load_progress = True\n",
    "    \n",
    "if load_progress:\n",
    "    ckpt_path = outdir+'/last'\n",
    "    prev_params = torch.load(ckpt_path+\"/params.pt\")\n",
    "    optimizer.load_state_dict(prev_params[\"optimizer\"])\n",
    "    lr_scheduler.load_state_dict(prev_params[\"scheduler\"])\n",
    "    epoch = prev_params[\"epoch\"]\n",
    "    recon_losses = prev_params[\"recon_losses\"]\n",
    "    contrastive_losses = prev_params[\"contrastive_losses\"]\n",
    "    test_losses = prev_params[\"test_losses\"]\n",
    "    lrs = prev_params[\"lrs\"]\n",
    "    print(\"Loaded model params from\", ckpt_path, \"at epoch\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab64a75a-3126-4fa7-a53e-d69160758bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dl, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dl, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e258c4-8477-48ed-81af-d415cee246f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "if use_contrastive_loss:\n",
    "    logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))  # learned logit scale\n",
    "\n",
    "progress_bar = tqdm(range(epoch, num_epochs), disable=not accelerator.is_main_process, desc=\"Overall\")\n",
    "for epoch in progress_bar:\n",
    "    # get the masking ratio for the current epoch\n",
    "    tube_mask_ratio = utils.get_masking_ratio(\n",
    "        current_epoch=epoch, \n",
    "        total_epochs=num_epochs, \n",
    "        start_masking_ratio=tube_start_masking_ratio, \n",
    "        end_masking_ratio=tube_end_masking_ratio\n",
    "    )\n",
    "    with torch.cuda.amp.autocast(dtype=data_type):\n",
    "        model.train()\n",
    "        for train_i, batch in enumerate(tqdm(train_dl, disable=not accelerator.is_main_process, \n",
    "                 total=num_iterations_per_epoch, leave=False, desc=\"Training\")):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            func, meansd, brain_pos_pats = batch\n",
    "            if use_contrastive_loss:  # create positive pairs by duplicating the batch\n",
    "                func = torch.cat([func, func], dim=0)\n",
    "                meansd = torch.cat([meansd, meansd], dim=0)\n",
    "                brain_pos_pats = torch.cat([brain_pos_pats, brain_pos_pats], dim=0)\n",
    "\n",
    "            func = func.unsqueeze(1).to(device)\n",
    "\n",
    "            # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "            tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            batch_positive_approx = (brain_pos_pats[:, :num_patches // num_frames].float().mean(dim=0) > 0)\n",
    "            mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "            # check if there's not enough brain left for code to continue\n",
    "            if len(mask_idx_candidates) < (int(num_patches/num_frames*(1-tube_mask_ratio))+int(num_patches/num_frames*(1-decoder_mask_ratio))):\n",
    "                print(\"Brain volume skipped due to not enough brain-positive patches remaining...\")\n",
    "                continue\n",
    "            mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "            tube_idx = mask_idx_candidates[:int(num_patches / num_frames * (1 - tube_mask_ratio))]\n",
    "            tube_mask[tube_idx] = True\n",
    "            tube_mask = tube_mask.tile(num_frames)\n",
    "\n",
    "            # create decoder mask\n",
    "            decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            remaining_mask_idx = mask_idx_candidates[int(num_patches / num_frames * (1 - tube_mask_ratio)):]\n",
    "            decoder_mask_idx = remaining_mask_idx[:int(num_patches / num_frames * (1 - decoder_mask_ratio))]\n",
    "            decoder_mask[decoder_mask_idx] = True\n",
    "            decoder_mask = decoder_mask.tile(num_frames)\n",
    "\n",
    "            # encode the tube patches\n",
    "            encoder_out = model(func, encoder_mask=tube_mask)\n",
    "            if use_cls_token:\n",
    "                enc_cls_token = encoder_out[:,:1,:]\n",
    "\n",
    "            # decode both the encoder_out patches and masked decoder patches\n",
    "            decoder_out = model(encoder_out, encoder_mask=tube_mask, decoder_mask=decoder_mask)\n",
    "            # subset only the reconstructed decoder patches\n",
    "            output = decoder_out[:, -num_decoder_patches:]\n",
    "\n",
    "            # compare to ground truth and calculate loss\n",
    "            target_patches = model.patchify(func)\n",
    "            target_patches_vit = rearrange(target_patches, \"b ... d -> b (...) d\")\n",
    "            target = target_patches_vit[:, decoder_mask]\n",
    "            loss = mse(output, target)\n",
    "\n",
    "            # contrastive loss\n",
    "            if use_contrastive_loss:\n",
    "                n_b = len(func) // 2\n",
    "                cls_token1 = enc_cls_token[:n_b, 0, :]  # first half of batch, cls_token shape B, 1, d_model\n",
    "                cls_token2 = enc_cls_token[n_b:, 0, :]\n",
    "                contrastive_loss = utils.contrastive_loss(cls_token1, cls_token2, temperature=logit_scale)\n",
    "                loss += constrastive_loss_weight * contrastive_loss\n",
    "                contrastive_losses.append(contrastive_loss.item())\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            recon_losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        model.eval()\n",
    "        for test_i, batch in enumerate(tqdm(test_dl, disable=not accelerator.is_main_process, total=test_num_iterations_per_epoch, leave=False, desc=\"Testing\")):\n",
    "            func, meansd, brain_pos_pats = batch\n",
    "            func = func.unsqueeze(1).to(device)\n",
    "\n",
    "            # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "            tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            batch_positive_approx = brain_pos_pats[:, :num_patches // num_frames].float().mean(dim=0) > 0\n",
    "            mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "            # check if there's not enough brain left for code to continue\n",
    "            if len(mask_idx_candidates) < (int(num_patches/num_frames*(1-tube_mask_ratio))+int(num_patches/num_frames*(1-decoder_mask_ratio))):\n",
    "                if test_i==0:\n",
    "                    print(\"Brain volume skipped due to not enough brain-positive patches remaining...\")\n",
    "                continue\n",
    "            mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "            tube_idx = mask_idx_candidates[:int(num_patches / num_frames * (1 - tube_mask_ratio))]\n",
    "            tube_mask[tube_idx] = True\n",
    "            tube_mask = tube_mask.tile(num_frames)\n",
    "\n",
    "            # create decoder mask\n",
    "            decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            remaining_mask_idx = mask_idx_candidates[int(num_patches / num_frames * (1 - tube_mask_ratio)):]\n",
    "            decoder_mask_idx = remaining_mask_idx[:int(num_patches / num_frames * (1 - decoder_mask_ratio))]\n",
    "            decoder_mask[decoder_mask_idx] = True\n",
    "            decoder_mask = decoder_mask.tile(num_frames)\n",
    "\n",
    "            # encode the tube patches\n",
    "            encoder_out = model(func, encoder_mask=tube_mask)\n",
    "            # decode both the encoder_out patches and masked decoder patches\n",
    "            decoder_out = model(encoder_out, encoder_mask=tube_mask, decoder_mask=decoder_mask)\n",
    "            # subset only the reconstructed decoder patches\n",
    "            output = decoder_out[:, -num_decoder_patches:]\n",
    "\n",
    "            # compare to ground truth and calculate loss\n",
    "            target_patches = model.patchify(func)\n",
    "            target_patches_vit = rearrange(target_patches, \"b ... d -> b (...) d\")\n",
    "            target = target_patches_vit[:, decoder_mask]\n",
    "            loss = mse(output, target)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(recon_losses[-(train_i + 1) :]),\n",
    "            \"test/loss\": np.mean(test_losses[-(test_i + 1) :]),\n",
    "            \"train/num_steps\": len(recon_losses),\n",
    "            \"test/num_steps\": len(test_losses),\n",
    "            \"lr\": np.mean(lrs[-(train_i + 1) :]),\n",
    "            \"epoch\": epoch,\n",
    "            \"tube_mask_ratio\": tube_mask_ratio,\n",
    "            \"decoder_mask_ratio\": decoder_mask_ratio,\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        if wandb_log: wandb.log(logs)\n",
    "\n",
    "        # Plot progress (first sample in batch)\n",
    "        with torch.no_grad():\n",
    "            if utils.is_interactive():\n",
    "                # prep reference volumes for going back to original data\n",
    "                idx = 0\n",
    "                mean, sd = meansd[idx]\n",
    "                mean, sd = mean.to(device), sd.to(device)\n",
    "                if epoch == 0:\n",
    "                    print(\"original volumes without adding mean/sd references\")\n",
    "                    display(transforms.ToPILImage()(utils.reshape_to_2d(func[idx]) * 5))  # scaling by 5 for visualization contrast\n",
    "                    print(\"original volumes\")\n",
    "                    display(transforms.ToPILImage()(utils.reshape_to_2d(func[idx] * mean + sd)))\n",
    "                if epoch % 5 == 0:\n",
    "                    # undo patchification so we can visualize\n",
    "                    decode_vis = torch.zeros_like(target_patches_vit)\n",
    "                    decode_vis[:, decoder_mask] = output\n",
    "                    decoder_unpatches = rearrange(\n",
    "                        decode_vis,\n",
    "                        \"b (f d h w) c -> b f d h w c\",\n",
    "                        d=img_size[0]//patch_size,\n",
    "                        h=img_size[1]//patch_size,\n",
    "                        w=img_size[2]//patch_size,\n",
    "                    )\n",
    "                    decoder_func = rearrange(\n",
    "                        decoder_unpatches,\n",
    "                        \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "                        b=batch_size,\n",
    "                        f=num_frames,\n",
    "                        d=img_size[0]//patch_size,\n",
    "                        h=img_size[1]//patch_size,\n",
    "                        w=img_size[2]//patch_size,\n",
    "                        pd=patch_size,\n",
    "                        ph=patch_size,\n",
    "                        pw=patch_size,\n",
    "                        pf=frame_patch_size,\n",
    "                    )\n",
    "                    print(\"recons of decoded patches without adding mean/sd references\")\n",
    "                    display(transforms.ToPILImage()(utils.reshape_to_2d(decoder_func[idx] * 5)))  # scaling by 5 for visualization contrast\n",
    "                    print(\"recons of decoded patches\")\n",
    "                    display(transforms.ToPILImage()(utils.reshape_to_2d(decoder_func[idx] * mean + sd)))\n",
    "                \n",
    "        # Save model checkpoint\n",
    "        if (ckpt_saving) and ((epoch % ckpt_interval == 0) or (epoch==num_epochs-1)):\n",
    "            save_ckpt()\n",
    "            save_progress()\n",
    "            \n",
    "        # wait for other GPUs to catch up if needed\n",
    "        accelerator.wait_for_everyone()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "# remove cache directories\n",
    "os.system('rm -fr \"%s\"' % f\"{cache_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6e847-2511-4f69-aa70-f1471a5b7d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(recon_losses)\n",
    "plt.title(\"Training re-construction losses\")\n",
    "plt.show()\n",
    "if use_contrastive_loss:\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(contrastive_losses)\n",
    "    plt.title(\"Training contrastive losses\")\n",
    "    plt.show()\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(test_losses)\n",
    "plt.title(\"Test losses\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
