{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir /weka/proj-fmri/paulscotti/fMRI-foundation-model/flat/checkpoints/ps16_replicate\n",
      "Loaded config.yaml from ckpt folder /weka/proj-fmri/paulscotti/fMRI-foundation-model/flat/checkpoints/ps16_replicate\n",
      "\n",
      "__CONFIG__\n",
      "base_lr = 0.001\n",
      "batch_size = 32\n",
      "ckpt_interval = 5\n",
      "ckpt_saving = True\n",
      "cls_embed = True\n",
      "decoder_embed_dim = 512\n",
      "grad_accumulation_steps = 1\n",
      "grad_clip = 1.0\n",
      "hcp_flat_path = /weka/proj-medarc/shared/hcp_flat\n",
      "mask_ratio = 0.75\n",
      "model_name = ps16_replicate\n",
      "no_qkv_bias = False\n",
      "norm_pix_loss = False\n",
      "num_epochs = 100\n",
      "num_frames = 16\n",
      "num_samples_per_epoch = 200000\n",
      "num_workers = 10\n",
      "patch_size = 16\n",
      "pred_t_dim = 8\n",
      "print_interval = 20\n",
      "probe_base_lr = 0.0003\n",
      "probe_batch_size = 8\n",
      "probe_num_epochs = 30\n",
      "probe_num_samples_per_epoch = 100000\n",
      "resume_from_ckpt = True\n",
      "seed = 42\n",
      "sep_pos_embed = True\n",
      "t_patch_size = 2\n",
      "test_num_samples_per_epoch = 50000\n",
      "test_set = False\n",
      "trunc_init = False\n",
      "use_contrastive_loss = False\n",
      "wandb_log = True\n",
      "\n",
      "\n",
      "Number of available CUDA devices: 1\n",
      "LOCAL RANK=0\n",
      "NUM GPUS=1\n",
      "NODE=0\n",
      "GLOBAL RANK=0\n",
      "WORLD_SIZE=1\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "PID of this process = 9545\n",
      "device = cuda distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from flat_models import *\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# following fixes a Conv3D CUDNN_NOT_SUPPORTED error\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "## MODEL TO LOAD ##\n",
    "model_name = \"ps16_large\"\n",
    "outdir = os.path.abspath(f'checkpoints/{model_name}')\n",
    "print(\"outdir\", outdir)\n",
    "# Load previous config.yaml if available\n",
    "if os.path.exists(f\"{outdir}/config.yaml\"):\n",
    "    config = yaml.load(open(f\"{outdir}/config.yaml\", 'r'), Loader=yaml.FullLoader)\n",
    "    print(f\"Loaded config.yaml from ckpt folder {outdir}\")\n",
    "    # create global variables from the config\n",
    "    print(\"\\n__CONFIG__\")\n",
    "    for attribute_name in config.keys():\n",
    "        print(f\"{attribute_name} = {config[attribute_name]}\")\n",
    "        globals()[attribute_name] = config[f'{attribute_name}']\n",
    "    print(\"\\n\")\n",
    "\n",
    "### Multi-GPU config ###\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"Number of available CUDA devices: {device_count}\")\n",
    "\n",
    "local_rank = os.getenv('LOCAL_RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(f\"LOCAL RANK={local_rank}\")\n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(f\"NUM GPUS={num_devices}\")\n",
    "distributed = True if num_devices>1 else False\n",
    "if distributed: assert device_count==num_devices\n",
    "\n",
    "node = os.getenv('SLURM_NODEID')\n",
    "if node is None:\n",
    "    node = 0\n",
    "else:\n",
    "    node = int(node)\n",
    "print(f\"NODE={node}\")\n",
    "\n",
    "global_rank = os.getenv('RANK')\n",
    "if global_rank is None:\n",
    "    global_rank = 0\n",
    "else:\n",
    "    global_rank = int(global_rank)\n",
    "print(f\"GLOBAL RANK={global_rank}\")\n",
    "\n",
    "world_size = os.getenv('WORLD_SIZE')\n",
    "if world_size is None: \n",
    "    world_size = 1\n",
    "else:\n",
    "    world_size = int(world_size)\n",
    "print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "# base_lr = probe_base_lr\n",
    "batch_size = probe_batch_size\n",
    "num_epochs = probe_num_epochs\n",
    "data_type = torch.float32 # change depending on your mixed_precision\n",
    "global_batch_size = batch_size * world_size\n",
    "\n",
    "base_lr = 3e-4\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "\n",
    "# FSDP Setup\n",
    "if distributed:\n",
    "    import torch.distributed as dist\n",
    "    import torch.multiprocessing as mp\n",
    "    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "    from torch.distributed.fsdp.api import BackwardPrefetch, CPUOffload, ShardingStrategy\n",
    "    import functools\n",
    "    from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy, transformer_auto_wrap_policy\n",
    "    print(f\"setting device to cuda:{local_rank}\")\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device('cuda',local_rank)\n",
    "    dist.init_process_group(\"nccl\", rank=global_rank, world_size=world_size)\n",
    "    print(f\"\\nSuccessfully set cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "    dist.barrier()\n",
    "    print(f\"global_rank{global_rank} passed barrier\")\n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "print(\"device =\", device, \"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed + global_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15aca0-148e-435f-b8f2-7a708b61a6d9",
   "metadata": {},
   "source": [
    "# hcp_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de1a5b87-fa69-44e1-bdb9-bd9e257485c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_size (144, 320) patch_size (16, 16) frames 16 t_patch_size 2\n",
      "model initialized\n",
      "\n",
      "Loaded checkpoint epoch99.pth from /weka/proj-fmri/paulscotti/fMRI-foundation-model/flat/checkpoints/ps16_replicate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from util.hcp_flat import load_hcp_flat_mask\n",
    "from util.hcp_flat import create_hcp_flat\n",
    "import util.visualize as vis\n",
    "\n",
    "model = mae_vit_small_fmri(\n",
    "    patch_size=patch_size,\n",
    "    decoder_embed_dim=decoder_embed_dim,\n",
    "    t_patch_size=t_patch_size,\n",
    "    pred_t_dim=pred_t_dim,\n",
    "    decoder_depth=4,\n",
    "    cls_embed=cls_embed,\n",
    "    norm_pix_loss=norm_pix_loss,\n",
    "    no_qkv_bias=no_qkv_bias,\n",
    "    sep_pos_embed=sep_pos_embed,\n",
    "    trunc_init=trunc_init,\n",
    ")\n",
    "\n",
    "# Load ckpt\n",
    "if not os.path.exists(outdir) or not os.path.isdir(outdir):\n",
    "    print(f\"\\nCheckpoint folder {outdir} does not exist.\\n\")\n",
    "else:\n",
    "    checkpoint_files = [f for f in os.listdir(outdir) if f.endswith('.pth')]\n",
    "\n",
    "    # # Find the latest ckpt to load\n",
    "    epoch_numbers = []\n",
    "    for file in checkpoint_files:\n",
    "        try:\n",
    "            epoch_number = int(file.split('epoch')[-1].split('.')[0])\n",
    "            epoch_numbers.append(epoch_number)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    latest_epoch = max(epoch_numbers)\n",
    "    checkpoint_name = f\"epoch{latest_epoch}.pth\"\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint_path = os.path.join(outdir, checkpoint_name)\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state[\"model_state_dict\"], strict=True)\n",
    "\n",
    "    print(f\"\\nLoaded checkpoint {checkpoint_name} from {outdir}\\n\")\n",
    "\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd51ddf-fb71-48f4-bdbd-88753b44d2aa",
   "metadata": {},
   "source": [
    "## Create dataset and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "136417d0-9c1a-4574-94ac-305c4cf1e755",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_folder epoch99\n",
      "Target: trial_type\n",
      "train: (118336, 9), test: (12608, 9)\n",
      "X_train: (118336, 384), X_test: (12608, 384)\n",
      "classes (21): ['0bk_body' '0bk_faces' '0bk_places' '0bk_tools' '2bk_body' '2bk_faces'\n",
      " '2bk_places' '2bk_tools' 'fear' 'lf' 'lh' 'match' 'math' 'mental' 'neut'\n",
      " 'relation' 'rf' 'rh' 'rnd' 'story' 't']\n",
      "\n",
      "y_train: (118336,) [14  8 14  8 14 13 13 13 13 18 18 13 13 18 18 15 11 11 15 11]\n",
      "y_test: (12608,) [ 7  7  0  0  5  5  3  3  4  4  6  6  1  1  2  2 19 19 12 12]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "parquet_folder = f\"epoch{latest_epoch}\"\n",
    "print(\"parquet_folder\", parquet_folder)\n",
    "\n",
    "target = \"trial_type\"\n",
    "print(f\"Target: {target}\")\n",
    "\n",
    "train_features = pd.read_parquet(f\"{outdir}/{parquet_folder}/train.parquet\")\n",
    "test_features = pd.read_parquet(f\"{outdir}/{parquet_folder}/test.parquet\")\n",
    "print(f\"train: {train_features.shape}, test: {test_features.shape}\")\n",
    "\n",
    "X_train = np.stack(train_features[\"feature\"])\n",
    "X_test = np.stack(test_features[\"feature\"])\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "if target == \"task\":\n",
    "    labels_train = train_features[\"task\"].str.rstrip(\"1234\").values\n",
    "    labels_test = test_features[\"task\"].str.rstrip(\"1234\").values\n",
    "elif target == \"trial_type\":\n",
    "    labels_train = train_features[\"trial_type\"].values\n",
    "    labels_test = test_features[\"trial_type\"].values\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "y_train = label_enc.fit_transform(labels_train)\n",
    "y_test = label_enc.transform(labels_test)\n",
    "\n",
    "print(f\"classes ({len(label_enc.classes_)}): {label_enc.classes_}\")\n",
    "print(\n",
    "    f\"\\ny_train: {y_train.shape} {y_train[:20]}\\n\"\n",
    "    f\"y_test: {y_test.shape} {y_test[:20]}\"\n",
    ")\n",
    "del train_features, test_features\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8ef5a85-8ee3-4714-943f-19f90eb12dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probe_num_batches 925\n",
      "test_num_batches 99\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "probe_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "probe_num_batches = len(probe_dl)\n",
    "test_num_batches = len(test_dl)\n",
    "\n",
    "print(\"probe_num_batches\", probe_num_batches)\n",
    "print(\"test_num_batches\", test_num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd64ef-ef28-4dfd-b6fe-d4f55f207477",
   "metadata": {},
   "source": [
    "# Downstream probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb3df855-81c4-4d2c-aff2-42a19dd0e67e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input_dim, h=256, num_classes=8):\n",
    "        super(LinearProbe, self).__init__()\n",
    "        # self.classifier = nn.Linear(input_dim, num_classes)\n",
    "        \n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.LayerNorm(input_dim),\n",
    "        #     nn.Linear(input_dim, num_classes)\n",
    "        # )\n",
    "        \n",
    "        # self.linear = nn.Linear(input_dim, h)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(input_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.35),\n",
    "            nn.Linear(input_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            nn.Linear(h, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff4a0be4-26f3-4c6f-906d-8dec6a2e6984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linear_probe = LinearProbe(384,#model.embed_dim,#457344\n",
    "                 h=5024,\n",
    "                 num_classes=len(np.unique(y_train))).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e6baa-4b1c-4f38-b078-70b2b092d14d",
   "metadata": {},
   "source": [
    "# Set up optimizer and saving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4da73c08-ca61-48ef-9e63-b70db6f07a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply base lr 0.0003 by effective batch size 8\n",
      "lr = 9.375e-06\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "52,561,877 total\n",
      "52,561,877 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "probe_opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in linear_probe.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.05},\n",
    "    {'params': [p for n, p in linear_probe.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "lr = base_lr * global_batch_size / 256\n",
    "print(f\"multiply base lr {base_lr} by effective batch size {global_batch_size}\")\n",
    "print(f\"lr = {lr}\")\n",
    "\n",
    "probe_optimizer = torch.optim.AdamW(probe_opt_grouped_parameters, lr=lr, betas=(0.9, 0.95))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, warmup_epochs=5, min_lr=0.0):\n",
    "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        lr_ = lr * epoch / warmup_epochs\n",
    "    else:\n",
    "        lr_ = min_lr + (lr - min_lr) * 0.5 * (\n",
    "            1.0\n",
    "            + math.cos(\n",
    "                math.pi\n",
    "                * (epoch - warmup_epochs)\n",
    "                / (num_epochs - warmup_epochs)\n",
    "            )\n",
    "        )\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr_\n",
    "    return lr_\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(linear_probe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a5055-8afd-468a-93bf-32f94bd1d042",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5e394dd-2745-41fa-a5aa-54b7b1373a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "lrs, probe_losses, probe_accs, test_probe_losses, test_probe_accs = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3461199-e805-4e9c-8c91-894e83cf8bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:   2%|▋                                  | 1/50 [00:04<03:38,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 2.480 | probe_acc 0.243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:   4%|█▍                                 | 2/50 [00:08<03:16,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.865 | probe_acc 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:   6%|██                                 | 3/50 [00:12<03:06,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.631 | probe_acc 0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:   8%|██▊                                | 4/50 [00:15<03:00,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.540 | probe_acc 0.820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  10%|███▌                               | 5/50 [00:19<02:54,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.442 | probe_acc 0.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  12%|████▏                              | 6/50 [00:23<02:50,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.527 | probe_acc 0.826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  14%|████▉                              | 7/50 [00:27<02:45,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.390 | probe_acc 0.871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  16%|█████▌                             | 8/50 [00:31<02:41,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.362 | probe_acc 0.878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  18%|██████▎                            | 9/50 [00:35<02:37,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.343 | probe_acc 0.885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  20%|██████▊                           | 10/50 [00:38<02:33,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.341 | probe_acc 0.884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  22%|███████▍                          | 11/50 [00:42<02:29,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.356 | probe_acc 0.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  24%|████████▏                         | 12/50 [00:46<02:25,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.322 | probe_acc 0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  26%|████████▊                         | 13/50 [00:50<02:21,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.318 | probe_acc 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  28%|█████████▌                        | 14/50 [00:54<02:17,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.335 | probe_acc 0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  30%|██████████▏                       | 15/50 [00:58<02:13,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.316 | probe_acc 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  32%|██████████▉                       | 16/50 [01:01<02:10,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.309 | probe_acc 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  34%|███████████▌                      | 17/50 [01:05<02:06,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.323 | probe_acc 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  36%|████████████▏                     | 18/50 [01:09<02:02,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.303 | probe_acc 0.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  38%|████████████▉                     | 19/50 [01:13<01:58,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.308 | probe_acc 0.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  40%|█████████████▌                    | 20/50 [01:17<01:54,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.295 | probe_acc 0.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  42%|██████████████▎                   | 21/50 [01:21<01:52,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.300 | probe_acc 0.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  44%|██████████████▉                   | 22/50 [01:25<01:48,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.293 | probe_acc 0.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  46%|███████████████▋                  | 23/50 [01:29<01:45,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.305 | probe_acc 0.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  48%|████████████████▎                 | 24/50 [01:32<01:41,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.299 | probe_acc 0.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  50%|█████████████████                 | 25/50 [01:36<01:37,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.291 | probe_acc 0.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  52%|█████████████████▋                | 26/50 [01:40<01:33,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.297 | probe_acc 0.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  54%|██████████████████▎               | 27/50 [01:44<01:28,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.296 | probe_acc 0.909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  56%|███████████████████               | 28/50 [01:48<01:24,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.296 | probe_acc 0.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  58%|███████████████████▋              | 29/50 [01:52<01:20,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.295 | probe_acc 0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  60%|████████████████████▍             | 30/50 [01:56<01:16,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.298 | probe_acc 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  62%|█████████████████████             | 31/50 [01:59<01:12,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.301 | probe_acc 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  64%|█████████████████████▊            | 32/50 [02:03<01:09,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.301 | probe_acc 0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  66%|██████████████████████▍           | 33/50 [02:07<01:05,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.310 | probe_acc 0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  68%|███████████████████████           | 34/50 [02:11<01:01,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.309 | probe_acc 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  70%|███████████████████████▊          | 35/50 [02:15<00:57,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.317 | probe_acc 0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  72%|████████████████████████▍         | 36/50 [02:18<00:53,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.314 | probe_acc 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  74%|█████████████████████████▏        | 37/50 [02:22<00:49,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.315 | probe_acc 0.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  76%|█████████████████████████▊        | 38/50 [02:26<00:46,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.318 | probe_acc 0.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  78%|██████████████████████████▌       | 39/50 [02:30<00:42,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.320 | probe_acc 0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  80%|███████████████████████████▏      | 40/50 [02:34<00:38,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.322 | probe_acc 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  82%|███████████████████████████▉      | 41/50 [02:38<00:34,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.324 | probe_acc 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  84%|████████████████████████████▌     | 42/50 [02:42<00:30,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.323 | probe_acc 0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  86%|█████████████████████████████▏    | 43/50 [02:45<00:26,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.327 | probe_acc 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  88%|█████████████████████████████▉    | 44/50 [02:49<00:22,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.326 | probe_acc 0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  90%|██████████████████████████████▌   | 45/50 [02:53<00:19,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.327 | probe_acc 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  92%|███████████████████████████████▎  | 46/50 [02:57<00:15,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.327 | probe_acc 0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  94%|███████████████████████████████▉  | 47/50 [03:01<00:11,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.327 | probe_acc 0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  96%|████████████████████████████████▋ | 48/50 [03:04<00:07,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.327 | probe_acc 0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:  98%|█████████████████████████████████▎| 49/50 [03:08<00:03,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.328 | probe_acc 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall: 100%|██████████████████████████████████| 50/50 [03:12<00:00,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps16_replicate epoch99.pth | Test | iter 98 | probe_loss 0.328 | probe_acc 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "crossentropy = nn.CrossEntropyLoss()\n",
    "progress_bar = tqdm(range(epoch, num_epochs), disable=local_rank!=0, desc=\"Overall\")\n",
    "linear_probe.train()\n",
    "for epoch in progress_bar:\n",
    "    with torch.cuda.amp.autocast(dtype=data_type):  \n",
    "        for probe_i, batch in enumerate(probe_dl):\n",
    "            probe_optimizer.zero_grad()\n",
    "            adjust_learning_rate(probe_optimizer, probe_i / probe_num_batches + epoch)\n",
    "\n",
    "            latents = batch[0].to(device, non_blocking=True)\n",
    "            label = batch[1].to(device).long()\n",
    "            \n",
    "            task_pred = linear_probe(latents)\n",
    "            probe_loss = crossentropy(task_pred, label)\n",
    "            probe_acc = (torch.max(task_pred,1).indices == label).sum() / len(label)\n",
    "            \n",
    "            probe_loss.backward()\n",
    "            probe_optimizer.step()\n",
    "\n",
    "            probe_losses.append(probe_loss.item())\n",
    "            probe_accs.append(probe_acc.item())\n",
    "            \n",
    "            # if probe_i%print_interval==0 and probe_i>0:\n",
    "            #     print(f\"Ep. {epoch} | probe_loss {np.mean(probe_losses[-print_interval:]):.3f} | probe_acc {np.mean(probe_accs[-print_interval:]):.3f} | lr {probe_optimizer.param_groups[0]['lr']} | {probe_i}/{probe_num_batches}\")\n",
    "\n",
    "        # print(f\"Ep. {epoch} | iter {probe_i} | probe_loss {np.mean(probe_losses[-probe_i:]):.3f} | probe_acc {np.mean(probe_accs[-probe_i:]):.3f} | lr {probe_optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        logs = {\"train/probe_loss\": np.mean(probe_losses[-probe_i:]),\n",
    "                \"train/probe_acc\": np.mean(probe_accs[-probe_i:])}\n",
    "    \n",
    "    # Evaluate performance on held-out test dataset\n",
    "    linear_probe.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_i, batch in enumerate(test_dl):\n",
    "            latents = batch[0].to(device, non_blocking=True)\n",
    "            label = batch[1].to(device).long()\n",
    "\n",
    "            task_pred = linear_probe(latents)\n",
    "            probe_loss = crossentropy(task_pred, label)\n",
    "            probe_acc = (torch.max(task_pred,1).indices == label).sum() / len(label)\n",
    "\n",
    "            test_probe_losses.append(probe_loss.item())\n",
    "            test_probe_accs.append(probe_acc.item())\n",
    "\n",
    "            # if test_i%print_interval==0 and test_i>0:\n",
    "            #     print(f\"Test | probe_loss {np.mean(test_probe_losses[-print_interval:]):.3f} | probe_acc {np.mean(test_probe_accs[-print_interval:]):.3f} | {test_i}\")\n",
    "\n",
    "    print(f\"{model_name} {checkpoint_name} | Test | iter {test_i} | probe_loss {np.mean(test_probe_losses[-test_i:]):.3f} | probe_acc {np.mean(test_probe_accs[-test_i:]):.3f}\")\n",
    "    logs = {\"test/probe_loss\": np.mean(test_probe_losses[-test_i:]),\n",
    "            \"test/probe_acc\": np.mean(test_probe_accs[-test_i:])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31db575a-8127-4d45-9ef5-73e53b1e0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP ##\n",
    "# clane epoch99.pth | Test | iter 100 | probe_loss 0.233 | probe_acc 0.923\n",
    "# ps16 (using connor codebase) epoch99.pth | Test | iter 98 | probe_loss 0.410 | probe_acc 0.918\n",
    "# ps16_replicate (using paul codebase) epoch99.pth | Test | iter 98 | probe_loss 0.324 | probe_acc 0.914\n",
    "# ps8_30pct epoch99.pth | Test | iter 100 | probe_loss 0.442 | probe_acc 0.918\n",
    "# ps16_large epoch99.pth | Test | iter 98 | probe_loss 0.188 | probe_acc 0.969\n",
    "# ps8_30pct_512dec epoch99.pth | Test | iter 99 | probe_loss 0.290 | probe_acc 0.952\n",
    "\n",
    "# ps16_mask9_3losses_bs32 epoch49.pth | Test | iter 98 | probe_loss 0.315 | probe_acc 0.941\n",
    "# ps16_mask9_3losses_bs32 epoch65.pth | Test | iter 98 | probe_loss 0.335 | probe_acc 0.942\n",
    "# ps16_mask9_3losses_bs32 epoch99.pth | Test | iter 98 | probe_loss 0.304 | probe_acc 0.949\n",
    "# ps8_mask75_bs32_l epoch30.pth | Test | iter 100 | probe_loss 0.997 | probe_acc 0.695\n",
    "# ps16_mask75_bs32_l epoch30.pth | Test | iter 98 | probe_loss 0.592 | probe_acc 0.864\n",
    "# ps16_mask75_bs32_l epoch99.pth | Test | iter 98 | probe_loss 0.393 | probe_acc 0.894\n",
    "\n",
    "## Linear ##\n",
    "# ps16_mask9_3losses_bs32 epoch99.pth | Test | iter 98 | probe_loss 0.477 | probe_acc 0.864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e721ee3-5e9c-4b11-90a6-e38b0e566dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "found",
   "language": "python",
   "name": "found"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
