{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 877125\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import webdataset as wds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from mae_utils.flat_models import *\n",
    "import h5py\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import argparse\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# following fixes a Conv3D CUDNN_NOT_SUPPORTED error\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ## MODEL TO LOAD ##\n",
    "# model_name = \"HCPflat_large_gsrFalse_\"\n",
    "# parquet_folder = \"epoch99\"\n",
    "\n",
    "# # outdir = os.path.abspath(f'checkpoints/{model_name}')\n",
    "# outdir = os.path.abspath(f'checkpoints/{model_name}')\n",
    "\n",
    "# print(\"outdir\", outdir)\n",
    "# # Load previous config.yaml if available\n",
    "# if os.path.exists(f\"{outdir}/config.yaml\"):\n",
    "#     config = yaml.load(open(f\"{outdir}/config.yaml\", 'r'), Loader=yaml.FullLoader)\n",
    "#     print(f\"Loaded config.yaml from ckpt folder {outdir}\")\n",
    "#     # create global variables from the config\n",
    "#     print(\"\\n__CONFIG__\")\n",
    "#     for attribute_name in config.keys():\n",
    "#         print(f\"{attribute_name} = {config[attribute_name]}\")\n",
    "#         globals()[attribute_name] = config[f'{attribute_name}']\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# world_size = os.getenv('WORLD_SIZE')\n",
    "# if world_size is None: \n",
    "#     world_size = 1\n",
    "# else:\n",
    "#     world_size = int(world_size)\n",
    "# print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "# if utils.is_interactive():\n",
    "#     # Following allows you to change functions in models.py or utils.py and \n",
    "#     # have this notebook automatically update with your revisions\n",
    "#     %load_ext autoreload\n",
    "#     %autoreload 2\n",
    "\n",
    "# batch_size = probe_batch_size\n",
    "# num_epochs = probe_num_epochs\n",
    "\n",
    "# data_type = torch.float32 # change depending on your mixed_precision\n",
    "# global_batch_size = batch_size * world_size\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# hcp_flat_path = \"/weka/proj-medarc/shared/HCP-Flat\"\n",
    "# seed = 42\n",
    "num_frames = 16\n",
    "gsr = False\n",
    "# num_workers = 5\n",
    "batch_size = 16\n",
    "# target = 'sex' # This can be 'trial_type' 'age' 'sex'\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2539698-5566-48cf-8022-90cb70d94f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name_suffix: testing\n",
      "--hcp_flat_path=/weka/proj-medarc/shared/HCP-Flat                     --target=subject_id                     --model_suffix=testing                     --batch_size=16                     --max_lr=1e-5 --num_epochs=40 --no-save_ckpt --no-wandb_log --num_workers=10                     --weight_decay=1e-5\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name_suffix = \"testing\"\n",
    "    print(\"model_name_suffix:\", model_name_suffix)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the 2nd cell block\n",
    "    jupyter_args = f\"--hcp_flat_path=/weka/proj-medarc/shared/HCP-Flat \\\n",
    "                    --target=subject_id \\\n",
    "                    --model_suffix={model_name_suffix} \\\n",
    "                    --batch_size={batch_size} \\\n",
    "                    --max_lr=1e-5 --num_epochs=40 --no-save_ckpt --no-wandb_log --num_workers=10 \\\n",
    "                    --weight_decay=1e-5\"\n",
    "    # --multisubject_ckpt=../train_logs/multisubject_subj01_1024_24bs_nolow\n",
    "\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b9ee45-5409-4a03-b1a1-f59af40ccc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ ARGS ------- \n",
      " Namespace(model_suffix='testing', hcp_flat_path='/weka/proj-medarc/shared/HCP-Flat', batch_size=16, wandb_log=False, num_epochs=40, lr_scheduler_type='cycle', save_ckpt=False, seed=42, max_lr=1e-05, target='subject_id', num_workers=10, weight_decay=1e-05)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_suffix\", type=str, default=\"Testing_flat\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hcp_flat_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=128,\n",
    "    help=\"Batch size can be increased by 10x if only training retreival submodule and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=150,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_ckpt\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--target\",type=str,default='trial_type',  # \"trial_type\" or \"subject_id\" or see table on HCP_downstream.ipynb\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_workers\",type=int,default=10,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weight_decay\",type=float,default=1e-5,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "print(f\"------ ARGS ------- \\n {args}\")\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc00235a-a9e1-4dc4-8fd3-359c1d91bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### UNCOMMENT THIS TO SAVE THE HCP-FLAT IN HDF5 FORMAT\n",
    "\n",
    "\n",
    "# from torch.utils.data import default_collate\n",
    "# from mae_utils.flat import load_hcp_flat_mask\n",
    "# from mae_utils.flat import create_hcp_flat\n",
    "# from mae_utils.flat import batch_unmask\n",
    "# import mae_utils.visualize as vis\n",
    "\n",
    "\n",
    "# batch_size = 1\n",
    "# print(f\"changed batch_size to {batch_size}\")\n",
    "# load_file_frames = num_frames * 2\n",
    "# print(f\"Calculating with {load_file_frames} frames, doubling to approximate TR\")\n",
    "\n",
    "# ## Test ##\n",
    "# datasets_to_include = \"HCP\"\n",
    "# assert \"HCP\" in datasets_to_include\n",
    "# test_dataset = create_hcp_flat(root=hcp_flat_path, \n",
    "#                 clip_mode=\"event\", frames=load_file_frames, shuffle=False, gsr=gsr, sub_list = 'test')\n",
    "# test_dl = wds.WebLoader(\n",
    "#     test_dataset.batched(batch_size, partial=False, collation_fn=default_collate),\n",
    "#     batch_size=None,\n",
    "#     shuffle=False,\n",
    "#     num_workers=num_workers,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "\n",
    "# ## Train ##\n",
    "# assert \"HCP\" in datasets_to_include\n",
    "# train_dataset = create_hcp_flat(root=hcp_flat_path, \n",
    "#                 clip_mode=\"event\", frames=load_file_frames, shuffle=False, gsr=gsr, sub_list = 'train')\n",
    "# train_dl = wds.WebLoader(\n",
    "#     train_dataset.batched(batch_size, partial=True, collation_fn=default_collate),\n",
    "#     batch_size=None,\n",
    "#     shuffle=False,\n",
    "#     num_workers=num_workers,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "\n",
    "# def flatten_meta(meta_dict):\n",
    "#     \"\"\"\n",
    "#     Flatten the meta dictionary by:\n",
    "#     - Replacing single-item lists with the item itself.\n",
    "#     - Converting tensors to scalar numbers.\n",
    "#     \"\"\"\n",
    "#     flattened = {}\n",
    "#     for key, value in meta_dict.items():\n",
    "#         if isinstance(value, list):\n",
    "#             if len(value) == 1:\n",
    "#                 flattened[key] = value[0]  # Replace list with its single item\n",
    "#             else:\n",
    "#                 flattened[key] = value  # Keep as is if multiple items\n",
    "#         elif isinstance(value, torch.Tensor):\n",
    "#             # Convert tensor to scalar\n",
    "#             if value.numel() == 1:\n",
    "#                 flattened[key] = value.item()\n",
    "#             else:\n",
    "#                 flattened[key] = value.tolist()  # Convert multi-element tensor to list\n",
    "#         else:\n",
    "#             flattened[key] = value  # Keep the value as is\n",
    "#     return flattened\n",
    "\n",
    "\n",
    "# import h5py\n",
    "# meta_array = np.array([], dtype=object)\n",
    "# # Open an HDF5 file in write mode\n",
    "# with h5py.File(f'test_HCP_raw_flatmaps_{load_file_frames}f.hdf5', 'w') as h5f:\n",
    "#     flatmaps_dset = None\n",
    "    \n",
    "#     total_samples = 0\n",
    "\n",
    "#     for i, batch in tqdm(enumerate(test_dl), total = 12000):\n",
    "#         images = batch[0]\n",
    "#         meta = batch[1]\n",
    "#         batch_size = images.shape[0]\n",
    "#         meta_serializable = meta.copy()\n",
    "        \n",
    "        \n",
    "#         # Step 2: Serialize the dictionary to a JSON string\n",
    "#         meta_str = json.dumps(flatten_meta(meta_serializable), indent=4)\n",
    "#         meta_array = np.append(meta_array, meta_str)\n",
    "#         if flatmaps_dset is None:\n",
    "#             # Initialize datasets with unlimited (None) maxshape along the first axis\n",
    "#             flatmaps_shape = (0,) + images.shape[1:]\n",
    "#             flatmaps_maxshape = (None,) + images.shape[1:]\n",
    "\n",
    "#             flatmaps_dset = h5f.create_dataset(\n",
    "#                 'flatmaps',\n",
    "#                 shape=flatmaps_shape,\n",
    "#                 maxshape=flatmaps_maxshape,\n",
    "#                 dtype=np.float16,\n",
    "#                 chunks=True  # Enable chunking for efficient resizing\n",
    "#             )\n",
    "\n",
    "#         # Resize datasets to accommodate new data\n",
    "#         flatmaps_dset.resize(total_samples + batch_size, axis=0)\n",
    "\n",
    "#         # Write data to the datasets\n",
    "#         flatmaps_dset[total_samples:total_samples + batch_size] = images.numpy().astype(np.float16)\n",
    "\n",
    "#         total_samples += batch_size\n",
    "        \n",
    "#     print(f\"Processed {total_samples} samples\")\n",
    "# np.save(f'metadata_test_HCP_raw_flatmaps_{load_file_frames}f.npy', meta_array)\n",
    "\n",
    "\n",
    "# import h5py\n",
    "# meta_array = np.array([], dtype=object)\n",
    "# # Open an HDF5 file in write mode\n",
    "# with h5py.File(f'train_HCP_raw_flatmaps_{load_file_frames}f.hdf5', 'w') as h5f:\n",
    "#     flatmaps_dset = None\n",
    "    \n",
    "#     total_samples = 0\n",
    "\n",
    "#     for i, batch in tqdm(enumerate(train_dl), total = 120000):\n",
    "#         images = batch[0]\n",
    "#         meta = batch[1]\n",
    "#         batch_size = images.shape[0]\n",
    "#         meta_serializable = meta.copy()\n",
    "        \n",
    "        \n",
    "#         # Step 2: Serialize the dictionary to a JSON string\n",
    "#         meta_str = json.dumps(flatten_meta(meta_serializable), indent=4)\n",
    "#         meta_array = np.append(meta_array, meta_str)\n",
    "#         if flatmaps_dset is None:\n",
    "#             # Initialize datasets with unlimited (None) maxshape along the first axis\n",
    "#             flatmaps_shape = (0,) + images.shape[1:]\n",
    "#             flatmaps_maxshape = (None,) + images.shape[1:]\n",
    "\n",
    "#             flatmaps_dset = h5f.create_dataset(\n",
    "#                 'flatmaps',\n",
    "#                 shape=flatmaps_shape,\n",
    "#                 maxshape=flatmaps_maxshape,\n",
    "#                 dtype=np.float16,\n",
    "#                 chunks=True  # Enable chunking for efficient resizing\n",
    "#             )\n",
    "\n",
    "#         # Resize datasets to accommodate new data\n",
    "#         flatmaps_dset.resize(total_samples + batch_size, axis=0)\n",
    "\n",
    "#         # Write data to the datasets\n",
    "#         flatmaps_dset[total_samples:total_samples + batch_size] = images.numpy().astype(np.float16)\n",
    "\n",
    "#         total_samples += batch_size\n",
    "        \n",
    "#     print(f\"Processed {total_samples} samples\")\n",
    "# np.save(f'metadata_train_HCP_raw_flatmaps_{load_file_frames}f.npy', meta_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98629867-6d64-45ef-823d-5f4fdb279ec7",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e66e2c0-6a73-42d7-8414-3b8acafdb0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded flatmaps\n"
     ]
    }
   ],
   "source": [
    "hdf5_base_path_raw_file = '.'  # use this /weka/proj-fmri/ckadirt/fMRI-foundation-model/src if you don't want to store them again\n",
    "load_file_frames = num_frames * 2\n",
    "\n",
    "try:\n",
    "    f_train = h5py.File(f'{hdf5_base_path_raw_file}/train_HCP_raw_flatmaps_{load_file_frames}f.hdf5', 'r')\n",
    "    flatmaps_train = f_train['flatmaps']\n",
    "    \n",
    "    f_test = h5py.File(f'{hdf5_base_path_raw_file}/test_HCP_raw_flatmaps_{load_file_frames}f.hdf5', 'r')\n",
    "    flatmaps_test = f_test['flatmaps']\n",
    "    \n",
    "    metadata_train = np.load(f'{hdf5_base_path_raw_file}/metadata_train_HCP_raw_flatmaps_{load_file_frames}f.npy', allow_pickle=True)\n",
    "    metadata_test = np.load(f'{hdf5_base_path_raw_file}/metadata_test_HCP_raw_flatmaps_{load_file_frames}f.npy', allow_pickle=True)\n",
    "    print(\"Loaded flatmaps\")\n",
    "except:\n",
    "    print(f\"Make sure you have the raw flatmaps precomputed for this num frames: {load_file_frames}. You can do it uncommenting the cell above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d17bbc-1a58-4b8d-8a72-35677562e069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combined samples: 97246\n",
      "Number of subjects on train with just 1 sample: 0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# creating a new test train data split for target subject_id\n",
    "split_type = 'random' \n",
    "if target == 'subject_id':\n",
    "    if split_type == 'uniform':\n",
    "        print(\"Target is subject_id, so creating a new split on datasets\")\n",
    "        # 1. Combine old metadata\n",
    "        combined_metadata = []\n",
    "        # combined_metadata will hold tuples of (json_string, source, index_in_that_source)\n",
    "    \n",
    "        # Append all train metadata\n",
    "        for i, m_str in enumerate(metadata_train):\n",
    "            combined_metadata.append((m_str, 'train', i))\n",
    "        \n",
    "        train_len = len(metadata_train)\n",
    "        \n",
    "        # Append all test metadata\n",
    "        for i, m_str in enumerate(metadata_test):\n",
    "            combined_metadata.append((m_str, 'test', i))\n",
    "        \n",
    "        # Now we have a big list containing all samples from both old train & old test\n",
    "        print(f\"Total combined samples: {len(combined_metadata)}\")\n",
    "    \n",
    "        subject_to_indices = defaultdict(list)\n",
    "        \n",
    "        for global_idx, (m_str, source, idx_in_source) in enumerate(combined_metadata):\n",
    "            m_dict = json.loads(m_str)\n",
    "            subj = m_dict[\"sub\"]      # e.g. \"285446\"\n",
    "            subject_to_indices[subj].append(global_idx)\n",
    "    \n",
    "        train_ratio = 0.9\n",
    "        new_train_indices = []\n",
    "        new_test_indices = []\n",
    "        \n",
    "        for subj, global_idxs in subject_to_indices.items():\n",
    "            # Shuffle the subject’s indexes in-place so we can do a random split\n",
    "            random.shuffle(global_idxs)\n",
    "            cutoff = max(1, int(len(global_idxs) * train_ratio))        \n",
    "            \n",
    "            subj_train = global_idxs[:cutoff]\n",
    "            subj_test  = global_idxs[cutoff:]\n",
    "            \n",
    "            new_train_indices.extend(subj_train)\n",
    "            new_test_indices.extend(subj_test)\n",
    "\n",
    "    if split_type == 'random':\n",
    "        combined_metadata = []\n",
    "        # Label each sample by whether it came from old train/test\n",
    "        for i, m_str in enumerate(metadata_train):\n",
    "            combined_metadata.append((m_str, 'train', i)) \n",
    "        for i, m_str in enumerate(metadata_test):\n",
    "            combined_metadata.append((m_str, 'test', i)) \n",
    "    \n",
    "        print(f\"Total combined samples: {len(combined_metadata)}\")\n",
    "        p_train = 0.9  # Probability a given sample goes to train\n",
    "    \n",
    "        new_train_indices = []\n",
    "        new_test_indices = []\n",
    "    \n",
    "        # Randomly assign each sample to train or test\n",
    "        for global_idx, (m_str, source, idx_in_source) in enumerate(combined_metadata):\n",
    "            if random.random() < p_train:\n",
    "                new_train_indices.append(global_idx)\n",
    "            else:\n",
    "                new_test_indices.append(global_idx)\n",
    "\n",
    "        # 1) Build a set of subjects already in train\n",
    "        subject_in_train = set()\n",
    "        for g_idx in new_train_indices:\n",
    "            meta_str, source, idx_in_source = combined_metadata[g_idx]\n",
    "            subj = json.loads(meta_str)[\"sub\"]\n",
    "            subject_in_train.add(subj)\n",
    "        \n",
    "        # 2) Group test indices by subject\n",
    "        subject_to_test_indices = defaultdict(list)\n",
    "        for g_idx in new_test_indices:\n",
    "            meta_str, source, idx_in_source = combined_metadata[g_idx]\n",
    "            subj = json.loads(meta_str)[\"sub\"]\n",
    "            subject_to_test_indices[subj].append(g_idx)\n",
    "        \n",
    "        # 3) For any subject not in train, move exactly 1 test sample to train\n",
    "        for subj, test_g_idxs in subject_to_test_indices.items():\n",
    "            alone_samples = 0\n",
    "            if subj not in subject_in_train:\n",
    "                # Move one random test sample for this subject into train\n",
    "                chosen_idx = random.choice(test_g_idxs)\n",
    "                new_test_indices.remove(chosen_idx)\n",
    "                new_train_indices.append(chosen_idx)\n",
    "                subject_in_train.add(subj)\n",
    "                alone_samples = alone_samples + 1\n",
    "        print(f\"Number of subjects on train with just 1 sample: {alone_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368c4fa-f2f3-4997-8ea4-ae6542efc32c",
   "metadata": {},
   "source": [
    "### Create the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b52b668f-c143-40fe-b3d4-c21c7a406d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HCPSplitDataset(Dataset):\n",
    "    def __init__(self, combined_metadata, indices, flatmaps_train, flatmaps_test):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            combined_metadata: list of (json_string, source, idx_in_source)\n",
    "            indices: the list of global indices that define this split\n",
    "            flatmaps_train: h5py dataset for train\n",
    "            flatmaps_test:  h5py dataset for test\n",
    "        \"\"\"\n",
    "        self.combined_metadata = combined_metadata\n",
    "        self.indices = indices\n",
    "        self.flatmaps_train = flatmaps_train\n",
    "        self.flatmaps_test = flatmaps_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # i here is the i-th sample in our new train/test list\n",
    "        global_idx = self.indices[i]\n",
    "        m_str, source, idx_in_source = self.combined_metadata[global_idx]\n",
    "\n",
    "        # Parse JSON\n",
    "        m_dict = json.loads(m_str)\n",
    "\n",
    "        # Retrieve the actual flatmaps from the correct HDF5\n",
    "        if source == 'train':\n",
    "            # old train set\n",
    "            x = self.flatmaps_train[idx_in_source]\n",
    "        else:\n",
    "            # old test set\n",
    "            x = self.flatmaps_test[idx_in_source]\n",
    "\n",
    "        return x, m_dict\n",
    "\n",
    "class HCPFlatDataset(Dataset):\n",
    "    def __init__(self, flatmaps, metadata):\n",
    "        self.flatmaps = flatmaps\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.flatmaps[idx], json.loads(self.metadata[idx])\n",
    "\n",
    "if target == 'subject_id':\n",
    "    train_dataset_new = HCPSplitDataset(\n",
    "        combined_metadata,\n",
    "        new_train_indices,\n",
    "        flatmaps_train,\n",
    "        flatmaps_test\n",
    "    )\n",
    "\n",
    "    test_dataset_new = HCPSplitDataset(\n",
    "        combined_metadata,\n",
    "        new_test_indices,\n",
    "        flatmaps_train,\n",
    "        flatmaps_test\n",
    "    )\n",
    "\n",
    "    # And then the dataloaders\n",
    "    train_dl = DataLoader(train_dataset_new, batch_size=batch_size, shuffle=True, num_workers=num_workers//2)\n",
    "    test_dl = DataLoader(test_dataset_new, batch_size=batch_size, shuffle=False, num_workers=num_workers//2)\n",
    "\n",
    "else:\n",
    "    # original approach\n",
    "    train_dataset = HCPFlatDataset(flatmaps_train, metadata_train)\n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers//2)\n",
    "\n",
    "    test_dataset = HCPFlatDataset(flatmaps_test, metadata_test)\n",
    "    test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers//2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe0e0bb-a67e-468d-a21a-b0add258eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71674f50-3bdc-428b-9fcd-585f2285e3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 32, 144, 320])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a9b16-3c06-4c40-8c81-5b6169ef166e",
   "metadata": {},
   "source": [
    "### Load subject information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "784791d7-0ba2-4099-8155-f80949de122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "###### This is for restricted\n",
    "# Categorical columns (e.g., demographic categories, binary diagnoses)\n",
    "categorical_columns = [\n",
    "    \"Gender\",\n",
    "    \"Race\",\n",
    "    \"Ethnicity\",\n",
    "    \"SSAGA_PanicDisorder\",  # Panic disorder diagnosis (yes/no)\n",
    "    \"SSAGA_Depressive_Ep\"   # Depressive episode diagnosis (yes/no)\n",
    "]\n",
    "\n",
    "# Numerical columns (continuous, counts, raw scores, standardized scores, etc.)\n",
    "numerical_columns = [\n",
    "    # Basic demographics\n",
    "    \"Age_in_Yrs\",\n",
    "    \n",
    "    # Cognitive / \"IQ-like\" Measures\n",
    "    \"PMAT24_A_CR\",\n",
    "    \"CardSort_Unadj\",\n",
    "    \"CardSort_AgeAdj\",\n",
    "    \"ListSort_Unadj\",\n",
    "    \"ListSort_AgeAdj\",\n",
    "    \"PicSeq_Unadj\",\n",
    "    \"PicSeq_AgeAdj\",\n",
    "    \n",
    "    # Personality Traits (Big Five)\n",
    "    \"NEOFAC_A\",\n",
    "    \"NEOFAC_O\",\n",
    "    \"NEOFAC_C\",\n",
    "    \"NEOFAC_N\",\n",
    "    \"NEOFAC_E\",\n",
    "    # If you have all 60 NEO item-level responses:\n",
    "    # \"NEORAW_01\", \"NEORAW_02\", ..., \"NEORAW_60\",\n",
    "    \n",
    "    # Psychopathology / Mental Health\n",
    "    \"ASR_Anxd_Raw\",\n",
    "    \"ASR_Attn_Raw\",\n",
    "    \"ASR_Aggr_Raw\",\n",
    "    \"DSM_Depr_Raw\",\n",
    "    \"DSM_Anxi_Raw\",\n",
    "    \"SSAGA_Depressive_Sx\",  # Symptom count or severity\n",
    "    \n",
    "    # Substance Use Phenotypes\n",
    "    \"SSAGA_Alc_12_Frq\",\n",
    "    \"SSAGA_Alc_12_Max_Drinks\",\n",
    "    \"SSAGA_Times_Used_Illicits\",\n",
    "    \"SSAGA_Times_Used_Cocaine\",\n",
    "    \"Total_Drinks_7days\",\n",
    "    \"Total_Any_Tobacco_7days\",\n",
    "    \n",
    "    # Anthropometric / Basic Health\n",
    "    \"BMI\",\n",
    "    \"Height\",\n",
    "    \"Weight\",\n",
    "    \"BPSystolic\",\n",
    "    \"BPDiastolic\",\n",
    "    \"HbA1C\",\n",
    "    \"ThyroidHormone\",\n",
    "    \n",
    "    # Sleep / Quality of Life\n",
    "    \"PSQI_Score\",\n",
    "    # If you have separate PSQI component scores, list them here too, e.g.:\n",
    "    # \"PSQI_Component1\", \"PSQI_Component2\", ...\n",
    "    \"PainInterf_Tscore\",\n",
    "    \"LifeSatisf_Unadj\",\n",
    "    \"MeanPurp_Unadj\"\n",
    "]\n",
    "\n",
    "\n",
    "if target in ['subject_id', 'trial_type']:\n",
    "    target_type = 'special'\n",
    "elif target in categorical_columns:\n",
    "    target_type = 'categorical'\n",
    "elif target in numerical_columns:\n",
    "    target_type = 'numerical'\n",
    "\n",
    "\n",
    "# open the file containing subject information\n",
    "if not (target in ['subject_id', 'trial_type']):\n",
    "    subject_information_HCP_path = os.path.join(hcp_flat_path, \"subjects_data_restricted.csv\")\n",
    "    try:\n",
    "        subject_information_HCP = pd.read_csv(subject_information_HCP_path)\n",
    "    except:\n",
    "        try:\n",
    "            subject_information_HCP = pd.read_csv('./unrestricted_clane9_4_23_2024_13_28_14.csv')   \n",
    "        except:\n",
    "            assert False, \"Subject information file not found\"\n",
    "\n",
    "    # # show the first few rows of the subject information\n",
    "    # subject_information_HCP[age_related_columns + sex_related_columns].head()\n",
    "\n",
    "    # Handle missing values (e.g., impute with mean)\n",
    "    \n",
    "\n",
    "    if target in numerical_columns:\n",
    "        # Count NaNs or missing values\n",
    "        n_missing = subject_information_HCP[target].isnull().sum()\n",
    "        print(f\"Number of missing values in {target}: {n_missing}. Replacing with mean.\")\n",
    "        mean_ = subject_information_HCP[target].mean()\n",
    "        # Replace missing values or NaNs with the mean\n",
    "        subject_information_HCP[target].fillna(mean_, inplace=True)\n",
    "        # Initialize the scaler\n",
    "        scaler = StandardScaler()    \n",
    "        # Perform z-score normalization\n",
    "        subject_information_HCP[f'{target}_z'] = scaler.fit_transform(subject_information_HCP[[target]])\n",
    "\n",
    "    if target in categorical_columns:\n",
    "        # Perform label encoding\n",
    "        label_enc = LabelEncoder()\n",
    "        subject_information_HCP[f'{target}_encoded'] = label_enc.fit_transform(subject_information_HCP[target])\n",
    "\n",
    "def train_test_split_by_subject(df, test_ratio=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Split a dataframe into train and test so that\n",
    "    every subject in test also appears in train at least once.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Your dataset, containing at least the columns:\n",
    "        ['sub', ...]\n",
    "    test_ratio : float\n",
    "        Percentage of each subject's rows to allocate to test.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_df : pd.DataFrame\n",
    "    test_df : pd.DataFrame\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "    \n",
    "    # Group by subject\n",
    "    for subject, df_sub in df.groupby('sub'):\n",
    "        n = len(df_sub)\n",
    "        \n",
    "        # If the subject only has 1 row, put it all in train\n",
    "        if n == 1:\n",
    "            train_dfs.append(df_sub)\n",
    "        else:\n",
    "            # Decide how many rows go to test\n",
    "            n_test = int(round(test_ratio * n))\n",
    "            # Ensure at least 1 row ends up in train\n",
    "            # (i.e. if rounding leads to n_test == n, reduce n_test by 1)\n",
    "            if n_test >= n:\n",
    "                n_test = n - 1\n",
    "            \n",
    "            # Randomly sample n_test rows for test\n",
    "            test_rows = df_sub.sample(n_test, random_state=random_state)\n",
    "            # The remaining go to train\n",
    "            train_rows = df_sub.drop(test_rows.index)\n",
    "            \n",
    "            test_dfs.append(test_rows)\n",
    "            train_dfs.append(train_rows)\n",
    "    \n",
    "    # Combine all splits\n",
    "    train_df = pd.concat(train_dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "    \n",
    "def get_label_restricted(subject_id: List[str], target: str, normalized: bool = True) -> List:\n",
    "    \"\"\"\n",
    "    Get the label for the given subject id and target\n",
    "    \"\"\"\n",
    "    subject_id = [int(x) for x in subject_id]\n",
    "\n",
    "    if target in numerical_columns:\n",
    "        target_array = []\n",
    "        for subject in subject_id:\n",
    "            c_target = subject_information_HCP[subject_information_HCP['Subject'] == subject][f'{target}' if not normalized else f'{target}_z'].values\n",
    "            # if the subject is not in the subject information file trigger an error\n",
    "            if len(c_target) == 0:\n",
    "                assert False, f\"Subject {subject} not found in subject information file\"\n",
    "            if len(c_target) > 1:\n",
    "                print(f\"Warning: Multiple entries for subject {subject}\")\n",
    "\n",
    "            target_array.append(np.float32(c_target[0]))\n",
    "\n",
    "        return np.array(target_array)\n",
    "    \n",
    "    elif target in categorical_columns:\n",
    "        target_array = []\n",
    "        for subject in subject_id:\n",
    "            c_target = subject_information_HCP[subject_information_HCP['Subject'] == subject][f'{target}_encoded'].values\n",
    "            # if the subject is not in the subject information file trigger an error\n",
    "            if len(c_target) == 0:\n",
    "                assert False, f\"Subject {subject} not found in subject information file\"\n",
    "            if len(c_target) > 1:\n",
    "                print(f\"Warning: Multiple entries for subject {subject}\")\n",
    "\n",
    "            target_array.append(np.int8(c_target[0]))\n",
    "\n",
    "        return np.array(target_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7d8760b-2e62-4ba5-ae34-b42376ad0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # open the file containing subject information\n",
    "# if target == \"age\" or target == \"sex\":\n",
    "#     subject_information_HCP_path = os.path.join(hcp_flat_path, \"subjects_data_restricted.csv\")\n",
    "#     try:\n",
    "#         subject_information_HCP = pd.read_csv(subject_information_HCP_path)\n",
    "#     except:\n",
    "#         try:\n",
    "#             subject_information_HCP = pd.read_csv('./unrestricted_clane9_4_23_2024_13_28_14.csv')   \n",
    "#         except:\n",
    "#             assert False, \"Subject information file not found\"\n",
    "\n",
    "#     ###### This is for unrestricted\n",
    "#     # age_related_columns = [\n",
    "#     #     'Age', 'PicSeq_AgeAdj', 'CardSort_AgeAdj', 'Flanker_AgeAdj',\n",
    "#     #     'ReadEng_AgeAdj', 'PicVocab_AgeAdj', 'ProcSpeed_AgeAdj',\n",
    "#     #     'CogFluidComp_AgeAdj', 'CogEarlyComp_AgeAdj', 'CogTotalComp_AgeAdj',\n",
    "#     #     'CogCrystalComp_AgeAdj', 'Endurance_AgeAdj', 'Dexterity_AgeAdj',\n",
    "#     #     'Strength_AgeAdj', 'Odor_AgeAdj', 'Taste_AgeAdj'\n",
    "#     # ]\n",
    "    \n",
    "#     # sex_related_columns = [\n",
    "#     #     'Gender'\n",
    "#     # ]\n",
    "\n",
    "#     ###### This is for restricted\n",
    "#     gender_related_columns = [\n",
    "#         'Gender'\n",
    "#     ]\n",
    "\n",
    "#     age_related_columns = [\n",
    "#         'Age_in_Yrs',\n",
    "#         'Menstrual_AgeBegan',\n",
    "#         'Menstrual_AgeIrreg',\n",
    "#         'Menstrual_AgeStop',\n",
    "#         'SSAGA_Alc_Age_1st_Use',\n",
    "#         'SSAGA_TB_Age_1st_Cig',\n",
    "#         'SSAGA_Mj_Age_1st_Use',\n",
    "#         'Endurance_AgeAdj',\n",
    "#         'Dexterity_AgeAdj',\n",
    "#         'Strength_AgeAdj',\n",
    "#         'PicSeq_AgeAdj',\n",
    "#         'CardSort_AgeAdj',\n",
    "#         'Flanker_AgeAdj',\n",
    "#         'ReadEng_AgeAdj',\n",
    "#         'PicVocab_AgeAdj',\n",
    "#         'ProcSpeed_AgeAdj',\n",
    "#         'Odor_AgeAdj',\n",
    "#         'Taste_AgeAdj'\n",
    "#     ]\n",
    "\n",
    "#     # # show the first few rows of the subject information\n",
    "#     # subject_information_HCP[age_related_columns + sex_related_columns].head()\n",
    "\n",
    "#     # Handle missing values (e.g., impute with mean)\n",
    "#     mean_age = subject_information_HCP['Age_in_Yrs'].mean()\n",
    "    \n",
    "#     # Initialize the scaler\n",
    "#     scaler = StandardScaler()\n",
    "    \n",
    "#     # Perform z-score normalization\n",
    "#     subject_information_HCP['Age_in_Yrs_z'] = scaler.fit_transform(subject_information_HCP[['Age_in_Yrs']])\n",
    "\n",
    "\n",
    "    \n",
    "# def get_label_unrestricted(subject_id: List[str], target: str, method_for_age: str = 'mean') -> List:\n",
    "#     \"\"\"\n",
    "#     Get the label for the given subject id and target.\n",
    "\n",
    "#     For sex 0 is F and 1 is M\n",
    "#     \"\"\"\n",
    "\n",
    "#     # convert to list of ints\n",
    "#     subject_id = [int(x) for x in subject_id]\n",
    "\n",
    "#     if target == \"age\":\n",
    "#         age_array = []\n",
    "#         for subject in subject_id:\n",
    "#             c_age = subject_information_HCP[subject_information_HCP['Subject'] == subject]['Age'].values\n",
    "#             # if the subject is not in the subject information file trigger an error\n",
    "#             if len(c_age) == 0:\n",
    "#                 assert False, f\"Subject {subject} not found in subject information file\"\n",
    "#             if len(c_age) > 1:\n",
    "#                 print(f\"Warning: Multiple entries for subject {subject}\")\n",
    "\n",
    "#             c_age = c_age[0].split('-')\n",
    "#             if len(c_age) < 2:\n",
    "#                 c_age = c_age[0].split('+')\n",
    "#                 age_array.append(int(c_age[0]))\n",
    "#             else:\n",
    "#                 if method_for_age == 'mean':\n",
    "#                     age_array.append(np.mean([int(x) for x in c_age]))\n",
    "#                 elif method_for_age == 'min':\n",
    "#                     age_array.append(np.min([int(x) for x in c_age]))\n",
    "#                 elif method_for_age == 'max':\n",
    "#                     age_array.append(np.max([int(x) for x in c_age]))\n",
    "#                 else:\n",
    "#                     assert False, f\"Method {method_for_age} not recognized\"\n",
    "\n",
    "#         return np.array(age_array)  \n",
    "    \n",
    "#     elif target == 'sex':\n",
    "#         sex_array = []\n",
    "#         for subject in subject_id:\n",
    "#             c_sex = subject_information_HCP[subject_information_HCP['Subject'] == subject]['Gender'].values\n",
    "#             # if the subject is not in the subject information file trigger an error\n",
    "#             if len(c_sex) == 0:\n",
    "#                 assert False, f\"Subject {subject} not found in subject information file\"\n",
    "#             if len(c_sex) > 1:\n",
    "#                 print(f\"Warning: Multiple entries for subject {subject}\")\n",
    "#             sex_array.append(int(c_sex[0] == 'M'))\n",
    "#         return sex_array\n",
    "\n",
    "# def get_label_restricted(subject_id: List[str], target: str, normalized: bool = True) -> List:\n",
    "#     \"\"\"\n",
    "#     Get the label for the given subject id and target.\n",
    "\n",
    "#     For sex 0 is F and 1 is M\n",
    "#     \"\"\"\n",
    "\n",
    "#     # convert to list of ints\n",
    "#     subject_id = [int(x) for x in subject_id]\n",
    "\n",
    "#     if target == \"age\":\n",
    "#         age_array = []\n",
    "#         for subject in subject_id:\n",
    "#             c_age = subject_information_HCP[subject_information_HCP['Subject'] == subject]['Age_in_Yrs' if not normalized else 'Age_in_Yrs_z'].values\n",
    "#             # if the subject is not in the subject information file trigger an error\n",
    "#             if len(c_age) == 0:\n",
    "#                 assert False, f\"Subject {subject} not found in subject information file\"\n",
    "#             if len(c_age) > 1:\n",
    "#                 print(f\"Warning: Multiple entries for subject {subject}\")\n",
    "\n",
    "#             age_array.append(np.int8(c_age[0]))\n",
    "\n",
    "#         return np.array(age_array)  \n",
    "    \n",
    "#     elif target == 'sex':\n",
    "#         sex_array = []\n",
    "#         for subject in subject_id:\n",
    "#             c_sex = subject_information_HCP[subject_information_HCP['Subject'] == subject]['Gender'].values\n",
    "#             # if the subject is not in the subject information file trigger an error\n",
    "#             if len(c_sex) == 0:\n",
    "#                 assert False, f\"Subject {subject} not found in subject information file\"\n",
    "#             if len(c_sex) > 1:\n",
    "#                 print(f\"Warning: Multiple entries for subject {subject}\")\n",
    "#             sex_array.append(int(c_sex[0] == 'M'))\n",
    "#         return sex_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40f04009-13f1-473d-9808-181a46d3e97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 1093\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "if target == \"trial_type\":\n",
    "    INCLUDE_CONDS = {\n",
    "        \"fear\",\n",
    "        \"neut\",\n",
    "        \"math\",\n",
    "        \"story\",\n",
    "        \"lf\",\n",
    "        \"lh\",\n",
    "        \"rf\",\n",
    "        \"rh\",\n",
    "        \"t\",\n",
    "        \"match\",\n",
    "        \"relation\",\n",
    "        \"mental\",\n",
    "        \"rnd\",\n",
    "        \"0bk_body\",\n",
    "        \"2bk_body\",\n",
    "        \"0bk_faces\",\n",
    "        \"2bk_faces\",\n",
    "        \"0bk_places\",\n",
    "        \"2bk_places\",\n",
    "        \"0bk_tools\",\n",
    "        \"2bk_tools\",\n",
    "    }\n",
    "    # Initialize the label encoder\n",
    "    label_enc = LabelEncoder()\n",
    "    label_enc.fit(sorted(INCLUDE_CONDS))  # Ensure consistent ordering\n",
    "elif target == 'subject_id':\n",
    "    all_subs = [json.loads(i_data[0])['sub'] for i_data in combined_metadata]\n",
    "    label_enc = LabelEncoder()\n",
    "    label_enc.fit(all_subs)\n",
    "    \n",
    "if target_type in ['categorical', 'special']:\n",
    "    num_classes = len(label_enc.classes_)\n",
    "    print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "592b3cd3-6232-4cff-8d90-9034067b3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in tqdm(train_dl):\n",
    "#     x = sample[0]\n",
    "#     subject_id = sample[1]['sub']\n",
    "\n",
    "#     # benchmark time\n",
    "#     start = time.time()\n",
    "#     y = get_label(subject_id, 'age')\n",
    "#     end = time.time()\n",
    "#     print(f\"Time taken: {end - start}\")\n",
    "#     print(x.shape, y, subject_id, torch.Tensor(y).shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d7e4f-eb34-498b-bbc6-127eb4a7bdc8",
   "metadata": {},
   "source": [
    "### Create pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a2c6527-fa75-42ba-be43-3d753ed8100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: 1474560\n"
     ]
    }
   ],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the input except for the batch dimension\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.linear(x)\n",
    "        return out  # Raw logits\n",
    "\n",
    "# Determine the input dimension from a single sample\n",
    "# Assuming images are of shape [1, 16, 144, 320]\n",
    "sample_batch = batch\n",
    "sample_image = sample_batch[0][0]  # Shape: [16, 144, 320]\n",
    "input_dim = sample_image.view(-1).size(0)\n",
    "print(f\"Input dimension: {input_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8983a0b-c810-4424-8c31-32b9921ebd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 219400\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "\n",
    "if (target in [\"trial_type\", \"subject_id\"]) or (target in categorical_columns):\n",
    "    model = LinearClassifier(input_dim=input_dim, num_classes=num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "elif target in numerical_columns:\n",
    "    model = LinearClassifier(input_dim=input_dim, num_classes=1)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "# elif target == \"sex\":\n",
    "#     model = LinearClassifier(input_dim=input_dim, num_classes=1)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# import schedulefree\n",
    "# optimizer = schedulefree.AdamWScheduleFree(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "\n",
    "num_iterations_per_epoch = math.ceil(flatmaps_train.shape[0]/batch_size)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b205b2c-730c-4d9e-89b7-c28031de78a7",
   "metadata": {},
   "source": [
    "### Wandb logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63ce8f26-a1b9-4e29-badc-7b9b30d0d3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in interactive notebook. Disabling W&B and ckpt saving.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import uuid\n",
    "\n",
    "myuuid = uuid.uuid4()\n",
    "str(myuuid)\n",
    "if utils.is_interactive():\n",
    "    print(\"Running in interactive notebook. Disabling W&B and ckpt saving.\")\n",
    "    wandb_log = False\n",
    "    save_ckpt = False\n",
    "\n",
    "if wandb_log:\n",
    "    wandb_project = 'fMRI-foundation-model'\n",
    "    wandb_config = {\n",
    "        \"model_name\": f\"HCPflat_raw_{target}\",\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"seed\": seed,\n",
    "        \"lr_scheduler_type\": lr_scheduler_type,\n",
    "        \"save_ckpt\": save_ckpt,\n",
    "        \"seed\": seed,\n",
    "        \"max_lr\": max_lr,\n",
    "        \"target\": target,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"weight_decay\": weight_decay\n",
    "    }\n",
    "    print(\"wandb_config:\\n\", wandb_config)\n",
    "    random_id = random.randint(0, 100000)\n",
    "    wandb_id = \"HCPflat_raw\" + f\"_{model_suffix}_{target}_{myuuid}\"\n",
    "    print(\"wandb_id:\", wandb_id)\n",
    "    wandb.init(\n",
    "        id=wandb_id,\n",
    "        project=wandb_project,\n",
    "        name=\"HCPflat_raw\"+ f\"_{model_suffix}_{target}\",\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d87e5c-94fc-4528-b2eb-65ef59c79634",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43535f4-0f6e-4655-b4f0-926416685358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1a7bcd28464a7aa86337d42b4f6e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/40 - Training:   0%|          | 0/5465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [100/5465] - Training Loss: 7.2323 - Training Accuracy: 0.06%\n",
      "Step [200/5465] - Training Loss: 7.0261 - Training Accuracy: 0.03%\n",
      "Step [300/5465] - Training Loss: 7.1709 - Training Accuracy: 0.06%\n",
      "Step [400/5465] - Training Loss: 7.0584 - Training Accuracy: 0.08%\n",
      "Step [500/5465] - Training Loss: 7.2548 - Training Accuracy: 0.10%\n",
      "Step [600/5465] - Training Loss: 7.2418 - Training Accuracy: 0.17%\n",
      "Step [700/5465] - Training Loss: 7.3006 - Training Accuracy: 0.17%\n",
      "Step [800/5465] - Training Loss: 7.0092 - Training Accuracy: 0.17%\n",
      "Step [900/5465] - Training Loss: 7.0600 - Training Accuracy: 0.22%\n",
      "Step [1000/5465] - Training Loss: 7.0577 - Training Accuracy: 0.26%\n",
      "Step [1100/5465] - Training Loss: 6.8615 - Training Accuracy: 0.29%\n",
      "Step [1200/5465] - Training Loss: 7.2380 - Training Accuracy: 0.32%\n",
      "Step [1300/5465] - Training Loss: 7.2289 - Training Accuracy: 0.39%\n",
      "Step [1400/5465] - Training Loss: 7.0936 - Training Accuracy: 0.47%\n",
      "Step [1500/5465] - Training Loss: 7.4408 - Training Accuracy: 0.53%\n",
      "Step [1600/5465] - Training Loss: 7.4228 - Training Accuracy: 0.60%\n",
      "Step [1700/5465] - Training Loss: 7.4819 - Training Accuracy: 0.66%\n",
      "Step [1800/5465] - Training Loss: 7.2131 - Training Accuracy: 0.72%\n",
      "Step [1900/5465] - Training Loss: 7.2626 - Training Accuracy: 0.80%\n",
      "Step [2000/5465] - Training Loss: 6.4152 - Training Accuracy: 0.88%\n",
      "Step [2100/5465] - Training Loss: 7.6812 - Training Accuracy: 0.93%\n",
      "Step [2200/5465] - Training Loss: 7.4332 - Training Accuracy: 1.03%\n",
      "Step [2300/5465] - Training Loss: 7.0827 - Training Accuracy: 1.13%\n",
      "Step [2400/5465] - Training Loss: 7.3298 - Training Accuracy: 1.23%\n",
      "Step [2500/5465] - Training Loss: 6.7703 - Training Accuracy: 1.32%\n",
      "Step [2600/5465] - Training Loss: 7.3167 - Training Accuracy: 1.38%\n",
      "Step [2700/5465] - Training Loss: 7.4283 - Training Accuracy: 1.54%\n",
      "Step [2800/5465] - Training Loss: 6.8075 - Training Accuracy: 1.65%\n",
      "Step [2900/5465] - Training Loss: 7.1921 - Training Accuracy: 1.76%\n",
      "Step [3000/5465] - Training Loss: 7.6305 - Training Accuracy: 1.84%\n",
      "Step [3100/5465] - Training Loss: 7.0023 - Training Accuracy: 1.95%\n",
      "Step [3200/5465] - Training Loss: 6.6702 - Training Accuracy: 2.06%\n",
      "Step [3300/5465] - Training Loss: 7.1867 - Training Accuracy: 2.20%\n",
      "Step [3400/5465] - Training Loss: 7.0551 - Training Accuracy: 2.32%\n",
      "Step [3500/5465] - Training Loss: 6.4014 - Training Accuracy: 2.44%\n",
      "Step [3600/5465] - Training Loss: 7.4587 - Training Accuracy: 2.57%\n",
      "Step [3700/5465] - Training Loss: 8.2989 - Training Accuracy: 2.69%\n",
      "Step [3800/5465] - Training Loss: 6.8382 - Training Accuracy: 2.82%\n",
      "Step [3900/5465] - Training Loss: 8.0600 - Training Accuracy: 2.99%\n",
      "Step [4000/5465] - Training Loss: 7.1570 - Training Accuracy: 3.17%\n",
      "Step [4100/5465] - Training Loss: 7.1707 - Training Accuracy: 3.32%\n",
      "Step [4200/5465] - Training Loss: 7.1445 - Training Accuracy: 3.43%\n",
      "Step [4300/5465] - Training Loss: 5.7310 - Training Accuracy: 3.62%\n",
      "Step [4400/5465] - Training Loss: 7.8406 - Training Accuracy: 3.78%\n",
      "Step [4500/5465] - Training Loss: 7.2775 - Training Accuracy: 3.93%\n",
      "Step [4600/5465] - Training Loss: 7.1658 - Training Accuracy: 4.07%\n",
      "Step [4700/5465] - Training Loss: 6.3872 - Training Accuracy: 4.24%\n",
      "Step [4800/5465] - Training Loss: 6.4935 - Training Accuracy: 4.45%\n",
      "Step [4900/5465] - Training Loss: 6.4051 - Training Accuracy: 4.63%\n",
      "Step [5000/5465] - Training Loss: 5.6336 - Training Accuracy: 4.81%\n",
      "Step [5100/5465] - Training Loss: 6.5918 - Training Accuracy: 5.00%\n",
      "Step [5200/5465] - Training Loss: 6.5362 - Training Accuracy: 5.16%\n",
      "Step [5300/5465] - Training Loss: 6.7471 - Training Accuracy: 5.33%\n",
      "Step [5400/5465] - Training Loss: 9.1806 - Training Accuracy: 5.56%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8fde30d20843f884a549cf40737056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/40 - Validation:   0%|          | 0/613 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] - Training Loss: 7.1431, Training Accuracy: 5.68% - Validation Loss: 7.3734, Validation Accuracy: 15.28%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc426ad1d87464bb3424efb2a37f803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/40 - Training:   0%|          | 0/5465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [100/5465] - Training Loss: 1.0922 - Training Accuracy: 81.06%\n",
      "Step [200/5465] - Training Loss: 1.4752 - Training Accuracy: 81.16%\n",
      "Step [300/5465] - Training Loss: 3.1480 - Training Accuracy: 81.25%\n",
      "Step [400/5465] - Training Loss: 1.9623 - Training Accuracy: 81.25%\n",
      "Step [500/5465] - Training Loss: 1.3727 - Training Accuracy: 81.44%\n",
      "Step [600/5465] - Training Loss: 1.3185 - Training Accuracy: 81.59%\n",
      "Step [700/5465] - Training Loss: 1.5319 - Training Accuracy: 81.60%\n",
      "Step [800/5465] - Training Loss: 1.0762 - Training Accuracy: 81.27%\n",
      "Step [900/5465] - Training Loss: 1.9257 - Training Accuracy: 81.10%\n",
      "Step [1000/5465] - Training Loss: 1.9981 - Training Accuracy: 80.86%\n",
      "Step [1100/5465] - Training Loss: 2.4925 - Training Accuracy: 80.57%\n",
      "Step [1200/5465] - Training Loss: 1.8952 - Training Accuracy: 80.53%\n",
      "Step [1300/5465] - Training Loss: 1.7435 - Training Accuracy: 80.40%\n",
      "Step [1400/5465] - Training Loss: 3.3180 - Training Accuracy: 80.15%\n",
      "Step [1500/5465] - Training Loss: 3.0942 - Training Accuracy: 79.89%\n",
      "Step [1600/5465] - Training Loss: 5.0473 - Training Accuracy: 79.70%\n",
      "Step [1700/5465] - Training Loss: 0.9989 - Training Accuracy: 79.54%\n",
      "Step [1800/5465] - Training Loss: 2.2491 - Training Accuracy: 79.41%\n",
      "Step [1900/5465] - Training Loss: 1.1098 - Training Accuracy: 79.26%\n",
      "Step [2000/5465] - Training Loss: 3.0678 - Training Accuracy: 78.92%\n",
      "Step [2100/5465] - Training Loss: 1.4907 - Training Accuracy: 78.71%\n",
      "Step [2200/5465] - Training Loss: 2.5832 - Training Accuracy: 78.41%\n",
      "Step [2300/5465] - Training Loss: 2.4911 - Training Accuracy: 78.17%\n",
      "Step [2400/5465] - Training Loss: 4.8635 - Training Accuracy: 77.85%\n",
      "Step [2500/5465] - Training Loss: 1.5200 - Training Accuracy: 77.63%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    mse_age_train = 0.0\n",
    "    total_train = 0\n",
    "    step = 0\n",
    "\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Reset gradients before starting training\n",
    "\n",
    "    for batch in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        images = batch[0].to(device).float()  # Shape: [batch_size, num_frames, 144, 320]\n",
    "\n",
    "        # Prepare labels based on target type\n",
    "        if target == \"trial_type\":\n",
    "            labels = batch[1]['trial_type']  # List of labels\n",
    "            labels = label_enc.transform(labels)\n",
    "            labels = torch.tensor(labels, dtype=torch.long).to(device)  # Shape: [batch_size]\n",
    "        if target == 'subject_id':\n",
    "            labels = label_enc.transform(batch[1]['sub'])\n",
    "            labels = torch.tensor(labels, dtype=torch.long).to(device)  # Shape: [batch_size]\n",
    "        elif target in numerical_columns:\n",
    "            labels = get_label_restricted(batch[1]['sub'], target)\n",
    "            labels = torch.tensor(labels, dtype=torch.float).to(device)  # Shape: [batch_size]\n",
    "        elif target in categorical_columns:\n",
    "            labels = get_label_restricted(batch[1]['sub'], target)\n",
    "            labels = torch.tensor(labels, dtype=torch.long).to(device)  # Shape: [batch_size]\n",
    "\n",
    "        # labels = labels.unsqueeze(1)\n",
    "        # Forward pass\n",
    "        outputs = model(images)  # Output shape depends on the target\n",
    "\n",
    "        # # Compute loss\n",
    "        # if target in [\"trial_type\", \"sex\"]:\n",
    "        #     # For classification, ensure outputs are logits\n",
    "        #     loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "        # elif target == \"age\":\n",
    "        #     # For regression, ensure outputs are single values\n",
    "        loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "        # Calculate and accumulate metrics\n",
    "        if target_type in [\"categorical\", \"special\"]:\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        elif target_type == \"numerical\":\n",
    "            mse_age_train += (torch.sum((outputs.squeeze() - labels) ** 2).item())\n",
    "        # elif target == \"sex\":\n",
    "        #     threshold = 0.5\n",
    "        #     predicted = (torch.sigmoid(outputs) > threshold).float().squeeze()\n",
    "        #     correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        total_train += labels.size(0)\n",
    "        step += 1\n",
    "\n",
    "        # Print intermediate metrics every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            if target_type in ['categorical', 'special']:\n",
    "                current_accuracy = 100 * correct_train / total_train if total_train > 0 else 0.0\n",
    "                print(f\"Step [{step}/{len(train_dl)}] - Training Loss: {loss.item():.4f} - Training Accuracy: {current_accuracy:.2f}%\")\n",
    "            elif target_type == 'numerical':\n",
    "                current_mse = mse_age_train / total_train if total_train > 0 else 0.0\n",
    "                print(f\"Step [{step}/{len(train_dl)}] - Training Loss: {loss.item():.4f} - Training MSE: {current_mse:.4f}\")\n",
    "\n",
    "        if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    # Calculate epoch-level metrics\n",
    "    epoch_train_loss = running_train_loss / step if step > 0 else 0.0\n",
    "\n",
    "    if target_type in ['categorical', 'special']:\n",
    "        train_accuracy = 100 * correct_train / total_train if total_train > 0 else 0.0\n",
    "    elif target_type == 'numerical':\n",
    "        train_mse = mse_age_train / total_train if total_train > 0 else 0.0\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    mse_age_val = 0.0\n",
    "    total_val = 0\n",
    "    step_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dl, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            images = batch[0].to(device).float()  # Removed unsqueeze(1) unless specifically needed\n",
    "\n",
    "            # Prepare labels based on target type\n",
    "            if target == \"trial_type\":\n",
    "                labels = batch[1]['trial_type']  # List of labels\n",
    "                labels = label_enc.transform(labels)\n",
    "                labels = torch.tensor(labels, dtype=torch.long).to(device)  # Shape: [batch_size]\n",
    "            if target == 'subject_id':\n",
    "                labels = label_enc.transform(batch[1]['sub'])\n",
    "                labels = torch.tensor(labels, dtype=torch.long).to(device)  # Shape: [batch_size]\n",
    "            elif target in numerical_columns:\n",
    "                labels = get_label_restricted(batch[1]['sub'], target)\n",
    "                labels = torch.tensor(labels, dtype=torch.float).to(device)  # Shape: [batch_size]\n",
    "            elif target in categorical_columns:\n",
    "                labels = get_label_restricted(batch[1]['sub'], target)\n",
    "                labels = torch.tensor(labels, dtype=torch.long).to(device)  # Shape: [batch_size]\n",
    "\n",
    "            # labels = labels.unsqueeze(1)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            \n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "\n",
    "            # Accumulate loss\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate metrics\n",
    "            if target_type in [\"categorical\", \"special\"]:\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "            elif target_type == \"numerical\":\n",
    "                mse_age_val += (torch.sum((outputs.squeeze() - labels) ** 2).item())\n",
    "            # elif target == \"sex\":\n",
    "            #     threshold = 0.5\n",
    "            #     predicted = (torch.sigmoid(outputs) > threshold).float().squeeze()\n",
    "            #     correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            total_val += labels.size(0)\n",
    "            step_val += 1\n",
    "\n",
    "    # Calculate epoch-level validation metrics\n",
    "    epoch_val_loss = running_val_loss / step_val if step_val > 0 else 0.0\n",
    "\n",
    "    if target_type in ['categorical', 'special']:\n",
    "        val_accuracy = 100 * correct_val / total_val if total_val > 0 else 0.0\n",
    "    elif target_type == 'numerical':\n",
    "        val_mse = mse_age_val / total_val if total_val > 0 else 0.0\n",
    "\n",
    "    # Print epoch-level metrics\n",
    "    if target_type in ['categorical', 'special']:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"- Training Loss: {epoch_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}% \"\n",
    "              f\"- Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    elif target_type == 'numerical':\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"- Training Loss: {epoch_train_loss:.4f}, Training MSE: {train_mse:.4f} \"\n",
    "              f\"- Validation Loss: {epoch_val_loss:.4f}, Validation MSE: {val_mse:.4f}\")\n",
    "\n",
    "    # Log metrics with wandb\n",
    "    if wandb_log:\n",
    "        log_dict = {\n",
    "            \"epoch_train_loss\": epoch_train_loss,\n",
    "            \"epoch_val_loss\": epoch_val_loss,\n",
    "        }\n",
    "        if target_type in ['categorical', 'special']:\n",
    "            log_dict.update({\n",
    "                f\"train_accuracy_{target}\": train_accuracy,\n",
    "                f\"val_accuracy_{target}\": val_accuracy,\n",
    "            })\n",
    "        elif target_type == 'numerical':\n",
    "            log_dict.update({\n",
    "                f\"train_mse_{target}\": train_mse,\n",
    "                f\"val_mse_{target}\": val_mse,\n",
    "            })\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "# Save checkpoint if required\n",
    "if save_ckpt:\n",
    "    outdir = os.path.abspath(f'checkpoints/{\"HCPflat_raw\"+ f\"_{model_suffix}_{target}\"}_{random_id}')\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    print(\"Saving checkpoint to:\", outdir)\n",
    "    # Save model state\n",
    "    torch.save(model.state_dict(), os.path.join(outdir, \"model.pth\"))\n",
    "    # Save configuration\n",
    "    with open(os.path.join(outdir, \"config.yaml\"), 'w') as f:\n",
    "        yaml.dump(wandb_config, f)\n",
    "    print(f\"Model and config saved to {outdir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation_env",
   "language": "python",
   "name": "foundation_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
