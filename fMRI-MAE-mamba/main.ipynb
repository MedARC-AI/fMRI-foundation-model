{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "import models\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "### Multi-GPU config ###\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"Number of available CUDA devices: {device_count}\")\n",
    "\n",
    "local_rank = os.getenv('LOCAL_RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(f\"LOCAL RANK={local_rank}\")\n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(f\"NUM GPUS={num_devices}\")\n",
    "distributed = True if num_devices>1 else False\n",
    "if distributed: assert device_count==num_devices\n",
    "\n",
    "node = os.getenv('SLURM_NODEID')\n",
    "if node is None:\n",
    "    node = 0\n",
    "else:\n",
    "    node = int(node)\n",
    "print(f\"NODE={node}\")\n",
    "\n",
    "global_rank = os.getenv('RANK')\n",
    "if global_rank is None:\n",
    "    global_rank = 0\n",
    "else:\n",
    "    global_rank = int(global_rank)\n",
    "print(f\"GLOBAL RANK={global_rank}\")\n",
    "\n",
    "world_size = os.getenv('WORLD_SIZE')\n",
    "if world_size is None: \n",
    "    world_size = 1\n",
    "else:\n",
    "    world_size = int(world_size)\n",
    "print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "# batch_size = global_batch_size // num_devices\n",
    "global_batch_size = batch_size * world_size\n",
    "\n",
    "# FSDP Setup\n",
    "if distributed:\n",
    "    import torch.distributed as dist\n",
    "    import torch.multiprocessing as mp\n",
    "    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "    from torch.distributed.fsdp.api import BackwardPrefetch, CPUOffload, ShardingStrategy\n",
    "    import functools\n",
    "    from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy, transformer_auto_wrap_policy\n",
    "    print(\"starting init_process_group...\")\n",
    "    dist.init_process_group(\"nccl\", rank=global_rank, world_size=world_size)\n",
    "    print(f\"setting device to cuda:{local_rank}\")\n",
    "    try:\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.device('cuda',local_rank)\n",
    "        print(f\"\\nSuccessfully set cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "    except Exception as error:        \n",
    "        print(f\"\\nFAILED TO SET DEVICE cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "        print(\"An exception occurred:\", error)\n",
    "        \n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "print(\"device =\", device, \"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08179db-9c6a-4bc6-a245-79fae6884ca2",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1c3fe-28ab-40b7-8906-c6a9c8070d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(config)\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "num_patches = int(\n",
    "    (img_size[0] / patch_size)\n",
    "    * (img_size[1] / patch_size)\n",
    "    * (img_size[2] / patch_size)\n",
    "    * num_frames\n",
    ")\n",
    "num_patches_per_timepoint = num_patches // num_frames\n",
    "num_encoder_patches = int(num_patches_per_timepoint * (1 - tube_start_masking_ratio) * num_frames)\n",
    "# num_decoder_patches = int(num_patches_per_timepoint * (1 - decoder_mask_ratio) * num_frames)\n",
    "print(\"num_patches\", num_patches)\n",
    "print(\"num_encoder_patches\", num_encoder_patches)\n",
    "# print(\"num_decoder_patches\", num_decoder_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8419d-988f-42c6-acb6-b258e0694eee",
   "metadata": {},
   "source": [
    "# Prep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e37b6b9-5b91-4c4a-af85-ac2af69704e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# vit_size = {\n",
    "#     \"encoder\": encoder_model,\n",
    "#     \"decoder\": decoder_model\n",
    "# }\n",
    "    \n",
    "# vit_model = models.get_vit(\n",
    "#     size=vit_size,\n",
    "#     image_size=img_size,  # depth, height, width\n",
    "#     image_patch_size=(patch_size,patch_size,patch_size),  # depth, height, width patch size\n",
    "#     frames=num_frames,\n",
    "#     frame_patch_size=frame_patch_size,\n",
    "#     channels=1,\n",
    "#     use_rope_emb=use_rope_emb,\n",
    "#     use_cls_token=use_cls_token,\n",
    "# )\n",
    "model = models.get_mamba(\"middle\",\n",
    "                        channels=1,\n",
    "                        img_size=img_size,  # depth, height, width\n",
    "                        patch_size=(patch_size,patch_size,patch_size),\n",
    "                        num_frames=num_frames,\n",
    "                        frame_patch_size=frame_patch_size,\n",
    "                        device=device,\n",
    "                        embed_dim=512, \n",
    "                        encoder_outdim=encoder_outdim,\n",
    "                        decoder_outdim=decoder_outdim,\n",
    "                        encoder_depth=encoder_depth, \n",
    "                        decoder_depth=decoder_depth,\n",
    "                        )\n",
    "\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works without error\n",
    "model = model.to(device)\n",
    "# encoder_mask = torch.zeros(num_patches).to(device).to(torch.bool)\n",
    "# encoder_mask[:num_encoder_patches] = True\n",
    "# decoder_mask = torch.zeros(num_patches).to(device).to(torch.bool)\n",
    "# decoder_mask[-num_decoder_patches:] = True\n",
    "# with torch.no_grad():\n",
    "#     print(\"\\nencoder\")\n",
    "#     encoder_out = model(\n",
    "#                 torch.randn(6, 1, 4, 64, 64, 48).to(device),\n",
    "#                 mask=encoder_mask)\n",
    "#     print(\"\\ndecoder\")\n",
    "#     decoder_out = model(\n",
    "#                 encoder_out, \n",
    "#                 encoder_mask=encoder_mask, \n",
    "#                 decoder_mask=decoder_mask)\n",
    "#     if use_cls_token:\n",
    "#         enc_cls_token = encoder_out[:, :1, :]\n",
    "#         encoder_patches = encoder_out[:, 1:, :]\n",
    "#         dec_cls_token = decoder_out[:, :1, :]\n",
    "#         decoder_patches = decoder_out[:, 1:, :]\n",
    "#         print(\"\\nenc_cls_token\", enc_cls_token.shape)\n",
    "#         print(\"encoder_patches\", encoder_patches.shape)\n",
    "#         print(\"dec_cls_token\", dec_cls_token.shape)\n",
    "#         print(\"decoder_patches\", decoder_patches.shape)\n",
    "\n",
    "# aug_transform = utils.DataPrepper(\n",
    "#     masking_strategy=\"conservative\",\n",
    "#     patch_depth=patch_size,\n",
    "#     patch_height=patch_size,\n",
    "#     patch_width=patch_size,\n",
    "#     frame_patch_size=frame_patch_size,\n",
    "# )\n",
    "# function to select random num_frames from sample and obtain brain-positive patches\n",
    "aug_transform = utils.DataPrepper(\n",
    "    num_frames=num_frames*2,\n",
    "    masking_strategy=masking_strategy,\n",
    "    patch_depth=patch_depth,\n",
    "    patch_height=patch_height,\n",
    "    patch_width=patch_width,\n",
    "    frame_patch_size=frame_patch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b0ce2",
   "metadata": {},
   "source": [
    "## Add \"linear\" probe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a8d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input_dim, h=256, num_classes=8):\n",
    "        super(LinearProbe, self).__init__()\n",
    "        # self.classifier = nn.Linear(input_dim, num_classes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(input_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(input_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c055614",
   "metadata": {},
   "source": [
    "# Create dataset and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd0398-0eb8-464e-8bb8-13e7ce1a8480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_and_continue(exn):\n",
    "    \"\"\"Call in an exception handler to ignore any exception, issue a warning, and continue.\"\"\"\n",
    "    print(f'Handling webdataset error ({repr(exn)}). Ignoring.')\n",
    "    return True\n",
    "\n",
    "def filter_corrupted_images(sample):\n",
    "    \"\"\"If all the required files are not present don't use them.\"\"\"\n",
    "    correct_data = (\"func.npy\" in sample)\n",
    "    return correct_data\n",
    "\n",
    "### ================      Train Dataset and DataLoader    ====================\n",
    "from braceexpand import braceexpand\n",
    "print(train_urls)\n",
    "\n",
    "if is_s3:\n",
    "    expanded_urls = [f\"pipe:aws s3 cp {url} -\" for pattern in train_urls for url in braceexpand(pattern)]\n",
    "else:\n",
    "    expanded_urls = [str(url) for pattern in train_urls for url in braceexpand(pattern)]\n",
    "# train_data = (\n",
    "#     wds.WebDataset(expanded_urls, resampled=True, nodesplitter=wds.split_by_node, handler=log_and_continue)\n",
    "#     .shuffle(100, initial=100, rng=random.Random(seed))\n",
    "#     .select(filter_corrupted_images)\n",
    "#     .decode(\"torch\")\n",
    "#     .rename(key=\"__key__\", func=\"func.npy\")\n",
    "#     .to_tuple(*(\"key\",\"func\"))\n",
    "# )\n",
    "train_data = (\n",
    "    wds.WebDataset(expanded_urls, resampled=True, nodesplitter=wds.split_by_node, handler=log_and_continue)\n",
    "    .shuffle(100, initial=100, rng=random.Random(seed))\n",
    "    .select(filter_corrupted_images)\n",
    "    .decode(\"torch\")\n",
    ")\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "#    .map_dict(func=utils.numpy_decoder)\n",
    "# train_dl = wds.WebLoader(\n",
    "#     train_data.batched(batch_size), \n",
    "#     pin_memory=True,\n",
    "#     shuffle=False,\n",
    "#     batch_size=None,\n",
    "#     num_workers=num_workers, \n",
    "#     persistent_workers=num_workers>0,\n",
    "# ).with_epoch(num_samples_per_epoch//batch_size)\n",
    "\n",
    "if is_s3:\n",
    "    expanded_urls = [f\"pipe:aws s3 cp {url} -\" for pattern in test_urls for url in braceexpand(pattern)]\n",
    "else:\n",
    "    expanded_urls = [str(url) for pattern in test_urls for url in braceexpand(pattern)]\n",
    "\n",
    "test_data = (\n",
    "    wds.WebDataset(expanded_urls, resampled=True, nodesplitter=wds.split_by_node, handler=log_and_continue)\n",
    "    .shuffle(100, initial=100, rng=random.Random(seed))\n",
    "    .select(filter_corrupted_images)\n",
    "    .decode(\"torch\")\n",
    "    .rename(key=\"__key__\", func=\"func.npy\")\n",
    "    .to_tuple(*(\"key\",\"func\"))\n",
    ")\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d05a32-12eb-494a-82df-dce4c7e9924c",
   "metadata": {},
   "source": [
    "### Check data loaders work and calculate number of iterations per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e7c0b-f58e-4c35-80fe-1284ee0e3966",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not distributed:\n",
    "    start_time = time.time() \n",
    "    num_it = 2\n",
    "    print(f\"Yielding {num_it} batches\")\n",
    "    \n",
    "    for i, batch in enumerate(train_dl):\n",
    "        print(\"iter\",i)\n",
    "        key, input_func = batch\n",
    "        if i >= (num_it-1):\n",
    "            break\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    print(\"input_func\", input_func.shape)\n",
    "\n",
    "    end_time = time.time()  \n",
    "    execution_time = end_time - start_time  \n",
    "    print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c6fba-5494-4964-b03d-e01c3afe47db",
   "metadata": {},
   "source": [
    "# Playing with the data, visualization of patching + masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd7913-707a-44ba-b031-afc883fb357c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if masking_strategy==\"MNI\":\n",
    "    # MNI_brain = nib.load(f\"/weka/home-alexnguyen/mamba_fmri/fMRI-MAE/cache/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "    MNI_brain = nib.load(f\"/scratch/gpfs/qanguyen/mamba_fmri/fMRI-MAE/cache/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "    brain_pos_voxels = MNI_brain[6:94,8:112,10:82]\n",
    "    brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "    brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "    batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "    mask_idx = torch.where(batch_positive_approx)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if utils.is_interactive():\n",
    "#     # extract func volumes and their reference mean and standard deviation volumes\n",
    "#     if masking_strategy==\"MNI\":\n",
    "#         func, _ = aug_transform(input_func)\n",
    "#     else:\n",
    "#         func, brain_pos_voxels = aug_transform(input_func)\n",
    "#         brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "#         brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "#     func = func.reshape(-1, num_frames, func.shape[-3], func.shape[-2], func.shape[-1])\n",
    "#     func = func.unsqueeze(1)  # add empty first dimension to serve as 1d channel dimension\n",
    "\n",
    "#     # patchify func samples\n",
    "#     print(\"func\", func.shape)\n",
    "#     patches = model.patchify(func)\n",
    "#     print(\"patches\", patches.shape)\n",
    "#     patches_vit = rearrange(patches, \"b ... d -> b (...) d\")\n",
    "#     print(\"patches_vit\", patches_vit.shape)\n",
    "#     print(\"num patches in one timepoint\", patches_vit.shape[1] // num_frames)\n",
    "\n",
    "#     # start by masking everything (aka include nothing)\n",
    "#     tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "#     # approximate brain positive patches for the whole batch\n",
    "#     batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "#     mask_idx_candidates = torch.where(batch_positive_approx)[0]\n",
    "#     mask_idx_candidates = mask_idx_candidates[torch.randperm(len(mask_idx_candidates))]\n",
    "#     print(\"Percentage of brain positive patches\", len(mask_idx_candidates) / len(batch_positive_approx))\n",
    "#     tube_idx = mask_idx_candidates[: int(num_patches / num_frames * (1 - tube_start_masking_ratio))]\n",
    "#     print(\"num tube patches =\", len(tube_idx))\n",
    "#     tube_mask[tube_idx] = True  # Trues mean to include the patch, False means to remove the patch\n",
    "#     tube_mask = tube_mask.tile(num_frames)  # repeat masking for the other timepoints\n",
    "#     print(\"tube mask percent\", tube_mask.sum().item() / len(tube_mask))\n",
    "\n",
    "#     # create decoder mask similar to tube mask, but ensure no overlap\n",
    "#     decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)  # start by masking everything (aka include nothing)\n",
    "#     remaining_mask_idx = mask_idx_candidates[int(num_patches / num_frames * (1 - tube_start_masking_ratio)) :]  # brain positive tokens not selected for the encoder tokens\n",
    "#     decoder_mask_idx = remaining_mask_idx[:int(num_patches / num_frames * (1 - decoder_mask_ratio))]\n",
    "#     print(\"num decoder patches =\", len(decoder_mask_idx))\n",
    "#     decoder_mask[decoder_mask_idx] = True\n",
    "#     decoder_mask = decoder_mask.tile(num_frames)  # repeat masking for the other timepoints\n",
    "#     print(\"decoder_mask percent\", decoder_mask.sum().item() / len(decoder_mask))\n",
    "\n",
    "#     # apply masks to patches_vit\n",
    "#     tube_patches_vit = copy.deepcopy(patches_vit.detach())\n",
    "#     decoder_patches_vit = copy.deepcopy(patches_vit.detach())\n",
    "#     # tube_patches_vit[:, tube_mask] = 1\n",
    "#     # decoder_patches_vit[:, decoder_mask] = 1\n",
    "#     tube_patches_vit[:, ~tube_mask] = 0\n",
    "#     decoder_patches_vit[:, ~decoder_mask] = 0\n",
    "\n",
    "#     # undo patchification so we can visualize\n",
    "#     tube_unpatches = rearrange(\n",
    "#         tube_patches_vit,\n",
    "#         \"b (f d h w) c -> b f d h w c\",\n",
    "#         d=img_size[0]//patch_size,\n",
    "#         h=img_size[1]//patch_size,\n",
    "#         w=img_size[2]//patch_size,\n",
    "#     )\n",
    "#     decoder_unpatches = rearrange(\n",
    "#         decoder_patches_vit,\n",
    "#         \"b (f d h w) c -> b f d h w c\",\n",
    "#         d=img_size[0]//patch_size,\n",
    "#         h=img_size[1]//patch_size,\n",
    "#         w=img_size[2]//patch_size,\n",
    "#     )\n",
    "#     print(\"tube_unpatches\", tube_unpatches.shape)\n",
    "#     print(\"decoder_unpatches\", decoder_unpatches.shape)\n",
    "    \n",
    "#     encoder_func = rearrange(\n",
    "#         tube_unpatches,\n",
    "#         \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "#         b=len(func),\n",
    "#         f=num_frames,\n",
    "#         d=img_size[0] // patch_size,\n",
    "#         h=img_size[1] // patch_size,\n",
    "#         w=img_size[2] // patch_size,\n",
    "#         pd=patch_size,\n",
    "#         ph=patch_size,\n",
    "#         pw=patch_size,\n",
    "#         pf=frame_patch_size,\n",
    "#     )\n",
    "#     decoder_func = rearrange(\n",
    "#         decoder_unpatches,\n",
    "#         \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "#         b=len(func),\n",
    "#         f=num_frames,\n",
    "#         d=img_size[0] // patch_size,\n",
    "#         h=img_size[1] // patch_size,\n",
    "#         w=img_size[2] // patch_size,\n",
    "#         pd=patch_size,\n",
    "#         ph=patch_size,\n",
    "#         pw=patch_size,\n",
    "#         pf=frame_patch_size,\n",
    "#     )\n",
    "#     print(\"encoder_func\", encoder_func.shape)\n",
    "#     print(\"decoder_func\", decoder_func.shape)\n",
    "    \n",
    "#     brain_pos_vit = copy.deepcopy(patches_vit.detach())\n",
    "#     brain_pos_vit[:,batch_positive_approx.repeat(num_frames)] = 1\n",
    "#     brain_pos_vit[:,~batch_positive_approx.repeat(num_frames)] = 0\n",
    "#     brain_pos_unpatches = rearrange(\n",
    "#         brain_pos_vit,\n",
    "#         \"b (f d h w) c -> b f d h w c\",\n",
    "#         d=img_size[0]//patch_size,\n",
    "#         h=img_size[1]//patch_size,\n",
    "#         w=img_size[2]//patch_size,\n",
    "#     )\n",
    "#     brain_pos_func = rearrange(\n",
    "#         brain_pos_unpatches,\n",
    "#         \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "#         b=len(func),\n",
    "#         f=num_frames,\n",
    "#         d=img_size[0] // patch_size,\n",
    "#         h=img_size[1] // patch_size,\n",
    "#         w=img_size[2] // patch_size,\n",
    "#         pd=patch_size,\n",
    "#         ph=patch_size,\n",
    "#         pw=patch_size,\n",
    "#         pf=frame_patch_size,\n",
    "#     )\n",
    "\n",
    "#     # Visualize\n",
    "#     idx = 0\n",
    "#     print(\"original func\")\n",
    "#     display(transforms.ToPILImage()(utils.reshape_to_2d(func[idx].clamp(0,1))))\n",
    "    \n",
    "#     print(\"\\nbrain-positive patches\")\n",
    "#     display(transforms.ToPILImage()(utils.reshape_to_2d(brain_pos_func[idx].clamp(0,1))))\n",
    "\n",
    "#     print(\"\\nencoder func\")\n",
    "#     display(transforms.ToPILImage()(utils.reshape_to_2d(encoder_func[idx].clamp(0,1))))\n",
    "\n",
    "#     print(\"\\ndecoder func\")\n",
    "#     display(transforms.ToPILImage()(utils.reshape_to_2d(decoder_func[idx].clamp(0,1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604b380-0126-49b2-ab4e-59b1e463d6ad",
   "metadata": {},
   "source": [
    "# FSDP / optimizer / saving functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f057be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:    \n",
    "    # my_auto_wrap_policy = functools.partial(\n",
    "    #     size_based_auto_wrap_policy, min_num_params=200000\n",
    "    # )\n",
    "    from mamba_ssm.modules.mamba_simple import Block\n",
    "    my_auto_wrap_policy = functools.partial(\n",
    "        transformer_auto_wrap_policy, \n",
    "        transformer_layer_cls={\n",
    "            Block, # <--- Your Transformer layer class\n",
    "        },\n",
    "    )\n",
    "    print(f\"\\nPrepping FSDP on {global_rank} {node}...\\n\")\n",
    "    model = FSDP(\n",
    "        model,\n",
    "        sharding_strategy=ShardingStrategy.HYBRID_SHARD,\n",
    "        auto_wrap_policy=my_auto_wrap_policy,\n",
    "        use_orig_params=False,\n",
    "        cpu_offload=None, #CPUOffload(offload_params=True)\n",
    "        sync_module_states=True,\n",
    "        limit_all_gathers=True, # See https://github.com/pytorch/pytorch/issues/91165\n",
    "        device_id=device,\n",
    "    )\n",
    "    print(f\"\\nSuccessfully loaded FSDP model to device on global_rank {global_rank}\\n\")\n",
    "    dist.barrier()\n",
    "    print(f\"\\nSuccessfully loaded FSDP model to device on global_rank {global_rank}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da73c08-ca61-48ef-9e63-b70db6f07a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "if distributed:\n",
    "    max_lr = max_lr * global_batch_size\n",
    "    print(f\"multiply lr {max_lr} by global batch size: max_lr={max_lr}\")\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "num_iterations_per_epoch = num_samples_per_epoch // global_batch_size\n",
    "\n",
    "total_steps = num_epochs * num_iterations_per_epoch * num_devices\n",
    "print(\"total_steps\", total_steps)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    ")\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f674d-7cd4-49d1-bd8d-c697d9614f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_ckpt_path = outdir+f'/last.pth'\n",
    "\n",
    "def save_ckpt(model,tag=\"last\"):\n",
    "    if distributed: dist.barrier()\n",
    "    model_states = model.state_dict()\n",
    "    if global_rank == 0:\n",
    "        os.makedirs(outdir,exist_ok=True)\n",
    "        ckpt_path = outdir+f'/{tag}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_states,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'config': dict(config)\n",
    "        }, ckpt_path)\n",
    "        print(f\"\\n---saved {ckpt_path}!---\\n\")\n",
    "\n",
    "def resume_ckpt(model, optimizer, lr_scheduler, device, ckpt_path=default_ckpt_path):\n",
    "    if global_rank == 0:\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        epoch = 0\n",
    "    if distributed: dist.barrier()\n",
    "    torch.cuda.empty_cache()\n",
    "    return model, optimizer, lr_scheduler, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b4262-435b-4872-ab4a-424cb9dfd37a",
   "metadata": {},
   "source": [
    "# Start wandb (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56431733-4fb5-4072-840e-22536608f9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if utils.is_interactive():\n",
    "    ckpt_saving = False\n",
    "    wandb_log = False\n",
    "if global_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found' \n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "    #   \"encoder_model\": encoder_model,\n",
    "    #   \"decoder_model\": decoder_model,\n",
    "      \"tube_start_masking_ratio\": tube_start_masking_ratio,\n",
    "      \"tube_end_masking_ratio\": tube_end_masking_ratio,\n",
    "      \"decoder_mask_ratio\": decoder_mask_ratio,\n",
    "      \"num_frames\": num_frames,\n",
    "      \"patch_size\": patch_size,\n",
    "      \"frame_patch_size\": frame_patch_size,\n",
    "      \"use_contrastive_loss\": use_contrastive_loss,\n",
    "      \"use_cls_token\": use_cls_token,\n",
    "      \"constrastive_loss_weight\": constrastive_loss_weight,\n",
    "      \"num_params\": num_params,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_urls\": train_urls,\n",
    "      \"test_urls\": test_urls,\n",
    "    }\n",
    "    wandb_config.update(config) # add all config parameters to wandb_config\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        # id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "        group=wandb_group_name\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a5055-8afd-468a-93bf-32f94bd1d042",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e394dd-2745-41fa-a5aa-54b7b1373a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "lrs, train_losses, recon_losses, contrastive_losses, test_losses = [], [], [], [], []\n",
    "cos_sim_encoder_output, cos_sim_decoder_output, cos_sim_encoder_output_patchwise = [], [], []\n",
    "probe_losses, probe_accs, test_losses, test_accs = [], [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a5605-fd81-41d4-964e-1752ed6e1289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# debug=True\n",
    "if (resume_from_ckpt==True):# and (debug!=True):\n",
    "    if os.path.exists(default_ckpt_path):\n",
    "        print(f\"Resuming from {default_ckpt_path}...\")\n",
    "        model, optimizer, lr_scheduler, epoch = resume_ckpt(model, optimizer, lr_scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e258c4-8477-48ed-81af-d415cee246f3",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if distributed: dist.barrier()\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "crossentropy = nn.CrossEntropyLoss()\n",
    "\n",
    "if use_contrastive_loss:\n",
    "    contrastive_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs)\n",
    "model.train()\n",
    "progress_bar = tqdm(range(epoch, num_epochs), disable=global_rank!=0, desc=\"Overall\")\n",
    "for epoch in progress_bar:\n",
    "    # get the masking ratio for the current epoch\n",
    "    tube_mask_ratio = utils.get_masking_ratio(\n",
    "        current_epoch=epoch, \n",
    "        total_epochs=num_epochs, \n",
    "        start_masking_ratio=tube_start_masking_ratio, \n",
    "        end_masking_ratio=tube_end_masking_ratio\n",
    "    )\n",
    "    num_decoder_patches = int(num_patches * tube_mask_ratio)\n",
    "    with torch.cuda.amp.autocast(dtype=data_type):\n",
    "        model.train()\n",
    "        for train_i, batch in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            key, input_func = batch \n",
    "            if masking_strategy==\"MNI\":\n",
    "                func, _ = aug_transform(input_func)\n",
    "            else:\n",
    "                func, brain_pos_voxels = aug_transform(input_func)\n",
    "                brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "                brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "                batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "                mask_idx = torch.where(batch_positive_approx)[0]\n",
    "                \n",
    "            if is_random_tube_mask_ratio==True: tube_mask_ratio = np.random.uniform(low=0.001,high=0.75) # random tube mask ratio to train with different #s of patches\n",
    "            func = func.reshape(-1, num_frames, func.shape[-3], func.shape[-2], func.shape[-1]).float()\n",
    "            func = func.unsqueeze(1).clamp(0,1).to(device)\n",
    "\n",
    "            # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "            tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            \n",
    "            mask_idx_candidates = mask_idx[torch.randperm(len(mask_idx))]\n",
    "            num_masked_voxels = min(int(len(mask_idx_candidates) * (1 - tube_mask_ratio)), len(mask_idx_candidates) - 1)\n",
    "            tube_idx = mask_idx_candidates[:num_masked_voxels]\n",
    "            tube_mask[tube_idx] = True\n",
    "            tube_mask = tube_mask.tile(num_frames)#.to(device)\n",
    "             \n",
    "\n",
    "            # create decoder mask\n",
    "            decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "            remaining_mask_idx = mask_idx_candidates[num_masked_voxels:]\n",
    "            decoder_mask_idx = remaining_mask_idx#[:int(num_patches / num_frames * (1 - decoder_mask_ratio))]\n",
    "            decoder_mask[decoder_mask_idx] = True\n",
    "            decoder_mask = decoder_mask.tile(num_frames)#.to(device)\n",
    " \n",
    "            # decoder_mask = ~tube_mask\n",
    "            \n",
    "            # encode the tube patches\n",
    "            # encoder_out = model(func, encoder_mask=tube_mask)\n",
    "            encoder_out = model(func, mask_idx=mask_idx.tile(num_frames), \n",
    "                                    encoder_mask=tube_mask, decoder_mask=decoder_mask, is_encode_phase=True)\n",
    "            if use_cls_token:\n",
    "                enc_cls_token = encoder_out[:,:1,:]\n",
    "            # print (\"tube_mask\", tube_mask.shape)\n",
    "            \n",
    "            # decode both the encoder_out patches and masked decoder patches\n",
    "            decoder_out = model(encoder_out, \n",
    "                                mask_idx=mask_idx.tile(num_frames), \n",
    "                                encoder_mask=tube_mask, decoder_mask=decoder_mask,\n",
    "                                is_encode_phase=False)\n",
    "            # decoder_out = model(func, encoder_mask=tube_mask, decoder_mask=decoder_mask)\n",
    "            # subset only the reconstructed decoder patches\n",
    "            # print (\"decoder_out\", decoder_out.shape, \"num_decoder_patches\", num_decoder_patches)\n",
    "            # the decoder mask gets concatenated to the end\n",
    "            # output = decoder_out[:, -decoder_mask.sum():].clamp(0,1)\n",
    "            output = decoder_out.clamp(0,1)\n",
    "            \n",
    "\n",
    "            # compare to ground truth and calculate loss \n",
    "            target_patches = model.patchify(func)\n",
    "            target_patches_vit = rearrange(target_patches, \"b ... d -> b (...) d\")\n",
    "            # target = target_patches_vit[:, decoder_mask]\n",
    "            target = target_patches_vit[:, mask_idx.tile(num_frames)]\n",
    "            #print(\"encoder_out\",encoder_out.shape,\"output\",output.shape,\"target\",target.shape)\n",
    "            #print(\"output\")\n",
    "            #print(output[0,:10,:10])\n",
    "\n",
    "            #print(target[0,:10,:10])\n",
    "            #plt.imshow(torch.corrcoef(output[0]).detach().cpu(),vmin=0,vmax=1 )\n",
    "            #plt.show()\n",
    "            #plt.hist(torch.corrcoef(output[0]).detach().cpu().flatten())\n",
    "            #plt.show()\n",
    "            #print(\"target\")\n",
    "            #plt.imshow(torch.corrcoef(target[0]).detach().cpu(),vmin=0,vmax=1 )\n",
    "            #plt.show()\n",
    "            #plt.hist(torch.corrcoef(target[0]).detach().cpu().flatten())\n",
    "            #plt.show()\n",
    "            \n",
    "            # print(\"torch.corrcoef(output[0])\",torch.corrcoef(output[0]))\n",
    "            # ## visualize\n",
    "            # idx=0\n",
    "            # decode_vis = target_patches_vit.clone().detach()#torch.zeros_like(target_patches_vit)\n",
    "            # print(\"output\",output.shape, \"-decoder_mask.sum()\",-decoder_mask.sum(), \"decode_vis[:, mask_idx.tile(num_frames)]\", decode_vis[:, mask_idx.tile(num_frames)].shape)\n",
    "            # # decode_vis[:, decoder_mask] = output.to(decode_vis.device).to(decode_vis.dtype)\n",
    "            # decode_vis[:, mask_idx.tile(num_frames), :] = output.to(decode_vis.device).to(decode_vis.dtype)\n",
    "            # decoder_unpatches = rearrange(\n",
    "            #     decode_vis,\n",
    "            #     \"b (f d h w) c -> b f d h w c\",\n",
    "            #     d=img_size[0]//patch_size,\n",
    "            #     h=img_size[1]//patch_size,\n",
    "            #     w=img_size[2]//patch_size,\n",
    "            # )\n",
    "            # decoder_func = rearrange(\n",
    "            #     decoder_unpatches,\n",
    "            #     \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "            #     b=batch_size,\n",
    "            #     f=num_frames,\n",
    "            #     d=img_size[0]//patch_size,\n",
    "            #     h=img_size[1]//patch_size,\n",
    "            #     w=img_size[2]//patch_size,\n",
    "            #     pd=patch_size,\n",
    "            #     ph=patch_size,\n",
    "            #     pw=patch_size,\n",
    "            #     pf=frame_patch_size,\n",
    "            # ) \n",
    "            \n",
    "            # # func_vis = torch.zeros_like(target_patches_vit)\n",
    "            # func_vis = target_patches_vit.clone().detach()\n",
    "            # print(\"output\",output.shape, \"-decoder_mask.sum()\",-decoder_mask.sum(), \"decode_vis[:, mask_idx.tile(num_frames)]\", decode_vis[:, mask_idx.tile(num_frames)].shape)\n",
    "            # # decode_vis[:, decoder_mask] = output.to(decode_vis.device).to(decode_vis.dtype)\n",
    "            # # func_vis[:, mask_idx.tile(num_frames)] = target.to(func_vis.device).to(func_vis.dtype)\n",
    "            # func_unpatches = rearrange(\n",
    "            #     func_vis,\n",
    "            #     \"b (f d h w) c -> b f d h w c\",\n",
    "            #     d=img_size[0]//patch_size,\n",
    "            #     h=img_size[1]//patch_size,\n",
    "            #     w=img_size[2]//patch_size,\n",
    "            # )\n",
    "            # func_func = rearrange(\n",
    "            #     func_unpatches,\n",
    "            #     \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "            #     b=batch_size,\n",
    "            #     f=num_frames,\n",
    "            #     d=img_size[0]//patch_size,\n",
    "            #     h=img_size[1]//patch_size,\n",
    "            #     w=img_size[2]//patch_size,\n",
    "            #     pd=patch_size,\n",
    "            #     ph=patch_size,\n",
    "            #     pw=patch_size,\n",
    "            #     pf=frame_patch_size,\n",
    "            # ) \n",
    "            \n",
    "            # orig_image =  (utils.reshape_to_2d(func[idx]))\n",
    "            # recon_image = utils.reshape_to_2d(decoder_func[idx])\n",
    "            \n",
    "            # print(\"orig_image\",orig_image.shape, orig_image.min(),orig_image.max(), \"recon_image\",recon_image.shape,recon_image.min(),recon_image.max(),\"recon_image.detach().cpu()[:,random_start:random_start+100]\",recon_image.detach().cpu()[:,random_start:random_start+100].shape)\n",
    "            # print(\"decoder_func\",decoder_func.shape,\"func\",func.shape,\"func_func\",func_func.shape)\n",
    "            # combined_image = orig_image.clone()\n",
    "            # combined_image[recon_image!=0] = recon_image[recon_image!=0]\n",
    "\n",
    "            \n",
    "            # # orig_image = transforms.ToPILImage()(orig_image[:,random_start:random_start+100])\n",
    "            # # recon_image = transforms.ToPILImage()(recon_image[:,random_start:random_start+100])\n",
    "            # # combined_image = transforms.ToPILImage()(combined_image[:,random_start:random_start+100])\n",
    "            # print(\"target_patches_vit[idx]\",target[idx].shape,\"output[idx]\",output[idx].shape)\n",
    "            # random_start = np.random.randint(target.shape[1]-400)\n",
    "            # random_start_y=500\n",
    "            # plt.imshow( target[idx].detach().cpu(),vmin=0,vmax=1 )# [random_start:random_start+100,random_start_y:random_start_y+100]\n",
    "            # plt.show()\n",
    "            # plt.imshow(output[idx].detach().cpu(),vmin=0,vmax=1)#[random_start:random_start+100,random_start_y:random_start_y+100]\n",
    "            # plt.show()\n",
    "            # random_start = np.random.randint(recon_image.shape[1]-400)\n",
    "            #plt.imshow( (utils.reshape_to_2d(func_func[idx])).detach().cpu() [:,random_start:random_start+100] ,vmin=0,vmax=1)\n",
    "            #plt.show()\n",
    "            #plt.imshow(utils.reshape_to_2d(decoder_func[idx]).detach().cpu()[:,random_start:random_start+100],vmin=0,vmax=1)\n",
    "            #plt.show()\n",
    "            #plt.imshow(combined_image.detach().cpu() [:,random_start:random_start+100] ,vmin=0,vmax=1)\n",
    "            #plt.show() \n",
    "            \n",
    "            # print(\"func\",func.shape,\"output\",output.shape,\"target\",target.shape)\n",
    "            loss = mse(output, target)\n",
    "            # print ( \"decoder_mask.sum()\", decoder_mask.sum(), \"loss\", loss.item())\n",
    "            recon_losses.append(loss.item())\n",
    " \n",
    "            # contrastive loss\n",
    "            if use_contrastive_loss:\n",
    "                enc_norm = nn.functional.normalize(encoder_out.flatten(1), dim=-1)\n",
    "                cosine_similarities = enc_norm @ enc_norm.T\n",
    "                \n",
    "                softmax_scores = nn.functional.softmax(cosine_similarities / contrastive_temps[epoch], dim=-1)\n",
    "                contrastive_loss = nn.functional.cross_entropy(softmax_scores, torch.arange(len(cosine_similarities)).to(device))\n",
    "                \n",
    "                loss += constrastive_loss_weight * contrastive_loss\n",
    "                contrastive_losses.append(contrastive_loss.item())\n",
    "\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            train_losses.append(loss.item())\n",
    "            # print(\"loss\",loss.item(), \"tube_mask_ratio\", tube_mask_ratio)\n",
    "            lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "            if (train_i >= (num_iterations_per_epoch-1)) or debug:\n",
    "                print(\"train_i\", train_i, \"local_rank\", local_rank, \"global_rank\", global_rank, \"debug\", debug, \"num_iterations_per_epoch\", num_iterations_per_epoch)\n",
    "                break\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for test_i, batch in enumerate(test_dl):\n",
    "                key, input_func = batch \n",
    "\n",
    "                if masking_strategy==\"MNI\":\n",
    "                    func, _ = aug_transform(input_func)\n",
    "                else:\n",
    "                    func, brain_pos_voxels = aug_transform(input_func)\n",
    "                    brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "                    brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "                    batch_positive_approx = (brain_pos_pats_vit > 0)\n",
    "                    mask_idx = torch.where(batch_positive_approx)[0]\n",
    "                    \n",
    "                if is_random_tube_mask_ratio==True: tube_mask_ratio = np.random.uniform(low=0.001,high=0.75) # random tube mask ratio to train with different #s of patches\n",
    "                func = func.reshape(-1, num_frames, func.shape[-3], func.shape[-2], func.shape[-1]).float()\n",
    "                func = func.unsqueeze(1).clamp(0,1).to(device)\n",
    "    # create tube mask (i.e., a mask that is the same for all frames/timepoints)\n",
    "                tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "\n",
    "                \n",
    "                mask_idx_candidates = mask_idx[torch.randperm(len(mask_idx))]\n",
    "                num_masked_voxels = min(int(len(mask_idx_candidates) * (1 - tube_mask_ratio)), len(mask_idx_candidates) - 1)\n",
    "                tube_idx = mask_idx_candidates[:num_masked_voxels]\n",
    "                tube_mask[tube_idx] = True\n",
    "                tube_mask = tube_mask.tile(num_frames)#.to(device)\n",
    "                \n",
    "\n",
    "                # create decoder mask \n",
    "                decoder_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "                remaining_mask_idx = mask_idx_candidates[num_masked_voxels:]\n",
    "                decoder_mask_idx = remaining_mask_idx#[:int(num_patches / num_frames * (1 - decoder_mask_ratio))]\n",
    "                decoder_mask[decoder_mask_idx] = True\n",
    "                decoder_mask = decoder_mask.tile(num_frames)#.to(device)\n",
    "\n",
    "                # decoder_mask = ~tube_mask\n",
    "                \n",
    "                # encode the tube patches\n",
    "                encoder_out = model(func, mask_idx=mask_idx.tile(num_frames), \n",
    "                                    encoder_mask=tube_mask, decoder_mask=decoder_mask, is_encode_phase=True)\n",
    "                if use_cls_token:\n",
    "                    enc_cls_token = encoder_out[:,:1,:]\n",
    "                # print (\"test encoder_out\", encoder_out.shape, \"test decoder_mask.sum()\", decoder_mask.sum())\n",
    "                # print (\"tube_mask\", tube_mask.shape)\n",
    "                # print (\"encoder_out\", encoder_out.shape)\n",
    "                # decode both the encoder_out patches and masked decoder patches\n",
    "                decoder_out = model(encoder_out, \n",
    "                                    mask_idx=mask_idx.tile(num_frames), \n",
    "                                    encoder_mask=tube_mask, decoder_mask=decoder_mask,\n",
    "                                    is_encode_phase=False)\n",
    "                # decoder_out = model(func, encoder_mask=tube_mask, decoder_mask=decoder_mask)\n",
    "                # subset only the reconstructed decoder patches\n",
    "                # print (\"decoder_out\", decoder_out.shape, \"num_decoder_patches\", num_decoder_patches)\n",
    "                # the decoder mask gets concatenated to the end\n",
    "                # output = decoder_out[:, -decoder_mask.sum():].clamp(0,1)\n",
    "                output = decoder_out.clamp(0,1)\n",
    "            \n",
    "\n",
    "                # compare to ground truth and calculate loss \n",
    "                target_patches = model.patchify(func)\n",
    "                target_patches_vit = rearrange(target_patches, \"b ... d -> b (...) d\")\n",
    "                # target = target_patches_vit[:, decoder_mask]\n",
    "                target = target_patches_vit[:, mask_idx.tile(num_frames)]\n",
    "                loss = mse(output, target)\n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "                if test_i >= (test_num_iterations_per_epoch-1):\n",
    "                    break\n",
    " \n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(train_losses[-(train_i + 1) :]),\n",
    "            \"train/recon_losses\": np.mean(recon_losses[-(train_i + 1) :]),\n",
    "            \"train/contrastive_losses\": np.mean(contrastive_losses[-(train_i + 1) :]),\n",
    "            \"train/num_steps\": len(recon_losses),\n",
    "            \"test/loss\": np.mean(test_losses[-(test_i + 1) :]),\n",
    "            \"test/num_steps\": len(test_losses),\n",
    "            \"lr\": np.mean(lrs[-(train_i + 1) :]),\n",
    "            \"epoch\": epoch,\n",
    "            \"tube_mask_ratio\": tube_mask_ratio,\n",
    "            \"decoder_mask_ratio\": decoder_mask_ratio,\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        if distributed: print(logs)\n",
    "\n",
    "#         if global_rank==0:\n",
    "           # Plot progress (first sample in batch)\n",
    "#             with torch.no_grad():\n",
    "#                 if utils.is_interactive():\n",
    "#                     idx = 0\n",
    "#                     if epoch % 5 == 0:\n",
    "#                         decode_vis = torch.zeros_like(target_patches_vit)\n",
    "#                         decode_vis[:, decoder_mask] = output.to(decode_vis.device).to(decode_vis.dtype)\n",
    "#                         decoder_unpatches = rearrange(\n",
    "#                             decode_vis,\n",
    "#                             \"b (f d h w) c -> b f d h w c\",\n",
    "#                             d=img_size[0]//patch_size,\n",
    "#                             h=img_size[1]//patch_size,\n",
    "#                             w=img_size[2]//patch_size,\n",
    "#                         )\n",
    "#                         decoder_func = rearrange(\n",
    "#                             decoder_unpatches,\n",
    "#                             \"b f d h w (pd ph pw pf c) -> b c (f pf) (d pd) (h ph) (w pw)\",\n",
    "#                             b=batch_size*2,\n",
    "#                             f=num_frames,\n",
    "#                             d=img_size[0]//patch_size,\n",
    "#                             h=img_size[1]//patch_size,\n",
    "#                             w=img_size[2]//patch_size,\n",
    "#                             pd=patch_size,\n",
    "#                             ph=patch_size,\n",
    "#                             pw=patch_size,\n",
    "#                             pf=frame_patch_size,\n",
    "#                         )\n",
    "#                         orig_image = utils.reshape_to_2d(func[idx])\n",
    "#                         recon_image = utils.reshape_to_2d(decoder_func[idx])\n",
    "    \n",
    "#                         combined_image = orig_image.clone()\n",
    "#                         combined_image[recon_image!=0] = recon_image[recon_image!=0]\n",
    "                        \n",
    "#                         random_start = np.random.randint(recon_image.shape[1]-400)\n",
    "#                         orig_image = transforms.ToPILImage()(orig_image[:,random_start:random_start+400])\n",
    "#                         recon_image = transforms.ToPILImage()(recon_image[:,random_start:random_start+400])\n",
    "#                         combined_image = transforms.ToPILImage()(combined_image[:,random_start:random_start+400])\n",
    "    \n",
    "#                         if wandb_log:\n",
    "#                             logs[f\"train/orig\"] = wandb.Image(orig_image, caption=f\"epoch{epoch:03d}\")\n",
    "#                             logs[f\"train/recon\"] = wandb.Image(recon_image, caption=f\"epoch{epoch:03d}\")\n",
    "#                             logs[f\"train/combined\"] = wandb.Image(combined_image, caption=f\"epoch{epoch:03d}\")\n",
    "#                         else:\n",
    "#                             # display(orig_image)\n",
    "#                             # display(recon_image)\n",
    "#                             display(combined_image)\n",
    "#             if wandb_log: wandb.log(logs)\n",
    "        \n",
    "        # wait for other GPUs to catch up if needed\n",
    "#         if distributed: dist.barrier()\n",
    "\n",
    "        # Save model checkpoint\n",
    "        if (ckpt_saving) and ((epoch % ckpt_interval == 0) or (epoch==num_epochs-1)) and (debug==False):\n",
    "            save_ckpt(model,\"last\")\n",
    "            \n",
    "        # wait for other GPUs to catch up if needed\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "if distributed:\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6e847-2511-4f69-aa70-f1471a5b7d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(recon_losses)\n",
    "plt.title(\"Training re-construction losses\")\n",
    "plt.show()\n",
    "if use_contrastive_loss:\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(contrastive_losses)\n",
    "    plt.title(\"Training contrastive losses\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mamba_fmri)",
   "language": "python",
   "name": "mamba_fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
