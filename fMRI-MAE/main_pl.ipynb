{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9348dca1-7608-48b1-bdbe-6fc2ac8fb892",
   "metadata": {},
   "source": [
    "## Read the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63dbc979-5f26-490e-8e4f-d1aa6d7bacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796f766c-95f2-45b2-a879-bfc110a21c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import torch\n",
    "import random\n",
    "import webdataset as wds\n",
    "# torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35b2cc54-99bd-44e5-93b2-dca5df4060d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tf32 data type is faster than standard float32\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# ### Multi-GPU config ###\n",
    "# device_count = torch.cuda.device_count()\n",
    "# print(f\"Number of available CUDA devices: {device_count}\")\n",
    "\n",
    "# local_rank = os.getenv('LOCAL_RANK')\n",
    "# if local_rank is None: \n",
    "#     local_rank = 0\n",
    "# else:\n",
    "#     local_rank = int(local_rank)\n",
    "# print(f\"LOCAL RANK={local_rank}\")\n",
    "\n",
    "# num_devices = os.getenv('NUM_GPUS')\n",
    "# if num_devices is None: \n",
    "#     num_devices = 1\n",
    "# else:\n",
    "#     num_devices = int(num_devices)\n",
    "# print(f\"NUM GPUS={num_devices}\")\n",
    "# distributed = True if num_devices>1 else False\n",
    "# if distributed: assert device_count==num_devices\n",
    "\n",
    "# node = os.getenv('SLURM_NODEID')\n",
    "# if node is None:\n",
    "#     node = 0\n",
    "# else:\n",
    "#     node = int(node)\n",
    "# print(f\"NODE={node}\")\n",
    "\n",
    "# global_rank = os.getenv('RANK')\n",
    "# if global_rank is None:\n",
    "#     global_rank = 0\n",
    "# else:\n",
    "#     global_rank = int(global_rank)\n",
    "# print(f\"GLOBAL RANK={global_rank}\")\n",
    "\n",
    "# world_size = os.getenv('WORLD_SIZE')\n",
    "# if world_size is None: \n",
    "#     world_size = 1\n",
    "# else:\n",
    "#     world_size = int(world_size)\n",
    "# print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "# if utils.is_interactive():\n",
    "#     # Following allows you to change functions in models.py or utils.py and \n",
    "#     # have this notebook automatically update with your revisions\n",
    "#     %load_ext autoreload\n",
    "#     %autoreload 2\n",
    "#     from tqdm.notebook import tqdm\n",
    "# else:\n",
    "#     from tqdm import tqdm\n",
    "\n",
    "# # Load parameters from yaml config\n",
    "# config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# # create global variables from the config\n",
    "# for attribute_name in config.keys():\n",
    "#     globals()[attribute_name] = config[f'{attribute_name}']\n",
    "\n",
    "# data_type = torch.float16 # change depending on your mixed_precision\n",
    "# # batch_size = global_batch_size // num_devices\n",
    "# global_batch_size = batch_size * world_size\n",
    "\n",
    "# # FSDP Setup\n",
    "# if distributed:\n",
    "#     import torch.distributed as dist\n",
    "#     import torch.multiprocessing as mp\n",
    "#     from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "#     from torch.distributed.fsdp.api import BackwardPrefetch, CPUOffload, ShardingStrategy\n",
    "#     import functools\n",
    "#     from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy, transformer_auto_wrap_policy\n",
    "#     print(\"starting init_process_group...\")\n",
    "#     dist.init_process_group(\"nccl\", rank=global_rank, world_size=world_size)\n",
    "#     print(f\"setting device to cuda:{local_rank}\")\n",
    "#     try:\n",
    "#         torch.cuda.set_device(local_rank)\n",
    "#         device = torch.device('cuda',local_rank)\n",
    "#         print(f\"\\nSuccessfully set cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "#     except Exception as error:        \n",
    "#         print(f\"\\nFAILED TO SET DEVICE cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "#         print(\"An exception occurred:\", error)\n",
    "        \n",
    "# else:\n",
    "#     device = torch.device('cuda')\n",
    "\n",
    "# print(\"PID of this process =\",os.getpid())\n",
    "# print(\"device =\", device, \"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ec9132-e791-422e-b1ff-ac0e1f4dbce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3629e195-395f-4237-8005-e05d315c54ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdir = os.path.abspath(f'../ckpts/{model_name}')\n",
    "# os.makedirs(outdir,exist_ok=True)\n",
    "# print(\"outdir\", outdir)\n",
    "# print(\"use_cls_token\", use_cls_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a1a1ed4-8f0d-45fe-a75f-0287dbdb18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_depth = patch_size[0]\n",
    "patch_height = patch_size[1]\n",
    "patch_width = patch_size[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5b352-b2de-4dfc-a265-d5ea1b8e9e52",
   "metadata": {},
   "source": [
    "## Define the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3255ab0-56fa-445b-a9bc-1b70edc4da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_transform = utils.DataPrepper(\n",
    "#     num_frames=num_frames*2,\n",
    "#     masking_strategy=masking_strategy,\n",
    "#     patch_depth=patch_depth,\n",
    "#     patch_height=patch_height,\n",
    "#     patch_width=patch_width,\n",
    "#     frame_patch_size=frame_patch_size,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "026a0453-975c-44e7-8c0e-32fe73b5634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_and_continue(exn):\n",
    "#     \"\"\"Call in an exception handler to ignore any exception, issue a warning, and continue.\"\"\"\n",
    "#     print(f'Handling webdataset error ({repr(exn)}). Ignoring.')\n",
    "#     return True\n",
    "\n",
    "# def filter_corrupted_images(sample):\n",
    "#     \"\"\"If all the required files are not present don't use them.\"\"\"\n",
    "#     correct_data = (\"func.npy\" in sample)\n",
    "#     return correct_data\n",
    "\n",
    "# ### ================      Train Dataset and DataLoader    ====================\n",
    "# from braceexpand import braceexpand\n",
    "# print(train_urls)\n",
    "# if is_s3:\n",
    "#     expanded_urls = [f\"pipe:aws s3 cp {url} -\" for pattern in train_urls for url in braceexpand(pattern)]\n",
    "# else:\n",
    "#     expanded_urls = [str(url) for pattern in train_urls for url in braceexpand(pattern)]\n",
    "\n",
    "# train_data = (\n",
    "#     wds.WebDataset(expanded_urls, resampled=True, nodesplitter=wds.split_by_node, handler=log_and_continue)\n",
    "#     .shuffle(100, initial=100, rng=random.Random(seed))\n",
    "#     .select(filter_corrupted_images)\n",
    "#     .decode(\"torch\")\n",
    "# )\n",
    "# train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "# ### ================      Test Dataset and DataLoader    ====================\n",
    "# print(test_urls)\n",
    "# if is_s3:\n",
    "#     expanded_urls = [f\"pipe:aws s3 cp {url} -\" for pattern in test_urls for url in braceexpand(pattern)]\n",
    "# else:\n",
    "#     expanded_urls = [str(url) for pattern in train_urls for url in braceexpand(pattern)]\n",
    "\n",
    "# test_data = (\n",
    "#     wds.WebDataset(expanded_urls, resampled=True, nodesplitter=wds.split_by_node, handler=log_and_continue)\n",
    "#     .shuffle(100, initial=100, rng=random.Random(seed))\n",
    "#     .select(filter_corrupted_images)\n",
    "#     .decode(\"torch\")\n",
    "# )\n",
    "# test_dl = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "590f4726-285f-4834-afc5-dc5a290df4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "\n",
    "# start_time = time.time() \n",
    "# num_it = 2\n",
    "# print(f\"Yielding {num_it} batches\")\n",
    "\n",
    "# for i, batch in enumerate(test_dl):\n",
    "#     print(\"iter\",i)\n",
    "#     input_func = batch['func.npy']\n",
    "#     subject_id = batch['subject_id.txt']\n",
    "#     subject_id = [int(subject[-2:]) for subject in subject_id]\n",
    "#     # session_id = batch['session_id.txt']\n",
    "#     # session_id = [int(session[-2:]) for session in session_id]\n",
    "#     func, brain_pos_pats = aug_transform(input_func)\n",
    "#     if i >= (num_it-1):\n",
    "#         break\n",
    "\n",
    "# print(\"Done!\")\n",
    "# print(\"input_func\", input_func.shape)\n",
    "# print(\"func\", func.shape)\n",
    "# print(\"subject_id\", subject_id)\n",
    "\n",
    "# end_time = time.time()  \n",
    "# execution_time = end_time - start_time  \n",
    "# print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d309490-9f1f-44fc-8fd8-0b2f694b1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### New datasets\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e5b93a-4448-4000-a25b-2fa994f7142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class fMRIDatasets(Dataset):\n",
    "    def __init__(self):\n",
    "        nsd_raw_path = \"/weka/proj-medarc/shared/mindeyev2_dataset/\"\n",
    "        f = h5py.File(f'{nsd_raw_path}/subj01_mnidata.h5', 'r') #subj01_rawdata_old.h5\n",
    "        # mindeye_global_trs = f['global_trs'][:]\n",
    "        self.data = f['funcs']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25809cc2-0d8d-48b8-86a1-f3f853fbae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# func, brain_pos_pats = aug_transform(input_func)\n",
    "# print(func.shape)\n",
    "# display(utils.view_brain(func,cut_coords=(44,44,44)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42387471-cd5e-4152-ae53-6b5cf4f650d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = fMRIDatasets()\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=512, num_workers=36, drop_last=True) # 42\n",
    "# test_dataset = fMRIDatasets(mindeye_funcs[-10000:])\n",
    "# test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562eeb2-c168-4364-ad64-09ded761b6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c056889-070a-4fda-80ef-6d1685f4f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(func[0,0][:,40,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f24e0c-8a8b-404a-b2a9-1fde370980a0",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcfe48a9-c2f0-4058-b746-ecb60478950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from timm.models.vision_transformer import Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92f7b05d-143a-484c-9d6e-c30f39c65c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, img_size = (88, 104, 72), patch_size = (8,8,6), in_chans: int = 1, bias: bool=True):\n",
    "        super().__init__()\n",
    "        self.input_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = torch.prod(torch.tensor([t1//t2 for t1,t2 in zip(img_size, patch_size)])).item()\n",
    "        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x.flatten(2).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18fdbc0b-056b-469b-89dc-21c622e5d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_3d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 3 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_d = get_1d_sincos_pos_embed_from_grid(embed_dim // 3, grid[0])  # (H*W, D/2)\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 3, grid[1])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 3, grid[2])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_d, emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "def get_3d_sincos_pos_embed(embed_dim, grid_d, grid_h, grid_w, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h_idx = np.arange(grid_h, dtype=np.float32)\n",
    "    grid_w_idx = np.arange(grid_w, dtype=np.float32)\n",
    "    grid_d_idx = np.arange(grid_d, dtype=np.float32)\n",
    "    \n",
    "    grid = np.meshgrid(grid_d_idx, grid_h_idx, grid_w_idx)  # here w goes first D, H, W \n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([3, 1, grid_d, grid_h, grid_w])\n",
    "    pos_embed = get_3d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75c43f92-c584-4a2f-b764-4ce48e83fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def contrastive_loss(\n",
    "#     cls_token1: torch.Tensor, cls_token2: torch.Tensor, temperature: float = 0.07, exclude_diagonal: bool = True\n",
    "# ):\n",
    "#     feat1 = cls_token1 / cls_token1.norm(dim=1, keepdim=True)\n",
    "#     feat2 = cls_token2 / cls_token2.norm(dim=1, keepdim=True)\n",
    "\n",
    "#     cosine_sim = feat1 @ feat2.T / temperature\n",
    "    \n",
    "#     # if exclude_diagonal:\n",
    "#     #     mask = torch.eye(cosine_sim.shape[0], device=cosine_sim.device)\n",
    "#         # cosine_sim = cosine_sim.masked_fill(mask.bool(), -float('inf'))\n",
    "\n",
    "#     labels = torch.arange(cosine_sim.shape[0]).to(cosine_sim.device)\n",
    "#     loss = (\n",
    "#         torch.nn.functional.cross_entropy(cosine_sim, labels)\n",
    "#         + torch.nn.functional.cross_entropy(cosine_sim.T, labels)\n",
    "#     ) / 2\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7585952-d814-4a97-bf1c-24c690e527d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Optional, List\n",
    "import pickle as cp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.return_intermediate = return_intermediate\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None,\n",
    "                query_pos: Optional[Tensor] = None):\n",
    "        output = tgt\n",
    "        T,B,C = memory.shape\n",
    "        intermediate = []\n",
    "        atten_layers = []\n",
    "        for n,layer in enumerate(self.layers):\n",
    "   \n",
    "            residual=True\n",
    "            output,ws = layer(output, memory,  tgt_mask=tgt_mask,\n",
    "                           memory_mask=memory_mask,\n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                           memory_key_padding_mask=memory_key_padding_mask,\n",
    "                           pos=pos, query_pos=query_pos,residual=residual)\n",
    "            atten_layers.append(ws)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(self.norm(output))\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.pop()\n",
    "                intermediate.append(output)\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            return torch.stack(intermediate)\n",
    "        return output,atten_layers\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_post(self, tgt, memory, \n",
    "                     tgt_mask: Optional[Tensor] = None,\n",
    "                     memory_mask: Optional[Tensor] = None,\n",
    "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None,\n",
    "                     query_pos: Optional[Tensor] = None,\n",
    "                     residual=True):\n",
    "        q = k = self.with_pos_embed(tgt, query_pos)\n",
    "        tgt2,ws = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2,ws = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
    "                                   key=self.with_pos_embed(memory, pos),\n",
    "                                   value=memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "\n",
    "        # attn_weights [B,NUM_Q,T]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt,ws\n",
    "\n",
    "    def forward_pre(self, tgt, memory,\n",
    "                    tgt_mask: Optional[Tensor] = None,\n",
    "                    memory_mask: Optional[Tensor] = None,\n",
    "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                    pos: Optional[Tensor] = None,\n",
    "                    query_pos: Optional[Tensor] = None):\n",
    "        tgt2 = self.norm1(tgt)\n",
    "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
    "        tgt2,ws = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt2 = self.norm2(tgt)\n",
    "        tgt2,attn_weights = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
    "                                   key=self.with_pos_embed(memory, pos),\n",
    "                                   value=memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt2 = self.norm3(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        return tgt,attn_weights\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None,\n",
    "                query_pos: Optional[Tensor] = None,\n",
    "                residual=True):\n",
    "        if self.normalize_before:\n",
    "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
    "                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
    "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
    "                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos,residual)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class DQNCOSLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQNCOSLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "        target = Variable(torch.LongTensor(range(batch_size))).to(input.device)\n",
    "        loss = 0\n",
    "        loss += nn.CrossEntropyLoss()(input, target)\n",
    "        # loss += nn.CrossEntropyLoss()(input.transpose(1, 0), target)\n",
    "        return loss #/ 2\n",
    "\n",
    "class Fusion(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, num_layers, state_prob, dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, n_heads , 1024, 0.1, 'relu',normalize_before=True)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_layers , decoder_norm, return_intermediate=False)\n",
    "\n",
    "        # Learnable Queries\n",
    "        #self.query_embed = nn.Embedding(config['num_queries'] ,self.d_model)\n",
    "        self.dropout_feas = nn.Dropout(dropout)\n",
    "\n",
    "        # Attribute classifier\n",
    "        self.classifier = nn.Linear(d_model, state_prob)\n",
    "\n",
    "    def forward(self, query_embed, features):\n",
    "        features,ws = self.decoder(query_embed, features, \n",
    "            memory_key_padding_mask=None, pos=None, query_pos=None)\n",
    "        out = self.dropout_feas(features)\n",
    "        out = self.classifier(out).transpose(0,1) #B query Atributes\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bc09d62-011e-4604-a67f-b217d4511c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoderViT(nn.Module):\n",
    "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=(88, 104, 72), patch_size=(8,8,6), in_chans=1,\n",
    "                 embed_dim=1536, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=576, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        self.actual_batch_size = 24\n",
    "        # MAE encoder specifics\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.img_size = img_size\n",
    "        self.patch_embed = PatchEmbed(embed_dim, img_size=img_size, patch_size=patch_size, in_chans=in_chans)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=True)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=True)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size[0] * patch_size[1] * patch_size[2] * in_chans, bias=True) # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE Fusion module\n",
    "        self.fusion_module = Fusion(d_model=embed_dim, num_layers=4, n_heads=4, state_prob=1, dropout=0.1) # base fusion module params are from the paper\n",
    "        self.ce_loss = DQNCOSLoss()\n",
    "        \n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        num_patches_d = self.img_size[0]//self.patch_size[0]\n",
    "        num_patches_h = self.img_size[1]//self.patch_size[1]\n",
    "        num_patches_w = self.img_size[2]//self.patch_size[2]\n",
    "        pos_embed = get_3d_sincos_pos_embed(self.pos_embed.shape[-1], num_patches_d, num_patches_h, num_patches_w, cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_3d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], num_patches_d, num_patches_h, num_patches_w, cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 1, D, H, W)\n",
    "        x: (N, L, p1 * p2 * p3 * 1)\n",
    "        \"\"\"\n",
    "\n",
    "        N, C, D, H, W = imgs.shape\n",
    "        p_d, p_h, p_w = self.patch_size\n",
    "        d, h, w = D//p_d, H//p_h, W//p_w\n",
    "        \n",
    "        x = imgs.reshape(shape=(N, C, d, p_d, h, p_h, w, p_w))\n",
    "        x = torch.einsum(\"nctuhpwq->nthwupqc\", x)\n",
    "        x = x.reshape(shape=(N, d * h * w, p_d * p_h * p_w * C))\n",
    "        self.patch_info = (N, C, D, H, W, p_d, p_h, p_w, d, h, w)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, p1 * p2 * p3 * 1)\n",
    "        imgs: (N, 1, D, H, W)\n",
    "\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        N, C, D, H, W, p_d, p_h, p_w, d, h, w = self.patch_info\n",
    "        x = x.reshape(shape=(N, d, h, w, p_d, p_h, p_w, C))\n",
    "        x = torch.einsum(\"nthwupqc->nctuhpwq\", x)\n",
    "        imgs = x.reshape(shape=(N, C, D, H, W))\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_encoder_latent(self, x, mask_ratio):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        \n",
    "        \n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        noise = torch.rand(N, L, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    # def forward(self, imgs, mask_ratio=0.85):\n",
    "    #     latent1, mask1, ids_restore1 = self.forward_encoder(imgs, mask_ratio)\n",
    "    #     # latent2, mask2, ids_restore2 = self.forward_encoder(imgs, mask_ratio)\n",
    "    #     pred1 = self.forward_decoder(latent1, ids_restore1)  # [N, L, p*p*3]\n",
    "    #     # pred2 = self.forward_decoder(latent2, ids_restore2)  # [N, L, p*p*3]\n",
    "\n",
    "    #     # cls_token1 = latent1[:, 0]\n",
    "    #     # cls_token2 = latent2[:, 0]\n",
    "        \n",
    "        \n",
    "    #     loss = self.forward_loss(imgs, pred1, mask1) # + self.forward_loss(imgs, pred2, mask2)\n",
    "    #     # loss +=  contrastive_loss(cls_token1, cls_token2) / 2\n",
    "        \n",
    "    #     return loss, pred1, mask1\n",
    "\n",
    "    def forward(self, imgs, mask_ratio=0.85):\n",
    "        # latent1, mask1, ids_restore1 = self.forward_encoder(imgs, mask_ratio)\n",
    "        # latent2, mask2, ids_restore2 = self.forward_encoder(imgs, mask_ratio)\n",
    "\n",
    "        # # recon loss\n",
    "        # pred1 = self.forward_decoder(latent1, ids_restore1)  # [N, L, p*p*3]\n",
    "        # loss_recon_1 = self.forward_loss(imgs, pred1, mask1)\n",
    "        # pred2 = self.forward_decoder(latent2, ids_restore2)  # [N, L, p*p*3]\n",
    "        # loss_recon_2 = self.forward_loss(imgs, pred2, mask2)\n",
    "\n",
    "        # # contrastive loss\n",
    "        # latent1_g = latent1[:, 0, ...]\n",
    "        # latent2_g = latent2[:, 0, ...]\n",
    "\n",
    "        # latent1_l = latent1[:, 1:, ...]\n",
    "        # latent2_l = latent2[:, 1:, ...]\n",
    "\n",
    "        # l1_2_l2_cls = self.fusion_module(latent1.permute(1, 0, 2), latent2[:, 0, :].unsqueeze(0)).squeeze(-1)\n",
    "        # l2_2_l1_cls = self.fusion_module(latent2.permute(1, 0, 2), latent1[:, 0, :].unsqueeze(0)).squeeze(-1)\n",
    "\n",
    "        # ce_loss0 = self.ce_loss(l1_2_l2_cls)\n",
    "        # ce_loss1 = self.ce_loss(l2_2_l1_cls)\n",
    "\n",
    "        # loss = loss_recon_1 + loss_recon_2 + ce_loss0/2 + ce_loss1/2\n",
    "\n",
    "        ## Large batch size\n",
    "        latent1, mask1, ids_restore1 = self.forward_encoder(imgs[:self.actual_batch_size, ...], mask_ratio)\n",
    "        latent2, mask2, ids_restore2 = self.forward_encoder(imgs[:self.actual_batch_size, ...], mask_ratio)\n",
    "\n",
    "        # recon loss\n",
    "        pred1 = self.forward_decoder(latent1, ids_restore1)  # [N, L, p*p*3]\n",
    "        loss_recon_1 = self.forward_loss(imgs, pred1, mask1)\n",
    "        pred2 = self.forward_decoder(latent2, ids_restore2)  # [N, L, p*p*3]\n",
    "        loss_recon_2 = self.forward_loss(imgs, pred2, mask2)\n",
    "\n",
    "        # contrastive loss\n",
    "        with torch.no_grad():\n",
    "            latent1_ng = self.forward_encoder_latent(imgs[self.actual_batch_size:, ...], mask_ratio)\n",
    "            latent2_ng = self.forward_encoder_latent(imgs[self.actual_batch_size:, ...], mask_ratio)\n",
    "        latent1 = torch.cat([latent1, latent1_ng])\n",
    "        latent2 = torch.cat([latent2, latent2_ng])\n",
    "        \n",
    "        latent1_g = latent1[:, 0, ...]\n",
    "        latent2_g = latent2[:, 0, ...]\n",
    "\n",
    "        latent1_l = latent1[:, 1:, ...]\n",
    "        latent2_l = latent2[:, 1:, ...]\n",
    "\n",
    "        l1_2_l2_cls = self.fusion_module(latent1.permute(1, 0, 2), latent2[:, 0, :].unsqueeze(0)).squeeze(-1)\n",
    "        l2_2_l1_cls = self.fusion_module(latent2.permute(1, 0, 2), latent1[:, 0, :].unsqueeze(0)).squeeze(-1)\n",
    "\n",
    "        ce_loss0 = self.ce_loss(l1_2_l2_cls)\n",
    "        ce_loss1 = self.ce_loss(l2_2_l1_cls)\n",
    "\n",
    "        loss = loss_recon_1 + loss_recon_2 + ce_loss0/2 + ce_loss1/2\n",
    "\n",
    "        # loss = loss_recon_1 + loss_recon_2\n",
    "        \n",
    "        return loss, pred1, mask1\n",
    "\n",
    "    def forward_inference(self, imgs, mask_ratio=0.75):\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "        if self.norm_pix_loss:\n",
    "            target = self.patchify(imgs)\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            pred = (pred * (var + 1.e-6)**.5) + mean\n",
    "            \n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f279acce-537a-4e21-8c09-de18ae78fdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1716"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 576, 14\n",
    "model = MaskedAutoencoderViT(depth=16, decoder_embed_dim=576, embed_dim=1056, decoder_num_heads=8)\n",
    "model.patch_embed.num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaf86126-36ba-4853-af08-eaff48f87797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## use large batch size\n",
    "# actial_batch_size = 24\n",
    "# mask_ratio = 0.75\n",
    "# imgs = torch.randn(1024, 1, 88, 104, 72)\n",
    "# device = torch.device(\"cuda\")\n",
    "# self = model\n",
    "# self.to(device)\n",
    "\n",
    "# latent1, mask1, ids_restore1 = self.forward_encoder(imgs[:actial_batch_size, ...].to(device), mask_ratio)\n",
    "# latent2, mask2, ids_restore2 = self.forward_encoder(imgs[:actial_batch_size, ...].to(device), mask_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a66ebad-03aa-4b6e-91a4-6598c597bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     latent1_ng = self.forward_encoder_latent(imgs[actial_batch_size:, ...].to(device), 0.75)\n",
    "#     latent2_ng = self.forward_encoder_latent(imgs[actial_batch_size:, ...].to(device), 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bda3323f-aa28-45d2-99f0-beaf95c74042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent1.shape, latent1_ng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbaf0278-3cab-46a3-b802-ee009591fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # ## load the previous model state dict\n",
    "# ckpt = torch.load(\"/weka/proj-fmri/souvik/fMRI-foundation-model/ckpts/mindeye_ds_exp_1_gl/epoch=32-train_loss=6.22782.ckpt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6520849f-e4e2-4852-be12-fd184ad972a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = ckpt[\"state_dict\"]\n",
    "# state_dict = {k.replace('model.', \"\"): v for k,v in state_dict.items()}\n",
    "# model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d0304a3-f4a1-4e56-8e84-50e2bd17bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_dataloader))\n",
    "# # input_func = batch\n",
    "# # func, _ = aug_transform(input_func)\n",
    "# # func = func[:, random.randint(0,7), ...].unsqueeze(1)\n",
    "# func = batch.unsqueeze(1)\n",
    "# func.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c65d833b-8eb4-4851-b7c8-d4e9f936ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "# model.to(device)\n",
    "# _, pred, mask = model.forward_inference(func.float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bf6daac-a8f1-4dfc-a189-93e344e266c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patched_imgs = model.patchify(func).to(device)\n",
    "# patched_imgs = patched_imgs * (~mask.unsqueeze(-1).bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6fc7eff-753f-42d8-b82d-54ed6b9da919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpatchify = model.unpatchify((pred* mask.unsqueeze(-1)) + patched_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dce4141b-c667-4e76-be64-ef37402b3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#     plt.figure()\n",
    "#     plt.subplot(1,3,1)\n",
    "#     # plt.title(\"pred\")\n",
    "#     plt.imshow(unpatchify[i,0,:, 90, ...].detach().cpu().numpy(), cmap=plt.cm.bone)\n",
    "#     plt.subplot(1,3,2)\n",
    "#     # plt.title(\"gt\")\n",
    "#     plt.imshow(func[i,0,:, 90, ...], cmap=plt.cm.bone)\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(model.unpatchify(patched_imgs).detach().cpu().numpy()[i,0,:, 90, ...], cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8b12271-a300-4585-863d-239545890511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(unpatchify[0,0,:, 30, ...].detach().cpu().numpy(), cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "952eb829-7c39-4541-be57-a811f10e9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(func[0,0,:, 90, ...], cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "779492a4-d0c8-437e-96de-e7091b2239d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## check the embeddings\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     x = model.patch_embed(func.to(device))\n",
    "#     x = x + model.pos_embed[:, 1:, :]\n",
    "#     cls_token = model.cls_token + model.pos_embed[:, :1, :]\n",
    "#     cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "#     x = torch.cat((cls_tokens, x), dim=1)\n",
    "#     for blk in model.blocks:\n",
    "#         x = blk(x)\n",
    "#     x = model.norm(x)\n",
    "\n",
    "# logits = x[:, 0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2efcf781-dfb5-4241-a6e9-2de19fc95d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.functional.cosine_similarity(logits[0].unsqueeze(0), logits[8].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c5e491-086f-4291-a59b-508d0e5c539d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30817e93-a47e-4c84-a599-37ea39ecc3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightning as L\n",
    "# class TrainerfMRIFM(L.LightningModule):\n",
    "#     def __init__(self, model, total_steps, masking_ratio = 0.75, lr = 1.0e-4):\n",
    "#         super().__init__()\n",
    "#         self.model = model\n",
    "#         self.total_steps = total_steps\n",
    "#         self.lr = lr\n",
    "#         self.masking_ratio = masking_ratio\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         loss, _, _ = self.model(x, self.masking_ratio)\n",
    "#         return loss\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         input_func = batch['func.npy']\n",
    "#         func, _ = aug_transform(input_func)\n",
    "#         func = func[:, random.randint(0,7), ...].unsqueeze(1)\n",
    "#         loss = self(func)\n",
    "#         self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=len(batch), sync_dist=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         input_func = batch['func.npy']\n",
    "#         func, _ = aug_transform(input_func)\n",
    "#         func = func[:, random.randint(0,7), ...].unsqueeze(1)\n",
    "#         loss = self(func)\n",
    "#         self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=len(batch), sync_dist=True)\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "#         self.lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=self.lr, total_steps=self.total_steps)\n",
    "#         return {\"optimizer\": self.optimizer, \"lr_scheduler\": {\n",
    "#             \"scheduler\": self.lr_scheduler,\n",
    "#             \"interval\": \"step\",\n",
    "#             \"frequency\": 1\n",
    "#         }}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3fb4667-24af-43d9-9a94-6f13c6e70143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # num_epochs = 10\n",
    "# # num_samples_per_epoch = 20\n",
    "# # batch_size = 4\n",
    "\n",
    "# max_steps = (num_epochs * num_samples_per_epoch)//batch_size\n",
    "# eval_per_n_steps = (num_samples_per_epoch//batch_size)\n",
    "# limit_val_batches = (test_num_samples_per_epoch//batch_size)\n",
    "# max_steps, eval_per_n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "662ea9a1-828c-4baf-b372-ca0ee6a6bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning import Trainer\n",
    "# from lightning.pytorch.loggers import WandbLogger\n",
    "# from lightning.pytorch.strategies import DDPStrategy\n",
    "# from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, RichModelSummary, RichProgressBar\n",
    "\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     dirpath=os.path.abspath(f'../ckpts/{model_name}'),\n",
    "#     filename='{epoch}-{val_loss:.5f}',\n",
    "#     save_top_k=1,\n",
    "#     verbose=True,\n",
    "#     monitor='val_loss',\n",
    "#     mode='min',\n",
    "# )\n",
    "# callbacks = [\n",
    "#     LearningRateMonitor(),\n",
    "#     checkpoint_callback,\n",
    "#     # RichModelSummary(max_depth=2),\n",
    "#     # RichProgressBar()\n",
    "# ]\n",
    "# trainer = Trainer(\n",
    "#     devices=\"auto\",\n",
    "#     num_nodes=int(os.getenv('NUM_NODES', 1)),\n",
    "#     precision=\"16-mixed\",\n",
    "#     logger=WandbLogger(project='mae_single_tf', id=model_name, name=model_name),\n",
    "#     callbacks=callbacks,\n",
    "#     max_steps=max_steps,\n",
    "#     # strategy = DDPStrategy(process_group_backend=\"gloo\"), # COMMENT THIS IF USING NOTEBOOK\n",
    "#     val_check_interval=eval_per_n_steps,\n",
    "#     log_every_n_steps=5,\n",
    "#     limit_val_batches=limit_val_batches,\n",
    "#     limit_train_batches = eval_per_n_steps,\n",
    "#     accumulate_grad_batches=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56b6723e-f8ce-43cd-a45a-528a923e5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module = TrainerfMRIFM(model=model, total_steps=max_steps, masking_ratio=0.8, lr=max_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53fa0d9f-09f4-43c4-b4ef-2f44be48dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(module, train_dataloaders=train_dl, val_dataloaders=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "250c4287-e09d-45d7-a597-5f743e19543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/weka/proj-fmri/souvik/fMRI-foundation-model/found/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /weka/proj-fmri/souvik/fMRI-foundation-model/fou ...\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "class TrainerfMRIFM(L.LightningModule):\n",
    "    def __init__(self, model, total_steps, masking_ratio = 0.75, lr = 1.0e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.total_steps = total_steps\n",
    "        self.lr = lr\n",
    "        self.masking_ratio = masking_ratio\n",
    "        \n",
    "    def forward(self, x):\n",
    "        loss, _, _ = self.model(x, self.masking_ratio)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch = batch.unsqueeze(1)\n",
    "        loss = self(batch)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=len(batch), sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch = batch.unsqueeze(1)\n",
    "        loss = self(batch)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=len(batch), sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=self.lr, total_steps=self.total_steps)\n",
    "        return {\"optimizer\": self.optimizer, \"lr_scheduler\": {\n",
    "            \"scheduler\": self.lr_scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1\n",
    "        }}\n",
    "\n",
    "num_epochs = 100\n",
    "max_steps = (num_epochs * len(train_dataloader))\n",
    "\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.strategies import DDPStrategy\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, RichModelSummary, RichProgressBar\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.abspath(f'../ckpts/{model_name}'),\n",
    "    filename='{epoch}-{train_loss:.5f}',\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='train_loss',\n",
    "    mode='min',\n",
    ")\n",
    "callbacks = [\n",
    "    LearningRateMonitor(),\n",
    "    checkpoint_callback,\n",
    "    # RichModelSummary(max_depth=2),\n",
    "    # RichProgressBar()\n",
    "]\n",
    "trainer = Trainer(\n",
    "    devices=\"auto\",\n",
    "    num_nodes=int(os.getenv('NUM_NODES', 1)),\n",
    "    precision=\"16-mixed\",\n",
    "    logger=WandbLogger(project='mae_single_tf', id=model_name, name=model_name),\n",
    "    callbacks=callbacks,\n",
    "    max_steps=max_steps,\n",
    "    # strategy = DDPStrategy(process_group_backend=\"gloo\"), # COMMENT THIS IF USING NOTEBOOK\n",
    "    # val_check_interval=eval_per_n_steps,\n",
    "    log_every_n_steps=5,\n",
    "    # limit_val_batches=limit_val_batches,\n",
    "    # limit_train_batches = eval_per_n_steps,\n",
    "    accumulate_grad_batches=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e86c355-dea9-490e-89ee-e61ff762fed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/weka/proj-fmri/souvik/fMRI-foundation-model/found/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmandalsouvik\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240714_064445-test_large_batch</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://stability.wandb.io/mandalsouvik/mae_single_tf/runs/test_large_batch' target=\"_blank\">test_large_batch</a></strong> to <a href='https://stability.wandb.io/mandalsouvik/mae_single_tf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://stability.wandb.io/mandalsouvik/mae_single_tf' target=\"_blank\">https://stability.wandb.io/mandalsouvik/mae_single_tf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://stability.wandb.io/mandalsouvik/mae_single_tf/runs/test_large_batch' target=\"_blank\">https://stability.wandb.io/mandalsouvik/mae_single_tf/runs/test_large_batch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [6]\n",
      "\n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | MaskedAutoencoderViT | 294 M  | train\n",
      "-------------------------------------------------------\n",
      "294 M     Trainable params\n",
      "0         Non-trainable params\n",
      "294 M     Total params\n",
      "1,178.744 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4885c1d0044dc39075f3a28468aaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "module = TrainerfMRIFM(model=model, total_steps=max_steps, masking_ratio=0.75, lr=max_lr)\n",
    "trainer.fit(module, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd0cc3-c51c-4c74-8040-660570257d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_dataloader))\n",
    "# batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13324517-24d7-4488-af65-331b04adf52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b27e4-d192-4a63-bd15-30d7edf6ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.patch_embed #(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363b3cf-3708-4d18-a2ae-0f8a2e0a9f1b",
   "metadata": {},
   "source": [
    "## Linear probe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b21b3b-b134-4f45-b132-1a112a3e42e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load the previous model state dict\n",
    "# ckpt = torch.load(\"/weka/proj-fmri/souvik/fMRI-foundation-model/ckpts/base_exp_223m/epoch=6-val_loss=0.24703.ckpt\", map_location=torch.device('cpu'))\n",
    "\n",
    "# state_dict = ckpt[\"state_dict\"]\n",
    "# state_dict = {k.replace('model.', \"\"): v for k,v in state_dict.items()}\n",
    "# model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d01392-8090-4ca5-ae4a-47ac243b2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightning as L\n",
    "# from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af53e4-18eb-4a93-b439-09a62afe9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class fMRIFMLP(L.LightningModule):\n",
    "#     def __init__(self, model, total_steps, lr = 1.0e-4, embed_dim=1056,num_classes=8):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.model = model\n",
    "#         self.model.eval()\n",
    "#         for param in self.model.parameters():\n",
    "#             param.requires_grad = False\n",
    "        \n",
    "#         self.total_steps = total_steps\n",
    "#         self.lr = lr\n",
    "\n",
    "#         self.head = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, num_classes),\n",
    "#         )\n",
    "#         self.loss = nn.CrossEntropyLoss()\n",
    "#         self.metric = nn.ModuleDict({\n",
    "#             \"train_metric\": Accuracy(task=\"multiclass\", num_classes=num_classes),\n",
    "#             \"val_metric\": Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "#         })\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         with torch.no_grad():\n",
    "#             x = self.model.patch_embed(x)\n",
    "#             x = x + self.model.pos_embed[:, 1:, :]\n",
    "#             cls_token = self.model.cls_token + self.model.pos_embed[:, :1, :]\n",
    "#             cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "#             x = torch.cat((cls_tokens, x), dim=1)\n",
    "#             for blk in self.model.blocks:\n",
    "#                 x = blk(x)\n",
    "#             x = self.model.norm(x)\n",
    "\n",
    "#         logits = self.head(x[:, 0, ...])\n",
    "        \n",
    "#         return logits\n",
    "#     def shared_step(self, batch, phase):\n",
    "#         input_func = batch['func.npy']\n",
    "#         func, _ = aug_transform(input_func)\n",
    "#         func = func[:, 0, ...].unsqueeze(1)\n",
    "\n",
    "#         subject_id = batch['subject_id.txt']\n",
    "#         subject_id = torch.Tensor([int(subject[-2:]) for subject in subject_id]).long().to(self.device) - 1\n",
    "        \n",
    "#         logits = self(func)\n",
    "#         loss = nn.functional.cross_entropy(logits, subject_id)\n",
    "#         self.metric[phase+ \"_metric\"](logits, subject_id)\n",
    "#         self.log(f\"{phase}_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=len(batch), sync_dist=True)\n",
    "#         self.log(f\"{phase}_acc\", self.metric[phase+ \"_metric\"], on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=len(batch), sync_dist=True)\n",
    "        \n",
    "#         return loss\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         return self.shared_step(batch, \"train\")\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         return self.shared_step(batch, \"val\")\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "#         self.lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=self.lr, total_steps=self.total_steps)\n",
    "#         return {\"optimizer\": self.optimizer, \"lr_scheduler\": {\n",
    "#             \"scheduler\": self.lr_scheduler,\n",
    "#             \"interval\": \"step\",\n",
    "#             \"frequency\": 1\n",
    "#         }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76171566-0311-4601-93a7-9f4c3e467dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples_per_epoch = 2048\n",
    "# test_num_samples_per_epoch = 2048\n",
    "\n",
    "# max_steps = (num_epochs * num_samples_per_epoch)//batch_size\n",
    "# eval_per_n_steps = (num_samples_per_epoch//batch_size)\n",
    "# limit_val_batches = (test_num_samples_per_epoch//batch_size)\n",
    "# print(max_steps, eval_per_n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048554f0-820d-4466-af0f-d27dff97fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lp = fMRIFMLP(model=model, total_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66993aa6-897b-4680-92f5-f44371771a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning import Trainer\n",
    "# from lightning.pytorch.loggers import WandbLogger\n",
    "# from lightning.pytorch.strategies import DDPStrategy\n",
    "# from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, RichModelSummary, RichProgressBar\n",
    "\n",
    "\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     dirpath=os.path.abspath(f'../ckpts/{model_name}'),\n",
    "#     filename='{epoch}-{val_loss:.5f}',\n",
    "#     save_top_k=1,\n",
    "#     verbose=True,\n",
    "#     monitor='val_loss',\n",
    "#     mode='min',\n",
    "# )\n",
    "# callbacks = [\n",
    "#     LearningRateMonitor(),\n",
    "#     # checkpoint_callback,\n",
    "#     # RichModelSummary(max_depth=2),\n",
    "#     # RichProgressBar()\n",
    "# ]\n",
    "# trainer = Trainer(\n",
    "#     devices=\"auto\",\n",
    "#     num_nodes=int(os.getenv('NUM_NODES', 1)),\n",
    "#     precision=\"16-mixed\",\n",
    "#     logger=WandbLogger(project='mae_single_tf', id=model_name+\"_linear\", name=model_name+\"_linear\"),\n",
    "#     callbacks=callbacks,\n",
    "#     max_steps=max_steps,\n",
    "#     # strategy = DDPStrategy(process_group_backend=\"gloo\"), # COMMENT THIS IF USING NOTEBOOK\n",
    "#     val_check_interval=eval_per_n_steps,\n",
    "#     log_every_n_steps=5,\n",
    "#     limit_val_batches=limit_val_batches,\n",
    "#     limit_train_batches = eval_per_n_steps,\n",
    "#     accumulate_grad_batches=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0610d5-8388-47b7-b963-50b78309f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(model_lp, train_dataloaders=train_dl, val_dataloaders=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f5f05-fb53-48cd-b91d-6021411d958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_checkpoint(\"/weka/proj-fmri/souvik/fMRI-foundation-model/ckpts/base_exp_223m_linear/trainer_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27638101-fc42-4cea-9cb2-69435f81798c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
