{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import tarfile\n",
    "import webdataset as wds\n",
    "from skimage import io\n",
    "import io as bio\n",
    "import datasets.utils as dutils \n",
    "from jepa.src.masks.multiblock3d import MaskCollator as MB3DMaskCollator\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from PIL import Image\n",
    "import yaml\n",
    "\n",
    "sys.path.append('/mnt/c/Users/Moham/Desktop/fMRI-foundation-model/jepa')\n",
    "\n",
    "from jepa.src.models.vision_transformer import vit_tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMRIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None, are_tars=True):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.folders = os.listdir(root)\n",
    "        self.are_tars = are_tars\n",
    "\n",
    "        self.files = []\n",
    "        if self.are_tars:\n",
    "            self.folders = [folder for folder in self.folders if folder[-4:] == \".tar\"]\n",
    "        \n",
    "        self._load_input_files()\n",
    "\n",
    "    def _load_from_tar(self, folder):\n",
    "            with tarfile.open(os.path.join(self.root, folder), 'r') as tar:\n",
    "                files = tar.getmembers()\n",
    "                return [(os.path.join(self.root, folder), file.name) for file in files if \"func\" in file.name]\n",
    "\n",
    "    def _load_from_directory(self, folder):\n",
    "        folder_path = os.path.join(self.root, folder)\n",
    "        return [(folder_path, f) for f in os.listdir(folder_path) if \"func\" in f and os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "    def _load_input_files(self):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for folder in self.folders:\n",
    "                if self.are_tars:\n",
    "                    futures.append(executor.submit(self._load_from_tar, folder))\n",
    "                else:\n",
    "                    futures.append(executor.submit(self._load_from_directory, folder))\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                self.files.extend(future.result())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path_tuple = self.files[idx]\n",
    "        fmri = self.load_file(file_path_tuple)        \n",
    "        if self.transform:\n",
    "            fmri = self.transform(fmri)\n",
    "        \n",
    "        return fmri\n",
    "\n",
    "    def load_file(self, file_path_tuple):\n",
    "        if self.are_tars:\n",
    "            tar_path, member_name = file_path_tuple\n",
    "            with tarfile.open(tar_path, 'r') as tar:\n",
    "                member = tar.getmember(member_name)\n",
    "                f = tar.extractfile(member)\n",
    "                fmri = io.imread(f)\n",
    "        else:\n",
    "            file_path = os.path.join(*file_path_tuple)\n",
    "            fmri = io.imread(file_path)\n",
    "        \n",
    "        return fmri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/c/Users/Moham/Desktop/fMRI-foundation-model/data/wds/\"\n",
    "# data = FMRIDataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536, 3072)\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/c/Users/Moham/Desktop/fMRI-foundation-model/data/wds/000001.tar\"\n",
    "cache_dir = \"./cache\"\n",
    "\n",
    "patch_size = 8\n",
    "frame_patch_size = 1\n",
    "num_samples_per_epoch = 1024\n",
    "batch_size = 1\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_and_continue(exn):\n",
    "    \"\"\"Call in an exception handler to ignore any exception, issue a warning, and continue.\"\"\"\n",
    "    print(f'Handling webdataset error ({repr(exn)}). Ignoring.')\n",
    "    return True\n",
    "\n",
    "def filter_corrupted_images(sample):\n",
    "    \"\"\"If all the required files are not present don't use them.\"\"\"\n",
    "    correct_data = (\"func.png\" in sample and \"dataset.txt\" in sample and \"header.npy\" in sample and \"meansd.png\" in sample and \"minmax.npy\" in sample)\n",
    "    return correct_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/c/Users/Moham/Desktop/fMRI-foundation-model/jepa/configs/pretrain/vitt16.yaml\", 'r') as y_file:\n",
    "    config_fnames = yaml.load(y_file, Loader=yaml.FullLoader)\n",
    "\n",
    "mask_cnfg = config_fnames.get(\"mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "_params = yaml.load(\"jepa/configs/pretrain/vitt16.yaml\", Loader=yaml.FullLoader)\n",
    "\n",
    "mask_collator = MB3DMaskCollator(\n",
    "    crop_size=64,\n",
    "    num_frames=20 ,\n",
    "    patch_size=patch_size,\n",
    "    tubelet_size=1,\n",
    "    cfgs_mask=mask_cnfg)\n",
    "\n",
    "aug_transform = dutils.DataPrepper(\n",
    "        masking_strategy=\"conservative\",\n",
    "        patch_depth=8,\n",
    "        patch_height=8,\n",
    "        patch_width=8,\n",
    "        frame_patch_size=1,\n",
    "        num_timepoints=20\n",
    "    )\n",
    "\n",
    "train_data = wds.WebDataset(path, resampled=False, cache_dir=cache_dir, handler=log_and_continue).select(filter_corrupted_images).rename(key=\"__key__\",\n",
    "    func=\"func.png\",\n",
    "    header=\"header.npy\",\n",
    "    dataset=\"dataset.txt\",\n",
    "    minmax=\"minmax.npy\",\n",
    "    meansd=\"meansd.png\").map_dict(func=dutils.grayscale_decoder,\n",
    "    meansd=dutils.grayscale_decoder,\n",
    "    minmax=dutils.numpy_decoder).to_tuple(*(\"func\", \"minmax\", \"meansd\")).map(aug_transform)\n",
    "\n",
    "train_dl = wds.WebLoader(\n",
    "    train_data.batched(batch_size), \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    batch_size=None,\n",
    "    collate_fn=mask_collator,\n",
    "    num_workers=num_workers, \n",
    "    \n",
    "    persistent_workers=num_workers>0,\n",
    ").with_epoch(num_samples_per_epoch//batch_size).with_length(num_samples_per_epoch//batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vit_tiny(in_chans=12, num_frames=48, img_size=64, patch_size=patch_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4187403430>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1409, in _shutdown_workers\n",
      "    if not self._shutdown:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_shutdown'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4187403430>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4187403430>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4187403430>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/mohammed/.local/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 64, 64, 48)\n",
      "(24, 64, 64, 48)\n",
      "batch 3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 42, in fetch\n    return self.collate_fn(data)\n  File \"/mnt/c/Users/Moham/Desktop/fMRI-foundation-model/jepa/src/masks/multiblock3d.py\", line 56, in __call__\n    collated_batch = torch.utils.data.default_collate(batch)\n  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_dl:\n\u001b[1;32m      2\u001b[0m     i \u001b[38;5;241m=\u001b[39m i[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# print(i[0].shape)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# print(i[1].shape)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# print(i[2].shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/webdataset/pipeline.py:70\u001b[0m, in \u001b[0;36mDataPipeline.iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create an iterator through the entire dataset, using the given number of repetitions.\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepetitions):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator1()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 42, in fetch\n    return self.collate_fn(data)\n  File \"/mnt/c/Users/Moham/Desktop/fMRI-foundation-model/jepa/src/masks/multiblock3d.py\", line 56, in __call__\n    collated_batch = torch.utils.data.default_collate(batch)\n  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/mohammed/.local/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 64, 64, 48)\n",
      "batch 3\n",
      "(24, 64, 64, 48)\n",
      "batch 3\n"
     ]
    }
   ],
   "source": [
    "for i in train_dl:\n",
    "    i = i[0].cuda()\n",
    "    # print(i[0].shape)\n",
    "    # print(i[1].shape)\n",
    "    # print(i[2].shape)\n",
    "    i = i.permute(0, 1, -1, 2, 3).contiguous()\n",
    "    print(i.shape)\n",
    "    # o = model(i)\n",
    "    # print(o.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
