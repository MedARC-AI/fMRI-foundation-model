{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 1,
>>>>>>> main
   "id": "4f0ba2f3-4e5b-4d03-89b4-9b6868536f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "LOCAL RANK  0\n"
=======
      "Number of available CUDA devices: 1\n",
      "LOCAL RANK=0\n",
      "NUM GPUS=1\n",
      "NODE=0\n",
      "GLOBAL RANK=0\n",
      "WORLD_SIZE=1\n",
      "NOT distributed\n",
      "PID of this process = 143404\n",
      "device = cuda distributed = False num_devices = 1 local rank = 0 world size = 1\n"
>>>>>>> main
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
=======
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
>>>>>>> main
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
<<<<<<< HEAD
=======
    "import gc\n",
>>>>>>> main
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "import webdataset as wds\n",
<<<<<<< HEAD
    "import gc\n",
=======
>>>>>>> main
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from models import *\n",
    "from mindeye_models import *\n",
<<<<<<< HEAD
    "from accelerate import Accelerator, load_checkpoint_in_model\n",
=======
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "# from accelerate import Accelerator\n",
>>>>>>> main
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "### Multi-GPU config ###\n",
<<<<<<< HEAD
    "local_rank = os.getenv('RANK')\n",
=======
    "device_count = torch.cuda.device_count()\n",
    "print(f\"Number of available CUDA devices: {device_count}\")\n",
    "\n",
    "local_rank = os.getenv('LOCAL_RANK')\n",
>>>>>>> main
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
<<<<<<< HEAD
    "print(\"LOCAL RANK \", local_rank) \n",
    "\n",
    "# Following allows you to change functions in models.py or utils.py and \n",
    "# have this notebook automatically update with your revisions\n",
    "if utils.is_interactive():\n",
=======
    "print(f\"LOCAL RANK={local_rank}\")\n",
    "\n",
    "num_devices = os.getenv('NUM_GPUS')\n",
    "if num_devices is None: \n",
    "    num_devices = 1\n",
    "else:\n",
    "    num_devices = int(num_devices)\n",
    "print(f\"NUM GPUS={num_devices}\")\n",
    "distributed = True if num_devices>1 else False\n",
    "if distributed: assert device_count==num_devices\n",
    "\n",
    "node = os.getenv('SLURM_NODEID')\n",
    "if node is None:\n",
    "    node = 0\n",
    "else:\n",
    "    node = int(node)\n",
    "print(f\"NODE={node}\")\n",
    "\n",
    "global_rank = os.getenv('RANK')\n",
    "if global_rank is None:\n",
    "    global_rank = 0\n",
    "else:\n",
    "    global_rank = int(global_rank)\n",
    "print(f\"GLOBAL RANK={global_rank}\")\n",
    "\n",
    "world_size = os.getenv('WORLD_SIZE')\n",
    "if world_size is None: \n",
    "    world_size = 1\n",
    "else:\n",
    "    world_size = int(world_size)\n",
    "print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
>>>>>>> main
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
<<<<<<< HEAD
    "    from tqdm import tqdm"
=======
    "    from tqdm import tqdm\n",
    "\n",
    "# FSDP Setup\n",
    "if distributed:\n",
    "    import torch.distributed as dist\n",
    "    import torch.multiprocessing as mp\n",
    "    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "    from torch.distributed.fsdp.api import BackwardPrefetch, CPUOffload, ShardingStrategy\n",
    "    import functools\n",
    "    from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\n",
    "    print(\"starting init_process_group...\")\n",
    "    dist.init_process_group(\"nccl\", rank=global_rank, world_size=world_size)\n",
    "    print(f\"setting device to cuda:{local_rank}\")\n",
    "    try:\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.cuda.current_device() #torch.device('cuda',local_rank)\n",
    "        print(f\"\\nSuccessfully set cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "    except Exception as error:        \n",
    "        print(f\"\\nFAILED TO SET DEVICE cuda:{local_rank} | global_rank{global_rank} | node{node}\")\n",
    "        print(\"An exception occurred:\", error)\n",
    "    dist.barrier()\n",
    "    print(\"passed barrier\\n\")\n",
    "else:\n",
    "    print(\"NOT distributed\")\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "print(\"device =\", device, \"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)"
>>>>>>> main
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> main
   "id": "ed487017-385c-4e79-b005-71eda9a4c960",
   "metadata": {
    "tags": []
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  64\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> main
   "source": [
    "# Load VJEPA parameters from yaml config\n",
    "config = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in config.keys():\n",
    "    globals()[attribute_name] = config[f'{attribute_name}']\n",
    "\n",
    "# Load MindEye parameters from yaml config (will override any params with same name)\n",
    "mindeye_config = yaml.load(open('mindeye_config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# create global variables from the config\n",
    "for attribute_name in mindeye_config.keys():\n",
<<<<<<< HEAD
    "    globals()[attribute_name] = mindeye_config[f'{attribute_name}']\n",
    "\n",
    "# First use \"accelerate config\" in terminal for setup\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "num_devices = torch.cuda.device_count()\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "batch_size = global_batch_size // num_devices\n",
    "print(\"batch_size: \", batch_size)"
=======
    "    globals()[attribute_name] = mindeye_config[f'{attribute_name}']"
>>>>>>> main
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "id": "b117f153-669e-407a-9e13-4a5268203f99",
   "metadata": {
    "tags": []
   },
=======
   "execution_count": 3,
   "id": "eb0abbeb-6d4e-4359-b955-ba23a0072767",
   "metadata": {},
>>>>>>> main
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "PID of this process = 1175153\n",
      "device: cuda\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
=======
      "batch_size:  8\n",
      "global_batch_size:  8\n"
>>>>>>> main
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
=======
    "# First use \"accelerate config\" in terminal for setup\n",
    "global_batch_size = batch_size * num_devices\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "# accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "global_batch_size = batch_size * num_devices\n",
    "print(\"batch_size: \", batch_size)\n",
    "print(\"global_batch_size: \", global_batch_size)"
>>>>>>> main
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3eb3e-e718-4741-a88b-4e858efcf1ab",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 10,
>>>>>>> main
   "id": "1b7b1894-b35e-4acc-9281-8025dddb09c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vjepa config\n",
      "\n",
<<<<<<< HEAD
      " {'model_name': 'patch8_100eps_4gpu_accelerate_b', 'use_cls_token': False, 'use_contrastive_loss': False, 'constrastive_loss_weight': 1.0, 'global_batch_size': 4, 'num_workers': 4, 'num_epochs': 300, 'seed': 42, 'max_lr': 3e-05, 'num_samples_per_epoch': 1024, 'cache_dir': 'cache/', 'ema': [0.998, 1.0], 'ipe_scale': 1.25, 'ckpt_saving': True, 'ckpt_interval': 50, 'resume_from_ckpt': False, 'wandb_log': True, 'tube_start_masking_ratio': 0.85, 'tube_end_masking_ratio': 0.85, 'decoder_mask_ratio': 0.85, 'depth': 12, 'heads': 12, 'dim': 512, 'mlp_dim': 512, 'patch_size': 8, 'frame_patch_size': 1, 'use_rope_emb': False, 'img_size': [64, 64, 48], 'num_frames': 4, 'train_urls': 's3://proj-fmri/fmri_foundation_datasets/openneuro/{000005..000664}.tar', 'test_urls': 's3://proj-fmri/fmri_foundation_datasets/openneuro/{000000..000004}.tar'}\n",
      "mindeye_config\n",
      " {'model_name': 'ME_patch8_60ep_nopretrain', 'mae_model_name': 'patch8', 'global_batch_size': 64, 'num_workers': 8, 'mixed_precision': 'fp16', 'num_epochs': 60, 'seed': 42, 'max_lr': 0.0003, 'num_samples_per_epoch': 512, 'ckpt_saving': True, 'ckpt_interval': 99, 'resume_from_ckpt': False, 'wandb_log': True, 'in_dim': 786432, 'hidden_dim': 512, 'drop': 0.15, 'mixup_pct': 0.33, 'nsd_wds_path': '/weka/proj-fmri/shared/mindeyev2_dataset/wds', 'nsd_raw_path': '/weka/proj-fmri/shared/mindeyev2_dataset', 'nsd_image_path': '/weka/proj-fmri/shared/mindeyev2_dataset', 'num_sessions': 10}\n",
      "outdir /weka/proj-fmri/ks9249/fMRI-foundation-model/ckpts/patch8_100eps_4gpu_accelerate_b\n",
      "num_patches 1536\n"
=======
      " {'model_name': 's3_jepa_small_1node_FSDP_1worker_clamped', 'use_cls_token': False, 'use_contrastive_loss': False, 'constrastive_loss_weight': 1.0, 'batch_size': 1, 'num_workers': 1, 'num_epochs': 200, 'seed': 42, 'max_lr': 3e-05, 'num_samples_per_epoch': 1024, 'ema': [0.998, 1.0], 'ipe_scale': 1.25, 'ckpt_saving': True, 'ckpt_interval': 50, 'resume_from_ckpt': False, 'wandb_log': True, 'x_encoder_start_masking_ratio': 0.95, 'x_encoder_end_masking_ratio': 0.95, 'y_encoder_mask_ratio': 0.95, 'patch_size': 8, 'frame_patch_size': 1, 'use_rope_emb': False, 'masking_strategy': 'MNI', 'embed_dim': 384, 'mlp_dim': 1536, 'depth': 12, 'num_heads': 6, 'dim_head': 64, 'image_size': [88, 104, 72], 'num_frames': 4, 'is_s3': True, 'train_urls': ['/scratch/fmri_foundation_datasets/NSD_MNI_wds/{000000..000494}.tar', '/scratch/fmri_foundation_datasets/NSD_MNI_wds/{000496..000740}.tar'], 's3_train_urls': ['s3://proj-fmri/fmri_foundation_datasets/NSD_MNI_wds/{000000..000494}.tar', 's3://proj-fmri/fmri_foundation_datasets/NSD_MNI_wds/{000496..000740}.tar']}\n",
      "mindeye_config\n",
      " {'model_name': 'jepa_NOpretrain_bs8_xc', 'mae_model_name': 's3_jepa_mini_4gpu_b2', 'batch_size': 8, 'num_workers': 10, 'mixed_precision': 'fp16', 'num_epochs': 50, 'seed': 42, 'max_lr': 0.0003, 'ckpt_saving': False, 'ckpt_interval': 50, 'resume_from_ckpt': False, 'wandb_log': True, 'in_dim': 1236480, 'hidden_dim': 512, 'drop': 0.15, 'mixup_pct': 0.33, 'nsd_wds_path': '/weka/proj-fmri/shared/mindeyev2_dataset/wds', 'nsd_raw_path': '/weka/proj-fmri/shared/mindeyev2_dataset', 'nsd_image_path': '/weka/proj-fmri/shared/mindeyev2_dataset', 'num_sessions': 5}\n",
      "outdir /weka/proj-fmri/paulscotti/fMRI-foundation-model/ckpts/s3_jepa_small_1node_FSDP_1worker_clamped\n",
      "num_patches 5148\n"
>>>>>>> main
     ]
    }
   ],
   "source": [
    "print(\"vjepa config\\n\\n\",config)\n",
    "print(\"mindeye_config\\n\",mindeye_config)\n",
    "\n",
    "if utils.is_interactive():\n",
    "    ckpt_saving = False\n",
    "    wandb_log = False\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../ckpts/{config[\"model_name\"]}')\n",
    "print(\"outdir\", outdir)\n",
    "\n",
    "num_patches = int(\n",
<<<<<<< HEAD
    "    (img_size[0] / patch_size)\n",
    "    * (img_size[1] / patch_size)\n",
    "    * (img_size[2] / patch_size)\n",
=======
    "    (image_size[0] / patch_size)\n",
    "    * (image_size[1] / patch_size)\n",
    "    * (image_size[2] / patch_size)\n",
>>>>>>> main
    "    * num_frames\n",
    ")\n",
    "print(\"num_patches\", num_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441c6fcd-1d17-4283-8c97-e8a10b932136",
   "metadata": {},
   "source": [
    "# Load VJEPA model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 11,
>>>>>>> main
   "id": "f03af316-fea0-4b50-b5ac-2ccdd3d08178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "param counts:\n",
      "76,138,496 total\n",
      "0 trainable\n"
=======
      "x_encoder\n",
      "param counts:\n",
      "21,275,904 total\n",
      "21,275,904 trainable\n",
      "21275904\n",
      "y_encoder\n",
      "param counts:\n",
      "21,275,904 total\n",
      "21,275,904 trainable\n",
      "21275904\n",
      "predictor\n",
      "param counts:\n",
      "21,275,904 total\n",
      "21,275,904 trainable\n",
      "21275904\n"
     ]
    }
   ],
   "source": [
    "image_depth, image_height, image_width = image_size\n",
    "image_patch_size=(patch_size,patch_size,patch_size)\n",
    "patch_depth, patch_height, patch_width = image_patch_size\n",
    "x_encoder = Transformer(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"x_encoder\")\n",
    "print(utils.count_params(x_encoder))\n",
    "y_encoder = Transformer(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"y_encoder\")\n",
    "print(utils.count_params(y_encoder))\n",
    "predictor = Transformer(\n",
    "    embed_dim,\n",
    "    depth,\n",
    "    num_heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    use_rope=use_rope_emb,\n",
    "    grid_time=num_frames // frame_patch_size,\n",
    "    grid_depth=image_depth // patch_depth,\n",
    "    grid_height=image_height // patch_height,\n",
    "    grid_width=image_width // patch_width,\n",
    "    cls_token=use_cls_token,\n",
    ")\n",
    "print(\"predictor\")\n",
    "print(utils.count_params(predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4278478f-378f-42cb-91bf-eae684ac997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "64,026,880 total\n",
      "64,026,880 trainable\n"
>>>>>>> main
     ]
    },
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0"
      ]
     },
     "execution_count": 6,
=======
       "64026880"
      ]
     },
     "execution_count": 12,
>>>>>>> main
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleViT(\n",
<<<<<<< HEAD
    "    image_size=img_size,  # depth, height, width\n",
    "    image_patch_size=(patch_size,patch_size,patch_size),  # depth, height, width patch size\n",
    "    frames=num_frames,\n",
    "    frame_patch_size=frame_patch_size,\n",
    "    depth=depth,\n",
    "    heads=heads,\n",
    "    dim=dim,\n",
    "    mlp_dim=mlp_dim, \n",
=======
    "    x_encoder=x_encoder,\n",
    "    y_encoder=y_encoder,\n",
    "    predictor=predictor,\n",
    "    image_size=image_size, \n",
    "    image_patch_size=image_patch_size, \n",
    "    num_frames=num_frames,\n",
    "    frame_patch_size=frame_patch_size,\n",
>>>>>>> main
    "    channels=1,\n",
    "    use_rope_emb=use_rope_emb,\n",
    "    use_cls_token=use_cls_token,\n",
    ")\n",
<<<<<<< HEAD
    "\n",
    "load_checkpoint_in_model(model, outdir+\"/last/vjepa2\")\n",
    "\n",
    "\n",
=======
    "utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212b974d-90b0-4d4b-888d-1137f6e26861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load from ckpt\n",
    "# mae_ckpt_pth = os.path.abspath(f'../ckpts/{mae_model_name}/last.pth')\n",
    "# print(\"mae_ckpt_pth\", mae_ckpt_pth)\n",
    "\n",
    "# checkpoint = torch.load(mae_ckpt_pth)\n",
    "# state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b845aa6d-a8b6-4f71-a029-ebb1429d3afb",
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> main
    "# set foundation model to evaluation\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "model.to(device)\n",
<<<<<<< HEAD
    "pass\n",
    "\n",
    "utils.count_params(model)"
=======
    "pass"
>>>>>>> main
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd412c-ac9e-482a-8c13-5e0a7967ca8a",
   "metadata": {},
   "source": [
    "# Setup MindEye Model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 15,
>>>>>>> main
   "id": "88f2266c-8278-470c-90fa-6aa77eb284f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nsddata_raw_stimuli = pd.read_csv(f\"{nsd_raw_path}/nsddata_rawdata.csv\")\n",
    "TR_delay = 3 # to account for bold hrf\n",
    "train_TRs = np.round(nsddata_raw_stimuli[nsddata_raw_stimuli['shared1000'] == False]['global_TR_onsets'].values + TR_delay).astype(np.int32)\n",
    "test_TRs = np.round(nsddata_raw_stimuli[nsddata_raw_stimuli['shared1000'] == True]['global_TR_onsets'].values + TR_delay).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": null,
>>>>>>> main
   "id": "369253a2-ba58-4362-a599-d5b2b21a2c1c",
   "metadata": {
    "tags": []
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images! torch.Size([73000, 3, 224, 224])\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> main
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{nsd_image_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:] \n",
    "images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images!\", images.shape)\n",
    "\n",
    "# Load MindEye hdf5\n",
<<<<<<< HEAD
    "f = h5py.File(f'{nsd_raw_path}/subj01_rawdata_old.h5', 'r')\n",
    "mindeye_global_trs = f['global_trs'][:]\n",
    "mindeye_funcs = f['funcs']\n",
    "mindeye_meansds = f['meansds']"
=======
    "f = h5py.File(f'{nsd_raw_path}/subj01_mnidata.h5', 'r') #subj01_rawdata_old.h5\n",
    "mindeye_global_trs = f['global_trs'][:]\n",
    "mindeye_funcs = f['funcs']"
>>>>>>> main
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": null,
>>>>>>> main
   "id": "b6ccbc02-1111-4e02-8809-68c319a1d037",
   "metadata": {
    "tags": []
   },
<<<<<<< HEAD
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aab66a2d5e44d33912e10f230a69e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_pytorch_model.bin:   0%|          | 0.00/10.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "outputs": [],
>>>>>>> main
   "source": [
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=\"ViT-bigG-14\",\n",
    "    version=\"laion2b_s39b_b160k\",\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "clip_img_embedder.to(device)\n",
    "clip_seq_dim, clip_emb_dim = 256, 1664"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": null,
>>>>>>> main
   "id": "dce30ef3-e898-412e-a6d7-2ab002dcf9ed",
   "metadata": {
    "tags": []
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iterations_per_epoch:  117\n",
      "Training with 10 sessions\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/train/{0..9}.tar\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/new_test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> main
   "source": [
    "subj = s = 1\n",
    "subj_list = [subj]\n",
    "\n",
    "num_samples_per_epoch = (750*num_sessions) // num_devices\n",
    "num_iterations_per_epoch = num_samples_per_epoch // batch_size\n",
    "print(\"num_iterations_per_epoch: \", num_iterations_per_epoch)\n",
    "train_data = {}\n",
    "train_dl = {}\n",
    "\n",
    "print(f\"Training with {num_sessions} sessions\")\n",
    "train_url = f\"{nsd_wds_path}/subj0{s}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "print(train_url)\n",
    "    \n",
    "train_data[f'subj0{s}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=utils.my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "# train_dl[f'subj0{s}'] = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "train_dl[f'subj0{s}'] = wds.WebLoader(\n",
    "    train_data[f'subj0{s}'].batched(batch_size), \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    batch_size=None,\n",
    "    num_workers=num_workers, \n",
    "    persistent_workers=num_workers>0,\n",
    ").with_epoch(num_iterations_per_epoch)\n",
    "\n",
<<<<<<< HEAD
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "if subj==3:\n",
    "    num_test=2371\n",
    "elif subj==4:\n",
    "    num_test=2188\n",
    "elif subj==6:\n",
    "    num_test=2371\n",
    "elif subj==8:\n",
    "    num_test=2188\n",
    "else:\n",
    "    num_test=3000\n",
    "test_url = f\"{nsd_wds_path}/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=utils.my_split_by_node)\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "# test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "test_dl = wds.WebLoader(\n",
    "    test_data.batched(num_test), \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    batch_size=None,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=num_workers>0,\n",
    ")\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
=======
    "if global_rank==0:\n",
    "    print(\"Loaded all subj train dls and betas!\\n\")\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{nsd_wds_path}/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "    print(test_url)\n",
    "    test_data = wds.WebDataset(test_url,resampled=True,nodesplitter=utils.my_split_by_node)\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    # test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "    test_dl = wds.WebLoader(\n",
    "        test_data.batched(num_test),\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        batch_size=None,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=num_workers>0,\n",
    "    ).with_epoch(10)\n",
    "    print(f\"Loaded test dl for subj{subj}!\\n\")"
>>>>>>> main
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "id": "28397ed7-25b7-4f4d-ae05-bc9ec2b7cbad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping deepspeed reconfiguration...\n"
     ]
    }
   ],
   "source": [
    "from accelerate.state import AcceleratorState\n",
    "try:\n",
    "    AcceleratorState().deepspeed_plugin.deepspeed_config['train_micro_batch_size_per_gpu'] = global_batch_size\n",
    "    print(\"deepspeed reconfigured, train_micro_batch_size_per_gpu = \", global_batch_size)\n",
    "except:\n",
    "    print(\"skipping deepspeed reconfiguration...\")"
=======
   "execution_count": null,
   "id": "60f49b37-0aa8-47f1-987f-e2f53000a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate.state import AcceleratorState\n",
    "# try:\n",
    "#     AcceleratorState().deepspeed_plugin.deepspeed_config['train_micro_batch_size_per_gpu'] = global_batch_size\n",
    "#     print(\"deepspeed reconfigured, train_micro_batch_size_per_gpu = \", global_batch_size)\n",
    "# except:\n",
    "#     print(\"skipping deepspeed reconfiguration...\")"
>>>>>>> main
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": null,
>>>>>>> main
   "id": "0444190e-33ff-4903-bdf2-0845f450d04a",
   "metadata": {
    "tags": []
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "209,715,712 total\n",
      "209,715,712 trainable\n",
      "param counts:\n",
      "228,956,824 total\n",
      "228,956,824 trainable\n",
      "param counts:\n",
      "438,672,536 total\n",
      "438,672,536 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "438672536"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "outputs": [],
>>>>>>> main
   "source": [
    "mindeye = MindEyeModule()\n",
    "mindeye.ridge = RidgeRegression(np.array([in_dim]), out_features=hidden_dim)\n",
    "mindeye.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, n_blocks=4, drop=drop,\n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim, clip_scale=1)\n",
    "utils.count_params(mindeye.ridge)\n",
    "utils.count_params(mindeye.backbone)\n",
    "utils.count_params(mindeye)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": null,
   "id": "84127f7a-2108-4e5d-8ac6-6149b13c1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed: \n",
    "    my_auto_wrap_policy = functools.partial(\n",
    "        size_based_auto_wrap_policy, min_num_params=200000\n",
    "    )\n",
    "    print(f\"\\nPrepping FSDP on {global_rank} {node}...\\n\")\n",
    "    mindeye = FSDP(\n",
    "        mindeye,\n",
    "        sharding_strategy=ShardingStrategy.HYBRID_SHARD,\n",
    "        auto_wrap_policy=my_auto_wrap_policy,\n",
    "        use_orig_params=False,\n",
    "        cpu_offload=None, #CPUOffload(offload_params=True)\n",
    "        sync_module_states=True,\n",
    "        limit_all_gathers=True, # See https://github.com/pytorch/pytorch/issues/91165\n",
    "        device_id=device,\n",
    "    )\n",
    "    print(f\"\\nSuccessfully loaded FSDP model to device on global_rank {global_rank}\\n\")\n",
    "    dist.barrier()\n",
    "    print(f\"\\nSuccessfully passed barrier! {global_rank}\\n\")\n",
    "else:\n",
    "    mindeye=mindeye.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> main
   "id": "5ad1b981-00f4-4072-8402-c0775881edd9",
   "metadata": {
    "tags": []
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 7020\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "76,138,496 total\n",
      "0 trainable\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> main
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in mindeye.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in mindeye.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in mindeye.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "total_steps = num_epochs * num_iterations_per_epoch\n",
    "print(\"total_steps\", total_steps)\n",
    "pct_start = 2/num_epochs if num_epochs>1 else 1.\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    ")\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": null,
>>>>>>> main
   "id": "55228b07-e8ae-448c-972e-800aa6dc7ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_ckpt(tag=\"last\"):\n",
    "    ckpt_path = outdir+f'/{tag}/downstream'\n",
    "    os.makedirs(ckpt_path,exist_ok=True)\n",
    "    accelerator.save_model(model, ckpt_path, max_shard_size=\"2GB\", safe_serialization=True)\n",
    "    print(f\"\\n---saved {ckpt_path}!---\\n\")\n",
    "        \n",
    "def save_progress(tag=\"last\"):\n",
    "    if accelerator.is_main_process:\n",
    "        ckpt_path = outdir+f'/{tag}/downstream'\n",
    "        torch.save(\n",
    "                {\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"scheduler\": lr_scheduler.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"losses\": losses,\n",
    "                    \"test_losses\": test_losses,\n",
    "                    \"lrs\": lrs,\n",
    "                },\n",
    "                os.path.join(ckpt_path, f\"params.pt\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290b05d-9985-4a68-919e-2baeccc4af21",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "if accelerator.is_main_process and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'vjepa_downstream_3'\n",
=======
    "if global_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found_downstream'\n",
>>>>>>> main
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"mae_model_name\": mae_model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"in_dim\": in_dim,\n",
    "      \"hidden_dim\": hidden_dim,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_params\": num_params,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2a116-fc5d-4be9-8f9c-01b00618c8f9",
   "metadata": {},
   "source": [
    "# Train MindEye with Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee95c4-2011-4187-96e7-4fa840458507",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "c6c9624a-5f39-46b9-83d3-5c8288511490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "MNI_brain = nib.load(\"/weka/proj-fmri/paulscotti/fMRI-foundation-model/dataset_creation/afni_conversion/tpl-MNI152NLin2009cAsym_res-02_T1w_brain.nii.gz\").get_fdata()\n",
    "brain_pos_voxels = MNI_brain[6:94,8:112,10:82]\n",
    "brain_pos_voxels[6:94,:(112-60),10:62] = 0\n",
    "brain_pos_pats = model.patchify(torch.Tensor(brain_pos_voxels)[None,None,None])\n",
    "brain_pos_pats_vit = rearrange(brain_pos_pats, \"b ... d -> b (...) d\").mean(-1)[0]\n",
    "tube_mask = (brain_pos_pats_vit > 0).tile(num_frames)\n",
    "print(tube_mask.sum(), tube_mask.sum() / len(tube_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> main
   "id": "e0eafa9d-8e17-436e-80b0-831996536452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume from ckpt (e.g., if you are resuming from a run that got pre-empted)\n",
    "load_progress = False\n",
    "if wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "        load_progress = True\n",
    "elif resume_from_ckpt: # if resuming without using wandb\n",
    "    load_checkpoint_in_model(model, outdir+\"/last\")\n",
    "    load_progress = True\n",
    "    \n",
    "if load_progress:\n",
    "    ckpt_path = outdir+'/last'\n",
    "    prev_params = torch.load(ckpt_path+\"/params.pt\")\n",
    "    optimizer.load_state_dict(prev_params[\"optimizer\"])\n",
    "    lr_scheduler.load_state_dict(prev_params[\"scheduler\"])\n",
    "    epoch = prev_params[\"epoch\"]\n",
    "    losses = prev_params[\"losses\"]\n",
    "    test_losses = prev_params[\"test_losses\"]\n",
    "    lrs = prev_params[\"lrs\"]\n",
    "    print(\"Loaded model params from\", ckpt_path, \"at epoch\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27dfcf-804f-4145-8c21-73740b280920",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
<<<<<<< HEAD
    "mindeye, optimizer, *train_dls, lr_scheduler = accelerator.prepare(\n",
    "    mindeye, optimizer, *train_dls, lr_scheduler\n",
    ")\n",
    "# skipping test_dl because we just use local_rank=0 for validation"
=======
    "# mindeye, optimizer, *train_dls, lr_scheduler = accelerator.prepare(\n",
    "#     mindeye, optimizer, *train_dls, lr_scheduler\n",
    "# )\n",
    "# # skipping test_dl because we just use local_rank=0 for validation"
>>>>>>> main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8d10d-63da-4ae9-8888-d7ee0db9995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
<<<<<<< HEAD
    "progress_bar = tqdm(range(epoch, num_epochs), disable=not accelerator.is_main_process)\n",
=======
    "progress_bar = tqdm(range(epoch, num_epochs), disable=global_rank!=0)\n",
>>>>>>> main
    "mse = nn.MSELoss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "test_image=None\n",
<<<<<<< HEAD
    "num_test_eval=300 # should instead be 300 to mimic MindEye2 retrieval evaluation, but this leads to OOM\n",
    "\n",
    "for epoch in progress_bar:\n",
=======
    "num_test_eval=batch_size # should instead be 300 to mimic MindEye2 retrieval evaluation, but this leads to OOM\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    print(f\"epoch {epoch}\")\n",
>>>>>>> main
    "    mindeye.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    loss_clip_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    annot_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    for s, train_dl in enumerate(train_dls):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            for iter, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(train_dl):\n",
    "                image0 = images[behav0[:,0,0].cpu().long()].float()\n",
    "                image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "\n",
<<<<<<< HEAD
=======
    "                if epoch==0 and iter==0: print(\"\\nreached start of train!\\n\")\n",
    "\n",
>>>>>>> main
    "                # if images are not fully preloaded, then can do this inefficient but more memory friendly approach\n",
    "                # for ib,b in enumerate(behav0[:,0,0].cpu().long()):\n",
    "                #     if ib==0:\n",
    "                #         image0 = torch.Tensor(images[[b]])\n",
    "                #     else:\n",
    "                #         image0 = torch.vstack((image0, torch.Tensor(images[[b]])))\n",
    "                # image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "                \n",
    "                # get the corresponding raw voxel time series\n",
    "                for ib,b in enumerate(behav0[:,0,5].cpu().long().numpy()):\n",
    "                    tr = (nsddata_raw_stimuli[nsddata_raw_stimuli['global_trial'].isin([b.item()])]['global_TR_onsets'].values + TR_delay).astype(np.int32).item()\n",
    "                    if ib==0:\n",
    "                        voxels_raw = mindeye_funcs[tr-2:tr+2][None][None]\n",
    "                    else:\n",
    "                        voxels_raw = np.vstack((voxels_raw, mindeye_funcs[tr-2:tr+2][None][None]))\n",
    "                voxels_raw = torch.Tensor(voxels_raw).to(device)\n",
    "                \n",
    "                ## Process it through pretrained VJEPA Y-Encoder (encodes the entire voxels_raw) ##\n",
<<<<<<< HEAD
    "                # tube masking\n",
    "                #tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "                #tube_mask[100:300] = True # arbitrarily deciding which patches to include\n",
    "                #tube_mask = tube_mask.tile(num_frames)\n",
    "                # encoding\n",
    "                #encoder_out = model(voxels_raw, encoder_mask=tube_mask)\n",
    "                encoder_out = model(voxels_raw, encoder_mask=None, encoder_type = \"y\")\n",
=======
    "                encoder_out = model(voxels_raw, encoder_mask=tube_mask, encoder_type = \"y\")\n",
>>>>>>> main
    "                voxel0 = encoder_out.flatten(1).unsqueeze(1).cpu()\n",
    "                \n",
    "                assert len(voxel0) == batch_size\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                    perm_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = perm\n",
    "                    betas_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = betas\n",
    "                    select_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = select\n",
    "\n",
    "                voxel_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = voxel0\n",
    "\n",
    "                if iter >= num_iterations_per_epoch:\n",
    "                    break\n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i in range(num_iterations_per_epoch):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss=0.\n",
<<<<<<< HEAD
=======
    "            if train_i==0 and epoch==0: print(f\"\\nreached start of {train_i} train loop!\\n\")\n",
>>>>>>> main
    "\n",
    "            voxel_list = [voxel_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "            image = image_iters[train_i].detach()\n",
    "            image = image.to(device)\n",
    "\n",
    "            clip_target = clip_img_embedder(image)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                perm_list = [perm_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                perm = torch.cat(perm_list, dim=0)\n",
    "                betas_list = [betas_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                betas = torch.cat(betas_list, dim=0)\n",
    "                select_list = [select_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                select = torch.cat(select_list, dim=0)\n",
    "\n",
    "            voxel_ridge_list = [mindeye.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "            voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "            backbone, clip_voxels = mindeye.backbone(voxel_ridge)\n",
    "\n",
    "            clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "            clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):                \n",
    "                loss_clip = utils.mixco_nce(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006,\n",
    "                    perm=perm, betas=betas, select=select)\n",
    "            else:\n",
    "                epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=epoch_temp)\n",
    "\n",
    "            loss_clip_total += loss_clip.item()\n",
    "            loss += loss_clip\n",
    "\n",
    "            # forward and backward top 1 accuracy        \n",
    "            labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "            fwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "            bwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            utils.check_loss(loss)\n",
<<<<<<< HEAD
    "            accelerator.backward(loss)\n",
=======
    "            loss.backward()\n",
    "            # accelerator.backward(loss)\n",
>>>>>>> main
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    mindeye.eval()\n",
<<<<<<< HEAD
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):  \n",
    "                # all test samples should be loaded per batch such that test_i should never exceed 0\n",
    "                assert len(behav) == num_test\n",
    "\n",
    "                ## Average same-image repeats ##\n",
    "                if test_image is None:\n",
    "                    print(\"prepping test set (only needs to be done once)...\")\n",
    "                    # get the corresponding raw voxel time series\n",
    "                    b = behav[:,0,5].cpu().long().numpy()\n",
    "                    trs = (nsddata_raw_stimuli[np.isin(nsddata_raw_stimuli['global_trial'].values, b)]['global_TR_onsets'].values + TR_delay).astype(np.int32)\n",
    "                    voxels_raw = np.concatenate((mindeye_funcs[trs-2][:,None,None], \n",
    "                                    mindeye_funcs[trs-1][:,None,None],\n",
    "                                    mindeye_funcs[trs][:,None,None],\n",
    "                                    mindeye_funcs[trs+1][:,None,None]), axis=2)\n",
    "                    voxels_raw = torch.Tensor(voxels_raw).to(device)\n",
    "                    assert len(voxels_raw) == num_test\n",
    "\n",
    "                    image = behav[:,0,0].cpu().long()\n",
    "                    unique_image, sort_indices = torch.unique(image, return_inverse=True)\n",
    "                    for im in unique_image[:num_test_eval]:\n",
    "                        locs = torch.where(im == image)[0]\n",
    "                        if len(locs)==1:\n",
    "                            locs = locs.repeat(3)\n",
    "                        elif len(locs)==2:\n",
    "                            locs = locs.repeat(2)[:3]\n",
    "                        assert len(locs)==3\n",
    "                        if test_image is None:\n",
    "                            test_image = torch.Tensor(images[im][None])\n",
    "                            test_voxel0 = voxels_raw[locs][None]\n",
    "                        else:\n",
    "                            test_image = torch.vstack((test_image, torch.Tensor(images[im][None])))\n",
    "                            test_voxel0 = torch.vstack((test_voxel0, voxels_raw[locs][None]))\n",
    "                            \n",
    "                    # tube masking\n",
    "                    #tube_mask = torch.zeros(num_patches // num_frames).to(torch.bool)\n",
    "                    #tube_mask[100:300] = True # arbitrarily deciding which patches to include\n",
    "                    #tube_mask = tube_mask.tile(num_frames)\n",
    "                            \n",
    "                    for rep in range(3):\n",
    "                        for mini_batch in np.arange(0,num_test_eval,30):\n",
    "                            batch_sel = np.arange(mini_batch,mini_batch+30)\n",
    "                            encoder_out = model(test_voxel0[batch_sel,rep].to(device), encoder_mask=None, encoder_type = \"y\").cpu()\n",
    "                            #encoder_out = model(test_voxel0[batch_sel,rep].to(device), encoder_mask=tube_mask).cpu()\n",
    "                            if mini_batch==0:\n",
    "                                encoder_out_stack = encoder_out\n",
    "                            else:\n",
    "                                encoder_out_stack = torch.vstack((encoder_out_stack, encoder_out))\n",
    "                        if rep == 0:\n",
    "                            test_voxel = encoder_out_stack.flatten(1).unsqueeze(1).unsqueeze(1)\n",
    "                        else:\n",
    "                            test_voxel = torch.cat((test_voxel, encoder_out_stack.flatten(1).unsqueeze(1).unsqueeze(1)), dim=1)\n",
    "                    print(\"test_voxel\", test_voxel.shape)\n",
    "                    print(\"test set prepped!\")\n",
    "\n",
    "                loss=0.\n",
    "                            \n",
    "                test_indices = torch.arange(num_test_eval)\n",
    "                voxel = test_voxel[test_indices].to(device)\n",
    "                image = test_image[test_indices].to(device)\n",
=======
    "    if global_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):  \n",
    "                loss=0.     \n",
    "                if test_i==0 and epoch==0: print(f\"\\nreached start of {test_i} test loop!\\n\")\n",
    "\n",
    "                coco_idx = behav[:,0,0].cpu().long()\n",
    "                _,test_indices = np.unique(coco_idx, return_index=True)\n",
    "                test_indices = np.random.permutation(test_indices)[:num_test_eval]\n",
    "                image = images[coco_idx[test_indices]].float().to(device)\n",
    "                \n",
    "                # get the corresponding raw voxel time series\n",
    "                for ib,b in enumerate(behav[test_indices,0,5].cpu().long().numpy()):\n",
    "                    tr = (nsddata_raw_stimuli[nsddata_raw_stimuli['global_trial'].isin([b.item()])]['global_TR_onsets'].values + TR_delay).astype(np.int32).item()\n",
    "                    if ib==0:\n",
    "                        voxels_raw = mindeye_funcs[tr-2:tr+2][None][None]\n",
    "                    else:\n",
    "                        voxels_raw = np.vstack((voxels_raw, mindeye_funcs[tr-2:tr+2][None][None]))\n",
    "                voxels_raw = torch.Tensor(voxels_raw).to(device)\n",
    "                \n",
    "                ## Process it through pretrained MAE ##\n",
    "                encoder_out = model(voxels_raw, encoder_mask=tube_mask, encoder_type = \"y\")\n",
    "                voxel = encoder_out.flatten(1).unsqueeze(1)\n",
    "\n",
>>>>>>> main
    "                assert len(image) == num_test_eval\n",
    "\n",
    "                clip_target = clip_img_embedder(image.float())\n",
    "\n",
<<<<<<< HEAD
    "                for rep in range(3):\n",
    "                    voxel_ridge = mindeye.ridge(voxel[:,rep],0) # 0th index of subj_list\n",
    "                    backbone0, clip_voxels0 = mindeye.backbone(voxel_ridge)\n",
    "                    if rep==0:\n",
    "                        clip_voxels = clip_voxels0\n",
    "                        backbone = backbone0\n",
    "                    else:\n",
    "                        clip_voxels += clip_voxels0\n",
    "                        backbone += backbone0\n",
    "                clip_voxels /= 3\n",
    "                backbone /= 3\n",
=======
    "                voxel_ridge = mindeye.ridge(voxel,0) # 0th index of subj_list\n",
    "                backbone, clip_voxels = mindeye.backbone(voxel_ridge)\n",
>>>>>>> main
    "\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006)\n",
    "\n",
    "                test_loss_clip_total += loss_clip.item()\n",
    "                loss += loss_clip\n",
    "\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                test_fwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                test_bwd_percent_correct += utils.topk(utils.prenormed_batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
<<<<<<< HEAD
    "            assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            if wandb_log: wandb.log(logs)\n",
=======
    "        logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "            \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "            \"train/lr\": lrs[-1],\n",
    "            \"train/num_steps\": len(losses),\n",
    "            \"test/num_steps\": len(test_losses),\n",
    "            \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "            \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "            \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "            \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "            \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "            \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        if not distributed: print(logs)\n",
    "        if wandb_log: wandb.log(logs)\n",
>>>>>>> main
    "            \n",
    "    # Save model checkpoint\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt()\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
<<<<<<< HEAD
    "    accelerator.wait_for_everyone()\n",
=======
    "    # accelerator.wait_for_everyone()\n",
    "    if distributed: dist.barrier()\n",
>>>>>>> main
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46b8a2-ac00-489d-8ab1-18b5b590d6ee",
<<<<<<< HEAD
   "metadata": {},
=======
   "metadata": {
    "tags": []
   },
>>>>>>> main
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training losses\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(test_losses)\n",
    "plt.title(\"Test losses\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3",
=======
   "display_name": "Python 3 (ipykernel)",
>>>>>>> main
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.8"
=======
   "version": "3.10.13"
>>>>>>> main
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
