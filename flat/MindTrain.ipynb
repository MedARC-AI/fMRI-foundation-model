{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "seed = 0\n",
    "import utils\n",
    "\n",
    "from IPython.display import clear_output # function to clear print outputs in cell\n",
    "%load_ext autoreload \n",
    "# this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2b2ad-e1ef-4262-8263-6ae9a0766caa",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127deb2-bf23-4a2e-8f86-fa8b22183546",
   "metadata": {},
   "source": [
    "### betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7063dcb1-5ae5-4a9d-a7b0-cb3323224840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>key</th>\n",
       "      <th>sub</th>\n",
       "      <th>ses</th>\n",
       "      <th>run</th>\n",
       "      <th>start</th>\n",
       "      <th>events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>[13.705362, -14.041232, 18.012413, -3.57638, 2...</td>\n",
       "      <td>sub-01_ses-21_run-13</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 38052}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>[12.209909, -18.410772, 18.181335, -2.8798864,...</td>\n",
       "      <td>sub-01_ses-21_run-13</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 38052}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>[12.927347, -26.471123, 17.100128, -3.2557995,...</td>\n",
       "      <td>sub-01_ses-21_run-13</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 38052}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>[14.002409, -29.603392, 16.541979, -3.890893, ...</td>\n",
       "      <td>sub-01_ses-21_run-13</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 38052}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>[14.845846, -33.42544, 13.760988, -4.193434, 2...</td>\n",
       "      <td>sub-01_ses-21_run-13</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 38052}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>[11.888725, 7.6382523, 11.458044, -18.99771, 2...</td>\n",
       "      <td>sub-01_ses-38_run-13</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>280</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 65554}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>[13.683649, 5.7004213, 10.686616, -20.367064, ...</td>\n",
       "      <td>sub-01_ses-38_run-13</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>281</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 65554}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>[13.473794, 2.9544108, 11.441803, -21.147268, ...</td>\n",
       "      <td>sub-01_ses-38_run-13</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>282</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 65554}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>[12.573965, -0.7948548, 11.8063965, -22.738613...</td>\n",
       "      <td>sub-01_ses-38_run-13</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>283</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 65554}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>[12.404061, -4.445602, 12.163863, -23.609213, ...</td>\n",
       "      <td>sub-01_ses-38_run-13</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>284</td>\n",
       "      <td>[{'index': 12, 'nsd_id': 65554}, {'index': 16,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5130 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                feature                   key  \\\n",
       "1710  [13.705362, -14.041232, 18.012413, -3.57638, 2...  sub-01_ses-21_run-13   \n",
       "1728  [12.209909, -18.410772, 18.181335, -2.8798864,...  sub-01_ses-21_run-13   \n",
       "1746  [12.927347, -26.471123, 17.100128, -3.2557995,...  sub-01_ses-21_run-13   \n",
       "1764  [14.002409, -29.603392, 16.541979, -3.890893, ...  sub-01_ses-21_run-13   \n",
       "1782  [14.845846, -33.42544, 13.760988, -4.193434, 2...  sub-01_ses-21_run-13   \n",
       "...                                                 ...                   ...   \n",
       "731   [11.888725, 7.6382523, 11.458044, -18.99771, 2...  sub-01_ses-38_run-13   \n",
       "749   [13.683649, 5.7004213, 10.686616, -20.367064, ...  sub-01_ses-38_run-13   \n",
       "767   [13.473794, 2.9544108, 11.441803, -21.147268, ...  sub-01_ses-38_run-13   \n",
       "785   [12.573965, -0.7948548, 11.8063965, -22.738613...  sub-01_ses-38_run-13   \n",
       "803   [12.404061, -4.445602, 12.163863, -23.609213, ...  sub-01_ses-38_run-13   \n",
       "\n",
       "      sub  ses  run  start                                             events  \n",
       "1710    1   21   13      0  [{'index': 12, 'nsd_id': 38052}, {'index': 16,...  \n",
       "1728    1   21   13      1  [{'index': 12, 'nsd_id': 38052}, {'index': 16,...  \n",
       "1746    1   21   13      2  [{'index': 12, 'nsd_id': 38052}, {'index': 16,...  \n",
       "1764    1   21   13      3  [{'index': 12, 'nsd_id': 38052}, {'index': 16,...  \n",
       "1782    1   21   13      4  [{'index': 12, 'nsd_id': 38052}, {'index': 16,...  \n",
       "...   ...  ...  ...    ...                                                ...  \n",
       "731     1   38   13    280  [{'index': 12, 'nsd_id': 65554}, {'index': 16,...  \n",
       "749     1   38   13    281  [{'index': 12, 'nsd_id': 65554}, {'index': 16,...  \n",
       "767     1   38   13    282  [{'index': 12, 'nsd_id': 65554}, {'index': 16,...  \n",
       "785     1   38   13    283  [{'index': 12, 'nsd_id': 65554}, {'index': 16,...  \n",
       "803     1   38   13    284  [{'index': 12, 'nsd_id': 65554}, {'index': 16,...  \n",
       "\n",
       "[5130 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.read_parquet('/weka/proj-fmri/paulscotti/fMRI-foundation-model/flat/checkpoints/nsdflat_large_gsr_mindeye_run2to12/epoch2/test.parquet')\n",
    "features = features.sort_values(by=[\"ses\", \"run\", \"start\"])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e368353e-665a-49f7-9d39-f8c6fb1774f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 18/18 [00:00<00:00, 89.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (1116, 1024)\n",
      "image_NSD73K_indices 1116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_TRs_per_image = 1\n",
    "TR_delay = 3\n",
    "data = []\n",
    "image_NSD73K_indices = []\n",
    "\n",
    "for sub in [1]:\n",
    "    for sess in tqdm(np.unique(features['ses'].values)):\n",
    "        # Filter DataFrame once per session\n",
    "        sess_features = features[(features['sub'] == sub) & (features['ses'] == sess)]\n",
    "        \n",
    "        for run in np.unique(sess_features['run'].values):\n",
    "            # Filter DataFrame once per run\n",
    "            run_features = sess_features[sess_features['run'] == run]\n",
    "            \n",
    "            # Extract events\n",
    "            events = run_features[run_features['start'] == 0]['events'].values[0]\n",
    "            timepoints = [e['index'] for e in events]\n",
    "            nsd_ids = [e['nsd_id'] - 1 for e in events]\n",
    "            \n",
    "            # Append NSD IDs\n",
    "            image_NSD73K_indices.extend(nsd_ids)\n",
    "            \n",
    "            # Iterate over timepoints and compute sliding windows\n",
    "            for time in timepoints:\n",
    "                sliding_windows = []\n",
    "                \n",
    "                for time_ in range(time, time + num_TRs_per_image):\n",
    "                    time_ += TR_delay\n",
    "                    sliding_window = run_features[run_features['start'] == time_]['feature'].values[0]\n",
    "                    sliding_windows.append(sliding_window)\n",
    "                \n",
    "                # Convert sliding_windows to a NumPy array and add to data\n",
    "                data.append(np.array(sliding_windows))\n",
    "\n",
    "data = np.array(data).reshape(len(data),-1)\n",
    "image_NSD73K_indices = np.array(image_NSD73K_indices)\n",
    "                \n",
    "print(\"data\", data.shape)\n",
    "print(\"image_NSD73K_indices\", len(image_NSD73K_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d05c11e-b4f9-4411-8ab1-d2d515696a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sessions 18  sess 1  n_runs 1  n_TRs 0  n_features 1024\n"
     ]
    }
   ],
   "source": [
    "n_sessions = len(np.unique(features['ses'].values)) # ses ranges from 1 to 40\n",
    "sess = 1\n",
    "n_runs = len(np.unique(sess_features['run'].values))\n",
    "n_TRs = len(features[(features['ses']==sess)&(features['run']==1)])\n",
    "n_features = len(sliding_window)\n",
    "print(f\"n_sessions {n_sessions}  sess {sess}  n_runs {n_runs}  n_TRs {n_TRs}  n_features {n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda83a7-803f-4d30-9301-0b0a533409fb",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ddf09b0-18fa-4c02-b609-2f6083a159d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 73k NSD images\n",
    "data_path = \"/weka/proj-medarc/shared/mindeyev2_dataset\"\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:]\n",
    "images = torch.Tensor(images).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27ba8ffe-0c74-4571-85f3-65fbcd0a42cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1116, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "images = images[image_NSD73K_indices]\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b80aeb2d-6d53-431c-90ed-658dca7ecebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1116, 1024)\n",
      "torch.Size([1116, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "vox = data\n",
    "print(vox.shape)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f554db1-f7cd-40d2-ab62-5d1e282c2bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1116\n",
      "837 279\n"
     ]
    }
   ],
   "source": [
    "utils.seed_everything(0)\n",
    "\n",
    "all_indices = np.random.permutation(np.arange(len(images)))\n",
    "print(len(all_indices))\n",
    "train_image_indices = all_indices[:int(len(images)*.75)]\n",
    "test_image_indices = all_indices[int(len(images)*.75):]\n",
    "print(len(train_image_indices), len(test_image_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "590f2b4b-db7c-42a1-bfd0-cc578e6af988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mean = np.mean(vox[train_image_indices],axis=0)\n",
    "# train_std = np.std(vox[train_image_indices],axis=0)\n",
    "\n",
    "# vox = utils.zscore(vox,train_mean=train_mean,train_std=train_std)\n",
    "# print(\"voxels have been zscored\")\n",
    "# print(vox[:,0].mean(), vox[:,0].std())\n",
    "# print(\"vox\", vox.shape)\n",
    "\n",
    "images = torch.Tensor(images)\n",
    "vox = torch.Tensor(vox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc5d2e32-6027-4a19-bef4-5ca068db35bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-paulscotti/found2/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float32 # change depending on your mixed_precision\n",
    "\n",
    "accelerator = Accelerator(split_batches=False)# mixed_precision=\"fp16\") # ['no', 'fp8', 'fp16', 'bf16']\n",
    "batch_size = 24 # 8 # 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 1746385\n",
      "device: cuda\n",
      "global_batch_size 24\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "global_batch_size = batch_size * num_devices\n",
    "print(\"global_batch_size\", global_batch_size)\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "# set data_type to match your mixed precision (automatically set based on deepspeed config)\n",
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    data_type = torch.bfloat16\n",
    "elif accelerator.mixed_precision == \"fp16\":\n",
    "    data_type = torch.float16\n",
    "else:\n",
    "    data_type = torch.float32\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: testing__\n",
      "--data_path=/weka/proj-medarc/shared/mindeyev2_dataset                     --model_name=testing__                     --no-multi_subject --subj=1 --batch_size=24                     --hidden_dim=1024 --clip_scale=1.                     --no-blurry_recon --blur_scale=.5                     --no-use_prior --prior_scale=30 --no-visualize_prior                     --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=10 --no-use_image_aug                     --ckpt_interval=999 --no-ckpt_saving --no-wandb_log --new_test\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"testing__\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/weka/proj-medarc/shared/mindeyev2_dataset \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --no-multi_subject --subj=1 --batch_size={batch_size} \\\n",
    "                    --hidden_dim=1024 --clip_scale=1. \\\n",
    "                    --no-blurry_recon --blur_scale=.5 \\\n",
    "                    --no-use_prior --prior_scale=30 --no-visualize_prior \\\n",
    "                    --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=10 --no-use_image_aug \\\n",
    "                    --ckpt_interval=999 --no-ckpt_saving --no-wandb_log --new_test\"# \\\n",
    "                    #--multisubject_ckpt=../../train_logs/multisubject_subj01_1024hid_nolow_300ep_seed0\"\n",
    "    # --multisubject_ckpt=../../train_logs/multisubject_subj01_1024hid_nolow_300ep_seed0\"\n",
    "    # /weka/proj-fmri/paulscotti/MindEye2_git/train_logs/final_subj01_pretrained_40sess_24bs\n",
    "    # subj01_pretrained_1sess_1024hid_nolow_seed7\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 0\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/weka/proj-fmri/shared/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=0,\n",
    "    help=\"Number of training sessions to include (if multi_subject, this variable doesnt matter)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"output visualizations from unCLIP every ckpt_interval (requires more memory!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=32,\n",
    "    help=\"Batch size can be increased by 10x if only training v2c and not diffusion diffuser\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--low_mem\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to preload images to cpu to speed things up but consume more memory\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=2,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    import kornia.augmentation as K\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    # Define the blurring augmentations\n",
    "    blur_augment = K.RandomGaussianBlur(kernel_size=(21, 21), sigma=(51.0, 51.0), p=1.)\n",
    "    \n",
    "if multi_subject:\n",
    "    subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 24 num_iterations_per_epoch = 34 num_samples_per_epoch = 837\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    # num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "    num_samples_per_epoch = len(train_image_indices)\n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e1942b0e-1223-40e6-b543-2f7ff2e8ebcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "\n",
    "train_data[f'subj0{subj}'] = torch.utils.data.TensorDataset(torch.tensor(train_image_indices))\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(torch.tensor(test_image_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "81084834-035f-4465-ad59-59e6b806a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 0 sessions\n",
      "num_voxels for subj01: 1024\n",
      "Loaded all subj train dls and vox!\n",
      "\n",
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for s in subj_list:\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    train_dl = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "\n",
    "    num_voxels_list.append(vox[0].shape[-1])\n",
    "    num_voxels[f'subj0{s}'] = vox[0].shape[-1]\n",
    "    voxels[f'subj0{s}'] = vox\n",
    "    print(f\"num_voxels for subj0{s}: {num_voxels[f'subj0{s}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and vox!\\n\")\n",
    "\n",
    "# Validate only on one subject\n",
    "if multi_subject: \n",
    "    subj = subj_list[0] # cant validate on the actual held out person so picking first in subj_list\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=24, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenOpenCLIPImageEmbedder(\n",
      "  (model): CLIP(\n",
      "    (visual): VisionTransformer(\n",
      "      (conv1): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (patch_dropout): Identity()\n",
      "      (ln_pre): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "      (transformer): Transformer(\n",
      "        (resblocks): ModuleList(\n",
      "          (0-47): 48 x ResidualAttentionBlock(\n",
      "            (ln_1): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1664, out_features=1664, bias=True)\n",
      "            )\n",
      "            (ls_1): Identity()\n",
      "            (ln_2): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1664, out_features=8192, bias=True)\n",
      "              (gelu): GELU(approximate='none')\n",
      "              (c_proj): Linear(in_features=8192, out_features=1664, bias=True)\n",
      "            )\n",
      "            (ls_2): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_post): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (token_embedding): Embedding(49408, 1280)\n",
      "    (ln_final): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## USING OpenCLIP ViT-bigG ###\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "# from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "# from omegaconf import OmegaConf\n",
    "\n",
    "try:\n",
    "    print(clip_img_embedder)\n",
    "except:\n",
    "    clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "        arch=\"ViT-bigG-14\",\n",
    "        version=\"laion2b_s39b_b160k\",\n",
    "        output_tokens=True,\n",
    "        only_tokens=True,\n",
    "    )\n",
    "    clip_img_embedder.to(device)\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664\n",
    "\n",
    "# ## USING OPEN AI CLIP ViT-L ###\n",
    "# import clip\n",
    "# try:\n",
    "#     print(clip_model)\n",
    "# except:\n",
    "#     clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "#     preprocess = transforms.Compose([\n",
    "#         transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "#         transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "#                              std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "#     ])\n",
    "# def clip_img_embedder(image):\n",
    "#     preproc_img = preprocess(image)\n",
    "#     return clip_model.encode_image(preproc_img)\n",
    "# clip_seq_dim = 1\n",
    "# clip_emb_dim = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RidgeRegression(torch.nn.Module):\n",
    "#     # make sure to add weight_decay when initializing optimizer\n",
    "#     def __init__(self, input_sizes, out_features, seq_len=1): \n",
    "#         super(RidgeRegression, self).__init__()\n",
    "#         self.seq_len = seq_len\n",
    "#         self.out_features = out_features\n",
    "#         self.linears = torch.nn.ModuleList([\n",
    "#                 torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "#             ])\n",
    "#     def forward(self, x, subj_idx=0):\n",
    "#         out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(self.seq_len)], dim=1)\n",
    "#         return out\n",
    "        \n",
    "# model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim)\n",
    "# utils.count_params(model.ridge)\n",
    "# utils.count_params(model)\n",
    "\n",
    "# # test on subject 1 with fake data\n",
    "# b = torch.randn((2,1,num_voxels_list[0]))\n",
    "# print(b.shape, model.ridge(b,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b8de65a-6d3b-4248-bea9-9b6f4d562321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "453,360,280 total\n",
      "453,360,280 trainable\n",
      "param counts:\n",
      "453,360,280 total\n",
      "453,360,280 trainable\n",
      "b.shape torch.Size([2, 1, 1024])\n",
      "torch.Size([2, 256, 1664]) torch.Size([2, 256, 1664]) torch.Size([1]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "# from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, h=4096, in_dim=15724, out_dim=768, seq_len=1, n_blocks=n_blocks, drop=.15, \n",
    "                 clip_size=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "        \n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.backbone_linear = nn.Linear(h * seq_len, out_dim, bias=True) \n",
    "        if clip_scale>0:\n",
    "            self.clip_proj = self.projector(clip_size, clip_size, h=clip_size)\n",
    "            \n",
    "    def projector(self, in_dim, out_dim, h=2048):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, out_dim)\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "    \n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make empty tensors\n",
    "        c,b = torch.Tensor([0.]), torch.Tensor([[0.],[0.]])\n",
    "        \n",
    "        # Mixer blocks\n",
    "        residual1 = x\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.backbone_linear(x).reshape(len(x), -1, self.clip_size)\n",
    "        if clip_scale>0:\n",
    "            c = self.clip_proj(backbone)\n",
    "        \n",
    "        return backbone, c, b\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, \n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,1,hidden_dim))\n",
    "print(\"b.shape\",b.shape)\n",
    "\n",
    "backbone_, clip_, blur_ = model.backbone(b)\n",
    "print(backbone_.shape, clip_.shape, blur_[0].shape, blur_[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397c0d7-52a3-4153-823b-c27d2eb3eeba",
   "metadata": {},
   "source": [
    "### Adding diffusion prior + unCLIP if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 52\n",
    "    heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = VersatileDiffusionPriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)\n",
    "    \n",
    "    # prep unCLIP\n",
    "    if visualize_prior:\n",
    "        config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "        config = OmegaConf.to_container(config, resolve=True)\n",
    "        unclip_params = config[\"model\"][\"params\"]\n",
    "        network_config = unclip_params[\"network_config\"]\n",
    "        denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "        first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "        conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "        sampler_config = unclip_params[\"sampler_config\"]\n",
    "        scale_factor = unclip_params[\"scale_factor\"]\n",
    "        disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "        offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "        first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "        sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "        diffusion_engine = DiffusionEngine(network_config=network_config,\n",
    "                               denoiser_config=denoiser_config,\n",
    "                               first_stage_config=first_stage_config,\n",
    "                               conditioner_config=conditioner_config,\n",
    "                               sampler_config=sampler_config,\n",
    "                               scale_factor=scale_factor,\n",
    "                               disable_first_stage_autocast=disable_first_stage_autocast)\n",
    "        # set to inference\n",
    "        diffusion_engine.eval().requires_grad_(False)\n",
    "        diffusion_engine.to(device)\n",
    "\n",
    "        ckpt_path = '/weka/proj-fmri/shared/mindeyev2_dataset/unclip6_epoch0_step110000.ckpt'\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "        image = images[:1].to(device)\n",
    "        batch={\"jpg\": image,\n",
    "              \"original_size_as_tuple\": torch.ones(image.shape[0], 2).to(device) * 768,\n",
    "              \"crop_coords_top_left\": torch.zeros(image.shape[0], 2).to(device)}\n",
    "        out = diffusion_engine.conditioner(batch)\n",
    "        vector_suffix = out[\"vector\"].to(device)\n",
    "        print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 340\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "453,360,280 total\n",
      "453,360,280 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "opt_grouped_parameters = [\n",
    "    # {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "# model.backbone.requires_grad_(False)\n",
    "\n",
    "if use_prior:\n",
    "    opt_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ])\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    if num_iterations_per_epoch==0:\n",
    "        num_iterations_per_epoch=1\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ea5cd-f7ba-4f15-8056-3cd2535bca97",
   "metadata": {},
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc7b19fa-0c75-4786-b24a-3b51e1737918",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'rtmindeye'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "607a7c7b-fe5e-41a4-80bf-d2814b3a57cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load multisubject stage1 ckpt if set\n",
    "if multisubject_ckpt is not None and not resume_from_ckpt:\n",
    "    load_ckpt(\"last\",outdir=multisubject_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,multisubj_loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00ea5ae0-5c92-4276-af5b-25a17ba4dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(multisubject_ckpt+'/last.pth', map_location='cpu')\n",
    "# state_dict = checkpoint['model_state_dict']\n",
    "# model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "\n",
    "model, optimizer, train_dl, lr_scheduler = accelerator.prepare(model, optimizer, train_dl, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing__ starting with epoch 0 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                       | 1/10 [00:07<01:03,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': 3.243920038728153, 'test/loss': 3.5680848251689565, 'train/lr': 0.00015262426940113047, 'train/num_steps': 34, 'test/num_steps': 11, 'train/fwd_pct_correct': 0.03921568711452624, 'train/bwd_pct_correct': 0.040441177675829214, 'test/test_fwd_pct_correct': 0.06439394063570282, 'test/test_bwd_pct_correct': 0.049242425709962845, 'train/loss_clip_total': 3.243920038728153, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 3.5680848251689565, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 2/10 [00:13<00:54,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': 2.888705758487477, 'test/loss': 3.910676891153509, 'train/lr': 0.0003, 'train/num_steps': 68, 'test/num_steps': 22, 'train/fwd_pct_correct': 0.4987745241207235, 'train/bwd_pct_correct': 0.12254902172614546, 'test/test_fwd_pct_correct': 0.03787878900766373, 'test/test_bwd_pct_correct': 0.03787878900766373, 'train/loss_clip_total': 2.888705758487477, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 3.910676891153509, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████▏                              | 3/10 [00:20<00:47,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': 2.694066650727216, 'test/loss': 3.743324388157238, 'train/lr': 0.0002885823865994979, 'train/num_steps': 102, 'test/num_steps': 33, 'train/fwd_pct_correct': 0.6102941413136089, 'train/bwd_pct_correct': 0.1568627504303175, 'test/test_fwd_pct_correct': 0.03787878900766373, 'test/test_bwd_pct_correct': 0.04545454680919647, 'train/loss_clip_total': 2.694066650727216, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 3.743324388157238, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▌                          | 4/10 [00:27<00:40,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': 2.5690394745153537, 'test/loss': 3.8814841400493276, 'train/lr': 0.000256067774537295, 'train/num_steps': 136, 'test/num_steps': 44, 'train/fwd_pct_correct': 0.8602941386839923, 'train/bwd_pct_correct': 0.2291666742633371, 'test/test_fwd_pct_correct': 0.034090910106897354, 'test/test_bwd_pct_correct': 0.0416666679084301, 'train/loss_clip_total': 2.5690394745153537, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 3.8814841400493276, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████                      | 5/10 [00:33<00:33,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': 1.45829756119672, 'test/loss': 3.8244844783436167, 'train/lr': 0.00020740621875416925, 'train/num_steps': 170, 'test/num_steps': 55, 'train/fwd_pct_correct': 0.9681372695109424, 'train/bwd_pct_correct': 0.48161765757729025, 'test/test_fwd_pct_correct': 0.04166666756976734, 'test/test_bwd_pct_correct': 0.04924242537130009, 'train/loss_clip_total': 1.45829756119672, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 3.8244844783436167, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████▍                 | 6/10 [00:40<00:26,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': 0.4902816555079292, 'test/loss': 3.993205048821189, 'train/lr': 0.00015000599999999997, 'train/num_steps': 204, 'test/num_steps': 66, 'train/fwd_pct_correct': 0.9828431518638835, 'train/bwd_pct_correct': 0.7941176663426792, 'test/test_fwd_pct_correct': 0.018939394503831863, 'test/test_bwd_pct_correct': 0.0340909097682346, 'train/loss_clip_total': 0.4902816555079292, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 3.993205048821189, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████▊             | 7/10 [00:46<00:20,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': 0.1845155318870264, 'test/loss': 4.00300483270125, 'train/lr': 9.260578124583071e-05, 'train/num_steps': 238, 'test/num_steps': 77, 'train/fwd_pct_correct': 0.9987745109726401, 'train/bwd_pct_correct': 0.9289215873269474, 'test/test_fwd_pct_correct': 0.02651515230536461, 'test/test_bwd_pct_correct': 0.05303030427206646, 'train/loss_clip_total': 0.1845155318870264, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 4.00300483270125, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████▏        | 8/10 [00:53<00:13,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': 0.08408313628066988, 'test/loss': 4.076170964674517, 'train/lr': 4.394422546270499e-05, 'train/num_steps': 272, 'test/num_steps': 88, 'train/fwd_pct_correct': 0.9987745109726401, 'train/bwd_pct_correct': 0.986519620699041, 'test/test_fwd_pct_correct': 0.0340909097682346, 'test/test_bwd_pct_correct': 0.03787878866900097, 'train/loss_clip_total': 0.08408313628066988, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 4.076170964674517, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████▌    | 9/10 [01:00<00:06,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': 0.0749330030315939, 'test/loss': 4.0991316925395616, 'train/lr': 1.1429613400502053e-05, 'train/num_steps': 306, 'test/num_steps': 99, 'train/fwd_pct_correct': 0.9987745109726401, 'train/bwd_pct_correct': 0.9938725531101227, 'test/test_fwd_pct_correct': 0.02651515230536461, 'test/test_bwd_pct_correct': 0.04166666756976734, 'train/loss_clip_total': 0.0749330030315939, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 4.0991316925395616, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [01:07<00:00,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': 0.06325493687215973, 'test/loss': 4.101426818154075, 'train/lr': 1.1999999999999998e-08, 'train/num_steps': 340, 'test/num_steps': 110, 'train/fwd_pct_correct': 1.0, 'train/bwd_pct_correct': 0.9987745109726401, 'test/test_fwd_pct_correct': 0.03030303120613098, 'test/test_bwd_pct_correct': 0.04166666756976734, 'train/loss_clip_total': 0.06325493687215973, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 4.101426818154075, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n",
      "\n",
      "===Finished!===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "test_image, test_voxel = None, None\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "skip_train = True if epoch>=(num_epochs-1) else False # skip training if you are resuming from a fully trained model\n",
    "\n",
    "for epoch in tqdm(range(epoch,num_epochs)):\n",
    "    model.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    loss_blurry_cont_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. \n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i, behav in enumerate(train_dl):  \n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0.\n",
    "            \n",
    "            behav = behav[0]\n",
    "\n",
    "            image = images[behav.long().cpu()].to(device)\n",
    "            voxel = vox[behav.long().cpu()]\n",
    "            # voxel = (voxel - train_mean) / train_std\n",
    "            voxel = torch.Tensor(voxel).unsqueeze(1).to(device)\n",
    "\n",
    "            if use_image_aug: \n",
    "                image = img_augment(image)\n",
    "\n",
    "            clip_target = clip_img_embedder(image)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                voxel, perm, betas, select = utils.mixco(voxel)\n",
    "\n",
    "            # voxel_ridge = model.ridge(voxel,0) #[model.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "            # voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "            backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel)#voxel_ridge)\n",
    "\n",
    "            if clip_scale>0:\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if use_prior:\n",
    "                loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                loss_prior_total += loss_prior.item()\n",
    "                loss_prior *= prior_scale\n",
    "                loss += loss_prior\n",
    "\n",
    "                recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                recon_mse += mse(prior_out, clip_target).item()\n",
    "\n",
    "            if clip_scale>0:\n",
    "                if epoch < int(mixup_pct * num_epochs):                \n",
    "                    loss_clip = utils.mixco_nce(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006,\n",
    "                        perm=perm, betas=betas, select=select)\n",
    "                else:\n",
    "                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=epoch_temp)\n",
    "\n",
    "                loss_clip_total += loss_clip.item()\n",
    "                loss_clip *= clip_scale\n",
    "                loss += loss_clip\n",
    "\n",
    "            if blurry_recon:     \n",
    "                image_enc_pred, transformer_feats = blurry_image_enc_\n",
    "\n",
    "                image_enc = autoenc.encode(2*image-1).latent_dist.mode() * 0.18215\n",
    "                loss_blurry = l1(image_enc_pred, image_enc)\n",
    "                loss_blurry_total += loss_blurry.item()\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    image_enc_shuf = image_enc[perm]\n",
    "                    betas_shape = [-1] + [1]*(len(image_enc.shape)-1)\n",
    "                    image_enc[select] = image_enc[select] * betas[select].reshape(*betas_shape) + \\\n",
    "                        image_enc_shuf[select] * (1 - betas[select]).reshape(*betas_shape)\n",
    "\n",
    "                image_norm = (image - mean)/std\n",
    "                image_aug = (blur_augs(image) - mean)/std\n",
    "                _, cnx_embeds = cnx(image_norm)\n",
    "                _, cnx_aug_embeds = cnx(image_aug)\n",
    "\n",
    "                cont_loss = utils.soft_cont_loss(\n",
    "                    nn.functional.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    temp=0.2)\n",
    "                loss_blurry_cont_total += cont_loss.item()\n",
    "\n",
    "                loss += (loss_blurry + 0.1*cont_loss) * blur_scale #/.18215\n",
    "\n",
    "            if clip_scale>0:\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            if blurry_recon:\n",
    "                with torch.no_grad():\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    blurry_pixcorr += pixcorr.item()\n",
    "            \n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "                \n",
    "            if train_i >= num_iterations_per_epoch-1:\n",
    "                break\n",
    "                \n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, behav in enumerate(test_dl):  \n",
    "                behav = behav[0]\n",
    "\n",
    "                loss=0.\n",
    "\n",
    "                if behav.ndim==2:\n",
    "                    image = images[behav[:,0].long().cpu()].to(device)\n",
    "                    voxel = vox[behav.long().cpu()].mean(1)\n",
    "                else:\n",
    "                    image = images[behav.long().cpu()].to(device)\n",
    "                    voxel = vox[behav.long().cpu()]\n",
    "                    \n",
    "                voxel = torch.Tensor(voxel).unsqueeze(1).to(device)\n",
    "\n",
    "                clip_target = clip_img_embedder(image.float())\n",
    "                \n",
    "                # voxel_ridge = model.ridge(voxel,0)\n",
    "\n",
    "                backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel) #voxel_ridge)\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                    clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                # for some evals, only doing a subset of the samples per batch because of computational cost\n",
    "                random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                \n",
    "                if use_prior:\n",
    "                    loss_prior, contaminated_prior_out = model.diffusion_prior(text_embed=backbone[random_samps], image_embed=clip_target[random_samps])\n",
    "                    test_loss_prior_total += loss_prior.item()\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                    \n",
    "                    if visualize_prior:\n",
    "                        # now get unCLIP prediction without feeding it the image embed to get uncontaminated reconstruction\n",
    "                        prior_out = model.diffusion_prior.p_sample_loop(backbone[random_samps].shape, \n",
    "                                        text_cond = dict(text_embed = backbone[random_samps]), \n",
    "                                        cond_scale = 1., timesteps = timesteps)\n",
    "\n",
    "                        test_recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target[random_samps]).mean().item()\n",
    "                        test_recon_mse += mse(prior_out, clip_target[random_samps]).item()\n",
    "                        \n",
    "                if clip_scale>0:\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006)\n",
    "\n",
    "                    test_loss_clip_total += loss_clip.item()\n",
    "                    loss_clip = loss_clip * clip_scale\n",
    "                    loss += loss_clip\n",
    "\n",
    "                if blurry_recon:\n",
    "                    image_enc_pred, _ = blurry_image_enc_\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    test_blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    # forward and backward top 1 accuracy        \n",
    "                    labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                    test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                    test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            # if utils.is_interactive(): clear_output(wait=True)\n",
    "            if skip_train: break\n",
    "            print(\"---\")\n",
    "\n",
    "            # assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"train/loss_blurry_cont_total\": loss_blurry_cont_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            # if finished training, save jpg recons if they exist\n",
    "            if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                if blurry_recon:    \n",
    "                    image_enc = autoenc.encode(2*image[:4]-1).latent_dist.mode() * 0.18215\n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc_pred[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "\n",
    "                    plt.show()\n",
    "                        \n",
    "                if use_prior and visualize_prior: # output recons every ckpt\n",
    "                    idx = np.random.randint(0, 3)\n",
    "                    print(f\"reconstructing... idx={idx}\")\n",
    "                    samples = utils.unclip_recon(prior_out[[idx]],\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix)\n",
    "                    if utils.is_interactive():\n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(image[idx]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(samples[0]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "\n",
    "            print(logs)\n",
    "\n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint and reconstruct\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt(f'last')\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5702acf6-45fe-44f5-8842-c0e2d4d8e8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_losses); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a54acc-1dce-4de4-9d5f-d0582f5097c5",
   "metadata": {},
   "source": [
    "**To tell if the model is working I'm looking at test_bwd/fwd_pct_correct and seeing if that is doing better than chance (1/batch_size)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "found2",
   "language": "python",
   "name": "found2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
